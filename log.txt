namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.303, acc: 10.36, fv: 2.303, accv: 10.39, lr: 0.0010, time: 14.11
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:41<5:36:08, 101.35s/it]  1%|          | 2/200 [03:08<5:07:28, 93.18s/it]   2%|▏         | 3/200 [04:35<4:56:10, 90.21s/it]  2%|▏         | 4/200 [06:00<4:47:53, 88.13s/it]  2%|▎         | 5/200 [07:24<4:41:10, 86.51s/it]  3%|▎         | 6/200 [07:44<3:27:11, 64.08s/it]  4%|▎         | 7/200 [08:18<2:54:09, 54.14s/it]  4%|▍         | 8/200 [08:39<2:19:30, 43.59s/it]  4%|▍         | 9/200 [09:13<2:09:12, 40.59s/it]  5%|▌         | 10/200 [09:33<1:49:01, 34.43s/it]  6%|▌         | 11/200 [10:08<1:48:15, 34.37s/it]  6%|▌         | 12/200 [10:28<1:34:16, 30.09s/it]  6%|▋         | 13/200 [11:03<1:38:03, 31.46s/it]  7%|▋         | 14/200 [11:23<1:26:50, 28.01s/it]  8%|▊         | 15/200 [11:58<1:32:59, 30.16s/it]  8%|▊         | 16/200 [12:32<1:35:52, 31.26s/it]  8%|▊         | 17/200 [13:06<1:38:01, 32.14s/it]  9%|▉         | 18/200 [13:26<1:26:20, 28.47s/it] 10%|▉         | 19/200 [14:01<1:31:52, 30.45s/it] 10%|█         | 20/200 [14:21<1:22:14, 27.41s/it] 10%|█         | 21/200 [14:56<1:28:12, 29.56s/it] 11%|█         | 22/200 [15:16<1:19:53, 26.93s/it] 12%|█▏        | 23/200 [15:51<1:26:05, 29.18s/it] 12%|█▏        | 24/200 [16:11<1:18:02, 26.61s/it] 12%|█▎        | 25/200 [16:46<1:24:52, 29.10s/it] 13%|█▎        | 26/200 [17:06<1:16:27, 26.37s/it] 14%|█▎        | 27/200 [17:42<1:23:58, 29.12s/it] 14%|█▍        | 28/200 [18:01<1:15:08, 26.21s/it] 14%|█▍        | 29/200 [18:37<1:23:09, 29.18s/it] 15%|█▌        | 30/200 [18:56<1:13:52, 26.07s/it] 16%|█▌        | 31/200 [19:32<1:21:44, 29.02s/it] 16%|█▌        | 32/200 [19:51<1:12:45, 25.98s/it] 16%|█▋        | 33/200 [20:27<1:20:38, 28.97s/it] 17%|█▋        | 34/200 [20:46<1:11:36, 25.88s/it] 18%|█▊        | 35/200 [21:22<1:19:29, 28.91s/it] 18%|█▊        | 36/200 [21:44<1:13:14, 26.80s/it] 18%|█▊        | 37/200 [22:06<1:09:21, 25.53s/it] 19%|█▉        | 38/200 [22:29<1:06:38, 24.68s/it] 20%|█▉        | 39/200 [22:51<1:04:34, 24.07s/it] 20%|██        | 40/200 [23:14<1:03:03, 23.64s/it] 20%|██        | 41/200 [23:37<1:01:52, 23.35s/it] 21%|██        | 42/200 [23:59<1:00:53, 23.12s/it] 22%|██▏       | 43/200 [24:22<1:00:09, 22.99s/it] 22%|██▏       | 44/200 [24:42<57:34, 22.14s/it]   22%|██▎       | 45/200 [25:03<55:53, 21.63s/it] 23%|██▎       | 46/200 [25:39<1:06:37, 25.96s/it] 24%|██▎       | 47/200 [26:01<1:03:01, 24.71s/it] 24%|██▍       | 48/200 [26:23<1:01:03, 24.10s/it] 24%|██▍       | 49/200 [26:46<59:34, 23.67s/it]   25%|██▌       | 50/200 [27:09<58:26, 23.37s/it] 26%|██▌       | 51/200 [27:31<57:31, 23.16s/it] 26%|██▌       | 52/200 [27:54<56:43, 23.00s/it] 26%|██▋       | 53/200 [28:16<56:03, 22.88s/it] 27%|██▋       | 54/200 [28:39<55:31, 22.82s/it] 28%|██▊       | 55/200 [29:02<55:03, 22.78s/it] 28%|██▊       | 56/200 [29:24<54:33, 22.73s/it] 28%|██▊       | 57/200 [29:47<54:07, 22.71s/it] 29%|██▉       | 58/200 [30:10<53:41, 22.69s/it] 30%|██▉       | 59/200 [30:29<51:02, 21.72s/it] 30%|███       | 60/200 [30:50<50:22, 21.59s/it] 30%|███       | 61/200 [31:26<59:58, 25.89s/it] 31%|███       | 62/200 [31:48<56:52, 24.73s/it] 32%|███▏      | 63/200 [32:11<55:01, 24.10s/it] 32%|███▏      | 64/200 [32:34<53:37, 23.66s/it] 32%|███▎      | 65/200 [32:56<52:34, 23.36s/it] 33%|███▎      | 66/200 [33:19<51:41, 23.15s/it] 34%|███▎      | 67/200 [33:42<50:58, 23.00s/it] 34%|███▍      | 68/200 [34:04<50:23, 22.91s/it] 34%|███▍      | 69/200 [34:27<49:48, 22.81s/it] 35%|███▌      | 70/200 [34:50<49:19, 22.76s/it] 36%|███▌      | 71/200 [35:12<48:50, 22.72s/it] 36%|███▌      | 72/200 [35:35<48:25, 22.70s/it] 36%|███▋      | 73/200 [35:57<48:00, 22.68s/it] 37%|███▋      | 74/200 [36:17<45:27, 21.65s/it] 38%|███▊      | 75/200 [36:38<45:06, 21.65s/it] 38%|███▊      | 76/200 [37:14<53:28, 25.88s/it] 38%|███▊      | 77/200 [37:36<50:44, 24.76s/it] 39%|███▉      | 78/200 [37:59<48:59, 24.09s/it] 40%|███▉      | 79/200 [38:21<47:42, 23.66s/it] 40%|████      | 80/200 [38:44<46:41, 23.35s/it] 40%|████      | 81/200 [39:07<45:55, 23.15s/it] 41%|████      | 82/200 [39:29<45:13, 23.00s/it] 42%|████▏     | 83/200 [39:52<44:35, 22.87s/it] 42%|████▏     | 84/200 [40:15<44:06, 22.82s/it] 42%|████▎     | 85/200 [40:37<43:36, 22.75s/it] 43%|████▎     | 86/200 [41:00<43:10, 22.73s/it] 44%|████▎     | 87/200 [41:22<42:42, 22.68s/it] 44%|████▍     | 88/200 [41:45<42:12, 22.61s/it] 44%|████▍     | 89/200 [42:04<39:54, 21.57s/it] 45%|████▌     | 90/200 [42:26<39:44, 21.67s/it] 46%|████▌     | 91/200 [43:01<46:52, 25.80s/it] 46%|████▌     | 92/200 [43:24<44:26, 24.69s/it] 46%|████▋     | 93/200 [43:46<42:57, 24.09s/it] 47%|████▋     | 94/200 [44:09<41:46, 23.65s/it] 48%|████▊     | 95/200 [44:31<40:49, 23.33s/it] 48%|████▊     | 96/200 [44:54<40:05, 23.13s/it] 48%|████▊     | 97/200 [45:17<39:26, 22.97s/it] 49%|████▉     | 98/200 [45:39<38:53, 22.88s/it] 50%|████▉     | 99/200 [46:02<38:23, 22.81s/it] 50%|█████     | 100/200 [46:25<37:58, 22.78s/it] 50%|█████     | 101/200 [46:47<37:31, 22.74s/it] 51%|█████     | 102/200 [47:10<37:04, 22.70s/it] 52%|█████▏    | 103/200 [47:32<36:23, 22.51s/it] 52%|█████▏    | 104/200 [47:51<34:31, 21.58s/it] 52%|█████▎    | 105/200 [48:14<34:27, 21.77s/it] 53%|█████▎    | 106/200 [48:49<40:24, 25.80s/it] 54%|█████▎    | 107/200 [49:11<38:14, 24.68s/it] 54%|█████▍    | 108/200 [49:34<36:53, 24.06s/it] 55%|█████▍    | 109/200 [49:56<35:50, 23.64s/it] 55%|█████▌    | 110/200 [50:19<34:59, 23.32s/it] 56%|█████▌    | 111/200 [50:41<34:17, 23.12s/it] 56%|█████▌    | 112/200 [51:04<33:40, 22.97s/it] 56%|█████▋    | 113/200 [51:27<33:07, 22.85s/it] 57%|█████▋    | 114/200 [51:49<32:39, 22.78s/it] 57%|█████▊    | 115/200 [52:12<32:13, 22.74s/it] 58%|█████▊    | 116/200 [52:35<31:48, 22.72s/it] 58%|█████▊    | 117/200 [52:57<31:24, 22.70s/it] 59%|█████▉    | 118/200 [53:19<30:35, 22.39s/it] 60%|█████▉    | 119/200 [53:39<29:10, 21.61s/it] 60%|██████    | 120/200 [54:01<29:07, 21.84s/it] 60%|██████    | 121/200 [54:36<33:56, 25.78s/it] 61%|██████    | 122/200 [54:58<32:04, 24.67s/it] 62%|██████▏   | 123/200 [55:21<30:53, 24.07s/it] 62%|██████▏   | 124/200 [55:43<29:55, 23.62s/it] 62%|██████▎   | 125/200 [56:06<29:08, 23.32s/it] 63%|██████▎   | 126/200 [56:29<28:30, 23.12s/it] 64%|██████▎   | 127/200 [56:51<27:56, 22.96s/it] 64%|██████▍   | 128/200 [57:14<27:25, 22.86s/it] 64%|██████▍   | 129/200 [57:36<26:59, 22.81s/it] 65%|██████▌   | 130/200 [57:59<26:34, 22.78s/it] 66%|██████▌   | 131/200 [58:22<26:07, 22.72s/it] 66%|██████▌   | 132/200 [58:44<25:43, 22.70s/it] 66%|██████▋   | 133/200 [59:06<24:52, 22.27s/it] 67%|██████▋   | 134/200 [59:26<23:43, 21.57s/it] 68%|██████▊   | 135/200 [59:48<23:42, 21.88s/it] 68%|██████▊   | 136/200 [1:00:23<27:34, 25.85s/it] 68%|██████▊   | 137/200 [1:00:45<25:55, 24.69s/it] 69%|██████▉   | 138/200 [1:01:08<24:52, 24.07s/it] 70%|██████▉   | 139/200 [1:01:31<24:02, 23.65s/it] 70%|███████   | 140/200 [1:01:53<23:21, 23.35s/it] 70%|███████   | 141/200 [1:02:16<22:44, 23.13s/it] 71%|███████   | 142/200 [1:02:39<22:13, 22.99s/it] 72%|███████▏  | 143/200 [1:03:01<21:43, 22.86s/it] 72%|███████▏  | 144/200 [1:03:24<21:16, 22.80s/it] 72%|███████▎  | 145/200 [1:03:46<20:51, 22.75s/it] 73%|███████▎  | 146/200 [1:04:09<20:26, 22.71s/it] 74%|███████▎  | 147/200 [1:04:32<20:03, 22.71s/it] 74%|███████▍  | 148/200 [1:04:53<19:13, 22.19s/it] 74%|███████▍  | 149/200 [1:05:13<18:20, 21.58s/it] 75%|███████▌  | 150/200 [1:05:35<18:13, 21.88s/it] 76%|███████▌  | 151/200 [1:06:11<21:07, 25.87s/it] 76%|███████▌  | 152/200 [1:06:33<19:47, 24.74s/it] 76%|███████▋  | 153/200 [1:06:55<18:53, 24.12s/it] 77%|███████▋  | 154/200 [1:07:18<18:09, 23.68s/it] 78%|███████▊  | 155/200 [1:07:41<17:31, 23.36s/it] 78%|███████▊  | 156/200 [1:08:03<16:58, 23.15s/it] 78%|███████▊  | 157/200 [1:08:26<16:28, 22.99s/it] 79%|███████▉  | 158/200 [1:08:49<16:01, 22.89s/it] 80%|███████▉  | 159/200 [1:09:11<15:35, 22.82s/it] 80%|████████  | 160/200 [1:09:34<15:11, 22.78s/it] 80%|████████  | 161/200 [1:09:57<14:47, 22.76s/it] 81%|████████  | 162/200 [1:10:19<14:23, 22.73s/it] 82%|████████▏ | 163/200 [1:10:40<13:38, 22.11s/it] 82%|████████▏ | 164/200 [1:11:01<12:58, 21.63s/it] 82%|████████▎ | 165/200 [1:11:23<12:48, 21.95s/it] 83%|████████▎ | 166/200 [1:11:58<14:40, 25.89s/it] 84%|████████▎ | 167/200 [1:12:20<13:35, 24.72s/it] 84%|████████▍ | 168/200 [1:12:43<12:50, 24.08s/it] 84%|████████▍ | 169/200 [1:13:05<12:12, 23.64s/it] 85%|████████▌ | 170/200 [1:13:28<11:39, 23.33s/it] 86%|████████▌ | 171/200 [1:13:51<11:09, 23.10s/it] 86%|████████▌ | 172/200 [1:14:13<10:42, 22.95s/it] 86%|████████▋ | 173/200 [1:14:36<10:17, 22.88s/it] 87%|████████▋ | 174/200 [1:14:59<09:53, 22.84s/it] 88%|████████▊ | 175/200 [1:15:21<09:29, 22.78s/it] 88%|████████▊ | 176/200 [1:15:44<09:05, 22.73s/it] 88%|████████▊ | 177/200 [1:16:07<08:42, 22.71s/it] 89%|████████▉ | 178/200 [1:16:27<08:05, 22.06s/it] 90%|████████▉ | 179/200 [1:16:48<07:34, 21.65s/it] 90%|█████████ | 180/200 [1:17:10<07:17, 21.86s/it] 90%|█████████ | 181/200 [1:17:46<08:12, 25.92s/it] 91%|█████████ | 182/200 [1:18:08<07:25, 24.75s/it] 92%|█████████▏| 183/200 [1:18:30<06:49, 24.12s/it] 92%|█████████▏| 184/200 [1:18:53<06:18, 23.67s/it] 92%|█████████▎| 185/200 [1:19:16<05:50, 23.37s/it] 93%|█████████▎| 186/200 [1:19:38<05:24, 23.15s/it] 94%|█████████▎| 187/200 [1:20:01<04:58, 22.99s/it] 94%|█████████▍| 188/200 [1:20:23<04:34, 22.87s/it] 94%|█████████▍| 189/200 [1:20:46<04:10, 22.81s/it] 95%|█████████▌| 190/200 [1:21:09<03:47, 22.76s/it] 96%|█████████▌| 191/200 [1:21:31<03:24, 22.72s/it] 96%|█████████▌| 192/200 [1:21:54<03:01, 22.70s/it] 96%|█████████▋| 193/200 [1:22:15<02:34, 22.05s/it] 97%|█████████▋| 194/200 [1:22:35<02:09, 21.66s/it] 98%|█████████▊| 195/200 [1:22:57<01:48, 21.79s/it] 98%|█████████▊| 196/200 [1:23:33<01:43, 25.98s/it] 98%|█████████▊| 197/200 [1:23:51<01:10, 23.64s/it] 99%|█████████▉| 198/200 [1:24:06<00:41, 20.97s/it][000001] f: 2.300, acc: 11.54, fv: 2.299, accv: 11.60, lr: 0.0010, time: 13.37
[000063] f: 1.674, acc: 38.26, fv: 1.652, accv: 38.73, lr: 0.0010, time: 13.45
[000125] f: 1.639, acc: 38.78, fv: 1.677, accv: 37.33, lr: 0.0010, time: 13.45
[000187] f: 1.489, acc: 46.86, fv: 1.460, accv: 47.38, lr: 0.0010, time: 13.52
[000249] f: 1.286, acc: 54.88, fv: 1.269, accv: 54.27, lr: 0.0010, time: 13.51
[000250] f: 1.298, acc: 53.28, fv: 1.295, accv: 52.59, lr: 0.0010, time: 13.51
[000251] f: 1.313, acc: 52.25, fv: 1.325, accv: 50.92, lr: 0.0010, time: 13.57
[000313] f: 1.233, acc: 55.78, fv: 1.211, accv: 55.53, lr: 0.0010, time: 13.58
[000375] f: 1.178, acc: 56.72, fv: 1.187, accv: 56.81, lr: 0.0010, time: 13.60
[000437] f: 1.206, acc: 56.79, fv: 1.203, accv: 57.45, lr: 0.0010, time: 13.64
[000499] f: 1.172, acc: 57.39, fv: 1.207, accv: 56.25, lr: 0.0010, time: 13.55
[000501] f: 1.097, acc: 61.29, fv: 1.111, accv: 60.69, lr: 0.0010, time: 13.59
[000563] f: 1.027, acc: 63.97, fv: 1.016, accv: 64.16, lr: 0.0010, time: 13.63
[000625] f: 1.100, acc: 60.60, fv: 1.103, accv: 60.94, lr: 0.0010, time: 13.73
[000687] f: 1.037, acc: 63.58, fv: 1.068, accv: 62.19, lr: 0.0010, time: 13.67
[000749] f: 1.020, acc: 63.59, fv: 1.057, accv: 62.20, lr: 0.0010, time: 13.57
[000751] f: 1.018, acc: 64.22, fv: 1.023, accv: 63.55, lr: 0.0010, time: 13.69
[000813] f: 0.929, acc: 66.88, fv: 0.956, accv: 66.19, lr: 0.0010, time: 13.62
[000875] f: 1.054, acc: 62.08, fv: 1.108, accv: 60.81, lr: 0.0010, time: 13.70
[000937] f: 0.901, acc: 68.06, fv: 0.903, accv: 67.81, lr: 0.0010, time: 13.59
[000999] f: 1.003, acc: 63.98, fv: 1.004, accv: 63.79, lr: 0.0010, time: 13.57
[001001] f: 0.937, acc: 67.09, fv: 0.934, accv: 67.04, lr: 0.0010, time: 13.66
[001063] f: 0.873, acc: 69.19, fv: 0.876, accv: 69.17, lr: 0.0010, time: 13.62
[001125] f: 0.956, acc: 66.31, fv: 1.044, accv: 63.93, lr: 0.0010, time: 13.60
[001187] f: 0.911, acc: 68.63, fv: 0.954, accv: 67.14, lr: 0.0010, time: 13.62
[001249] f: 0.859, acc: 68.93, fv: 0.904, accv: 67.40, lr: 0.0010, time: 13.71
[001750] f: 0.931, acc: 67.26, fv: 0.950, accv: 67.58, lr: 0.0010, time: 13.84
[002250] f: 1.600, acc: 54.37, fv: 1.782, accv: 54.20, lr: 0.0010, time: 13.89
[002750] f: 0.704, acc: 75.70, fv: 0.827, accv: 73.03, lr: 0.0010, time: 13.80
[003250] f: 0.618, acc: 78.40, fv: 0.715, accv: 75.09, lr: 0.0010, time: 13.82
[003750] f: 0.684, acc: 75.67, fv: 0.822, accv: 72.88, lr: 0.0010, time: 13.63
[004000] f: 0.721, acc: 76.15, fv: 0.844, accv: 74.20, lr: 0.0010, time: 13.61
[004250] f: 0.638, acc: 78.17, fv: 0.762, accv: 75.65, lr: 0.0010, time: 13.76
[004750] f: 0.552, acc: 80.91, fv: 0.687, accv: 77.30, lr: 0.0010, time: 13.74
[005250] f: 0.472, acc: 83.69, fv: 0.576, accv: 80.28, lr: 0.0010, time: 13.78
[005750] f: 0.438, acc: 84.88, fv: 0.572, accv: 81.26, lr: 0.0010, time: 13.83
[006250] f: 0.553, acc: 80.55, fv: 0.731, accv: 76.63, lr: 0.0010, time: 13.85
[006750] f: 0.556, acc: 80.56, fv: 0.696, accv: 77.13, lr: 0.0010, time: 13.78
[007250] f: 0.473, acc: 82.95, fv: 0.617, accv: 78.99, lr: 0.0009, time: 13.71
[007750] f: 0.378, acc: 86.88, fv: 0.537, accv: 82.73, lr: 0.0009, time: 13.74
[008250] f: 0.465, acc: 83.21, fv: 0.609, accv: 79.97, lr: 0.0009, time: 13.85
[008750] f: 0.485, acc: 82.78, fv: 0.669, accv: 78.42, lr: 0.0009, time: 14.22
[011500] f: 0.345, acc: 88.08, fv: 0.552, accv: 82.41, lr: 0.0009, time: 14.33
[015250] f: 0.411, acc: 85.74, fv: 0.684, accv: 80.00, lr: 0.0008, time: 14.46
[019000] f: 0.246, acc: 91.13, fv: 0.530, accv: 84.49, lr: 0.0007, time: 14.42
[022750] f: 0.237, acc: 91.28, fv: 0.587, accv: 83.21, lr: 0.0006, time: 14.35
[026500] f: 0.158, acc: 94.45, fv: 0.508, accv: 85.56, lr: 0.0005, time: 14.42
[030250] f: 0.121, acc: 95.98, fv: 0.488, accv: 86.17, lr: 0.0003, time: 14.41
[034000] f: 0.104, acc: 96.54, fv: 0.498, accv: 86.42, lr: 0.0002, time: 14.43
[037750] f: 0.082, acc: 97.54, fv: 0.504, accv: 86.60, lr: 0.0001, time: 14.53
[041500] f: 0.071, acc: 98.05, fv: 0.503, accv: 86.65, lr: 0.0001, time: 14.44
[045250] f: 0.063, acc: 98.27, fv: 0.505, accv: 86.57, lr: 0.0000, time: 14.49
[049000] f: 0.063, acc: 98.29, fv: 0.503, accv: 86.63, lr: 0.0000, time: 14.58
[050000] f: 0.063, acc: 98.30, fv: 0.501, accv: 86.67, lr: 0.0000, time: 13.79
100%|█████████▉| 199/200 [1:24:20<00:18, 18.74s/it]100%|██████████| 200/200 [1:24:48<00:00, 21.72s/it]100%|██████████| 200/200 [1:24:48<00:00, 25.44s/it]
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.297, acc: 12.16, fv: 2.296, accv: 12.65, lr: 0.0010, time: 14.11
[000001] f: 2.292, acc: 14.13, fv: 2.290, accv: 15.33, lr: 0.0010, time: 13.35
[000063] f: 1.974, acc: 27.93, fv: 2.021, accv: 27.74, lr: 0.0010, time: 13.44
[000125] f: 1.504, acc: 43.55, fv: 1.500, accv: 43.75, lr: 0.0010, time: 13.43
[000187] f: 1.346, acc: 51.08, fv: 1.327, accv: 51.98, lr: 0.0010, time: 13.51
[000249] f: 1.465, acc: 45.58, fv: 1.564, accv: 42.45, lr: 0.0010, time: 13.45
[000250] f: 1.440, acc: 46.90, fv: 1.523, accv: 44.10, lr: 0.0010, time: 13.48
[000251] f: 1.347, acc: 51.07, fv: 1.398, accv: 49.06, lr: 0.0010, time: 13.46
[000313] f: 1.574, acc: 44.41, fv: 1.525, accv: 46.95, lr: 0.0010, time: 13.54
[000375] f: 1.312, acc: 53.34, fv: 1.339, accv: 52.46, lr: 0.0010, time: 13.52
[000437] f: 1.238, acc: 54.91, fv: 1.207, accv: 56.13, lr: 0.0010, time: 13.62
[000499] f: 1.365, acc: 50.68, fv: 1.359, accv: 51.50, lr: 0.0010, time: 13.49
[000501] f: 1.196, acc: 56.83, fv: 1.189, accv: 57.47, lr: 0.0010, time: 13.53
[000563] f: 1.178, acc: 58.38, fv: 1.261, accv: 55.88, lr: 0.0010, time: 13.53
[000625] f: 1.258, acc: 56.97, fv: 1.333, accv: 56.20, lr: 0.0010, time: 13.67
[000687] f: 1.184, acc: 58.94, fv: 1.235, accv: 58.11, lr: 0.0010, time: 13.53
[000749] f: 1.174, acc: 58.30, fv: 1.199, accv: 58.76, lr: 0.0010, time: 13.54
[000751] f: 1.139, acc: 59.47, fv: 1.150, accv: 60.33, lr: 0.0010, time: 13.62
[000813] f: 0.962, acc: 65.87, fv: 0.953, accv: 65.63, lr: 0.0010, time: 13.54
[000875] f: 0.904, acc: 67.78, fv: 0.932, accv: 66.67, lr: 0.0010, time: 13.55
[000937] f: 0.968, acc: 65.60, fv: 0.999, accv: 64.44, lr: 0.0010, time: 13.53
[000999] f: 0.925, acc: 67.49, fv: 0.964, accv: 66.15, lr: 0.0010, time: 13.53
[001001] f: 0.865, acc: 69.40, fv: 0.882, accv: 68.99, lr: 0.0010, time: 13.56
[001063] f: 0.937, acc: 67.11, fv: 0.988, accv: 65.43, lr: 0.0010, time: 13.51
[001125] f: 0.849, acc: 69.35, fv: 0.875, accv: 68.68, lr: 0.0010, time: 13.53
[001187] f: 1.280, acc: 58.00, fv: 1.423, accv: 57.05, lr: 0.0010, time: 13.54
[001249] f: 1.024, acc: 64.52, fv: 1.118, accv: 62.61, lr: 0.0010, time: 13.52
[001750] f: 0.930, acc: 69.13, fv: 1.020, accv: 67.13, lr: 0.0010, time: 13.72
[002250] f: 0.659, acc: 77.09, fv: 0.731, accv: 74.52, lr: 0.0010, time: 13.76
[002750] f: 0.663, acc: 77.26, fv: 0.770, accv: 74.92, lr: 0.0010, time: 13.72
[003250] f: 0.603, acc: 78.45, fv: 0.713, accv: 75.18, lr: 0.0010, time: 13.71
[003750] f: 0.535, acc: 81.51, fv: 0.625, accv: 78.71, lr: 0.0010, time: 13.83
[004000] f: 0.529, acc: 81.72, fv: 0.637, accv: 78.65, lr: 0.0010, time: 13.98
[004250] f: 0.650, acc: 77.47, fv: 0.774, accv: 75.22, lr: 0.0010, time: 14.05
[004750] f: 0.532, acc: 81.48, fv: 0.651, accv: 78.38, lr: 0.0010, time: 14.11
[005250] f: 0.480, acc: 83.22, fv: 0.571, accv: 80.73, lr: 0.0010, time: 14.41
[005750] f: 0.458, acc: 84.15, fv: 0.591, accv: 80.09, lr: 0.0010, time: 14.64
[006250] f: 0.467, acc: 83.75, fv: 0.594, accv: 80.00, lr: 0.0010, time: 14.54
[006750] f: 0.492, acc: 82.95, fv: 0.657, accv: 78.82, lr: 0.0010, time: 14.64
[007250] f: 0.432, acc: 84.81, fv: 0.584, accv: 81.16, lr: 0.0009, time: 14.40
[007750] f: 0.412, acc: 85.66, fv: 0.560, accv: 81.52, lr: 0.0009, time: 14.20
[008250] f: 0.461, acc: 83.66, fv: 0.602, accv: 80.07, lr: 0.0009, time: 14.12
[008750] f: 0.423, acc: 85.47, fv: 0.586, accv: 80.74, lr: 0.0009, time: 14.00
[011500] f: 0.369, acc: 87.06, fv: 0.568, accv: 82.06, lr: 0.0009, time: 14.55
[015250] f: 0.363, acc: 87.08, fv: 0.651, accv: 80.70, lr: 0.0008, time: 14.32
[019000] f: 0.259, acc: 90.94, fv: 0.539, accv: 84.06, lr: 0.0007, time: 14.47
[022750] f: 0.203, acc: 93.02, fv: 0.531, accv: 84.95, lr: 0.0006, time: 14.68
[026500] f: 0.136, acc: 95.45, fv: 0.479, accv: 86.13, lr: 0.0005, time: 14.71
[030250] f: 0.121, acc: 96.00, fv: 0.505, accv: 85.90, lr: 0.0003, time: 14.63
[034000] f: 0.099, acc: 96.82, fv: 0.492, accv: 86.35, lr: 0.0002, time: 14.74
[037750] f: 0.089, acc: 97.19, fv: 0.509, accv: 86.69, lr: 0.0001, time: 14.72
[041500] f: 0.069, acc: 98.03, fv: 0.504, accv: 86.81, lr: 0.0001, time: 14.93
[045250] f: 0.062, acc: 98.43, fv: 0.500, accv: 87.03, lr: 0.0000, time: 14.59
[049000] f: 0.061, acc: 98.38, fv: 0.499, accv: 87.06, lr: 0.0000, time: 14.51
[050000] f: 0.061, acc: 98.39, fv: 0.499, accv: 87.15, lr: 0.0000, time: 14.10
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:41<5:35:43, 101.23s/it]  1%|          | 2/200 [03:08<5:07:06, 93.06s/it]   2%|▏         | 3/200 [04:35<4:55:45, 90.08s/it]  2%|▏         | 4/200 [05:59<4:47:15, 87.94s/it]  2%|▎         | 5/200 [07:22<4:40:00, 86.15s/it]  3%|▎         | 6/200 [07:42<3:25:54, 63.68s/it]  4%|▎         | 7/200 [08:16<2:53:30, 53.94s/it]  4%|▍         | 8/200 [08:37<2:18:56, 43.42s/it]  4%|▍         | 9/200 [09:11<2:09:03, 40.54s/it]  5%|▌         | 10/200 [09:32<1:48:34, 34.28s/it]  6%|▌         | 11/200 [10:06<1:48:05, 34.31s/it]  6%|▌         | 12/200 [10:26<1:33:32, 29.85s/it]  6%|▋         | 13/200 [11:00<1:37:25, 31.26s/it]  7%|▋         | 14/200 [11:19<1:25:22, 27.54s/it]  8%|▊         | 15/200 [11:54<1:32:13, 29.91s/it]  8%|▊         | 16/200 [12:27<1:34:18, 30.75s/it]  8%|▊         | 17/200 [12:59<1:35:12, 31.22s/it]  9%|▉         | 18/200 [13:16<1:21:24, 26.84s/it] 10%|▉         | 19/200 [13:52<1:29:12, 29.57s/it] 10%|█         | 20/200 [14:07<1:15:28, 25.16s/it] 10%|█         | 21/200 [14:43<1:25:01, 28.50s/it] 11%|█         | 22/200 [14:57<1:11:21, 24.05s/it] 12%|█▏        | 23/200 [15:33<1:21:33, 27.65s/it] 12%|█▏        | 24/200 [15:48<1:10:20, 23.98s/it] 12%|█▎        | 25/200 [16:22<1:18:32, 26.93s/it] 13%|█▎        | 26/200 [16:40<1:10:18, 24.25s/it] 14%|█▎        | 27/200 [17:12<1:16:12, 26.43s/it] 14%|█▍        | 28/200 [17:32<1:10:44, 24.68s/it] 14%|█▍        | 29/200 [18:01<1:13:54, 25.93s/it] 15%|█▌        | 30/200 [18:24<1:10:28, 24.87s/it] 16%|█▌        | 31/200 [18:52<1:12:43, 25.82s/it] 16%|█▌        | 32/200 [19:14<1:09:35, 24.85s/it] 16%|█▋        | 33/200 [19:44<1:13:10, 26.29s/it] 17%|█▋        | 34/200 [20:06<1:09:40, 25.18s/it] 18%|█▊        | 35/200 [20:37<1:13:47, 26.83s/it] 18%|█▊        | 36/200 [21:00<1:09:56, 25.59s/it] 18%|█▊        | 37/200 [21:17<1:02:48, 23.12s/it] 19%|█▉        | 38/200 [21:37<1:00:06, 22.26s/it] 20%|█▉        | 39/200 [22:00<1:00:04, 22.39s/it] 20%|██        | 40/200 [22:23<59:56, 22.48s/it]   20%|██        | 41/200 [22:45<59:41, 22.52s/it] 21%|██        | 42/200 [23:08<59:25, 22.57s/it] 22%|██▏       | 43/200 [23:31<59:10, 22.61s/it] 22%|██▏       | 44/200 [23:53<58:48, 22.62s/it] 22%|██▎       | 45/200 [24:16<58:31, 22.66s/it] 23%|██▎       | 46/200 [24:52<1:08:40, 26.75s/it] 24%|██▎       | 47/200 [25:15<1:05:07, 25.54s/it] 24%|██▍       | 48/200 [25:33<58:48, 23.21s/it]   24%|██▍       | 49/200 [25:53<55:55, 22.22s/it] 25%|██▌       | 50/200 [26:16<55:54, 22.36s/it] 26%|██▌       | 51/200 [26:38<55:44, 22.45s/it] 26%|██▌       | 52/200 [27:01<55:32, 22.52s/it] 26%|██▋       | 53/200 [27:23<55:10, 22.52s/it] 27%|██▋       | 54/200 [27:46<54:54, 22.57s/it] 28%|██▊       | 55/200 [28:09<54:36, 22.60s/it] 28%|██▊       | 56/200 [28:31<54:18, 22.63s/it] 28%|██▊       | 57/200 [28:54<53:57, 22.64s/it] 29%|██▉       | 58/200 [29:17<53:37, 22.66s/it] 30%|██▉       | 59/200 [29:39<53:12, 22.64s/it] 30%|███       | 60/200 [30:02<52:49, 22.64s/it] 30%|███       | 61/200 [30:38<1:01:38, 26.61s/it] 31%|███       | 62/200 [31:01<58:28, 25.43s/it]   32%|███▏      | 63/200 [31:19<53:08, 23.28s/it] 32%|███▏      | 64/200 [31:38<50:08, 22.12s/it] 32%|███▎      | 65/200 [32:01<50:09, 22.29s/it] 33%|███▎      | 66/200 [32:24<50:03, 22.41s/it] 34%|███▎      | 67/200 [32:46<49:51, 22.50s/it] 34%|███▍      | 68/200 [33:09<49:40, 22.58s/it] 34%|███▍      | 69/200 [33:32<49:22, 22.61s/it] 35%|███▌      | 70/200 [33:54<49:03, 22.64s/it] 36%|███▌      | 71/200 [34:17<48:41, 22.65s/it] 36%|███▌      | 72/200 [34:40<48:20, 22.66s/it] 36%|███▋      | 73/200 [35:03<48:00, 22.68s/it] 37%|███▋      | 74/200 [35:25<47:37, 22.68s/it] 38%|███▊      | 75/200 [35:48<47:14, 22.67s/it] 38%|███▊      | 76/200 [36:24<55:01, 26.62s/it] 38%|███▊      | 77/200 [36:46<52:05, 25.41s/it] 39%|███▉      | 78/200 [37:05<47:44, 23.48s/it] 40%|███▉      | 79/200 [37:24<44:33, 22.09s/it] 40%|████      | 80/200 [37:47<44:32, 22.27s/it] 40%|████      | 81/200 [38:09<44:22, 22.37s/it] 41%|████      | 82/200 [38:32<44:08, 22.44s/it] 42%|████▏     | 83/200 [38:55<43:53, 22.51s/it] 42%|████▏     | 84/200 [39:17<43:35, 22.55s/it] 42%|████▎     | 85/200 [39:40<43:17, 22.59s/it] 43%|████▎     | 86/200 [40:03<42:57, 22.61s/it] 44%|████▎     | 87/200 [40:25<42:35, 22.61s/it] 44%|████▍     | 88/200 [40:48<42:14, 22.63s/it] 44%|████▍     | 89/200 [41:11<41:52, 22.63s/it] 45%|████▌     | 90/200 [41:33<41:31, 22.65s/it] 46%|████▌     | 91/200 [42:09<48:20, 26.61s/it] 46%|████▌     | 92/200 [42:32<45:45, 25.42s/it] 46%|████▋     | 93/200 [42:51<42:11, 23.66s/it] 47%|████▋     | 94/200 [43:10<39:02, 22.10s/it] 48%|████▊     | 95/200 [43:33<39:00, 22.29s/it] 48%|████▊     | 96/200 [43:55<38:50, 22.41s/it] 48%|████▊     | 97/200 [44:18<38:35, 22.48s/it] 49%|████▉     | 98/200 [44:41<38:17, 22.53s/it] 50%|████▉     | 99/200 [45:03<37:59, 22.57s/it] 50%|█████     | 100/200 [45:26<37:40, 22.60s/it] 50%|█████     | 101/200 [45:49<37:18, 22.61s/it] 51%|█████     | 102/200 [46:11<36:57, 22.63s/it] 52%|█████▏    | 103/200 [46:34<36:37, 22.65s/it] 52%|█████▏    | 104/200 [46:57<36:16, 22.67s/it] 52%|█████▎    | 105/200 [47:19<35:52, 22.66s/it] 53%|█████▎    | 106/200 [47:55<41:37, 26.57s/it] 54%|█████▎    | 107/200 [48:18<39:21, 25.39s/it] 54%|█████▍    | 108/200 [48:38<36:28, 23.79s/it] 55%|█████▍    | 109/200 [48:56<33:32, 22.11s/it] 55%|█████▌    | 110/200 [49:19<33:25, 22.28s/it] 56%|█████▌    | 111/200 [49:41<33:13, 22.40s/it] 56%|█████▌    | 112/200 [50:04<32:58, 22.48s/it] 56%|█████▋    | 113/200 [50:27<32:40, 22.53s/it] 57%|█████▋    | 114/200 [50:49<32:20, 22.57s/it] 57%|█████▊    | 115/200 [51:12<32:02, 22.62s/it] 58%|█████▊    | 116/200 [51:35<31:41, 22.64s/it] 58%|█████▊    | 117/200 [51:57<31:19, 22.64s/it] 59%|█████▉    | 118/200 [52:20<30:57, 22.65s/it] 60%|█████▉    | 119/200 [52:43<30:33, 22.64s/it] 60%|██████    | 120/200 [53:05<30:13, 22.67s/it] 60%|██████    | 121/200 [53:41<34:51, 26.48s/it] 61%|██████    | 122/200 [54:03<32:57, 25.35s/it] 62%|██████▏   | 123/200 [54:24<30:36, 23.85s/it] 62%|██████▏   | 124/200 [54:42<28:01, 22.13s/it] 62%|██████▎   | 125/200 [55:04<27:52, 22.30s/it] 63%|██████▎   | 126/200 [55:27<27:37, 22.40s/it] 64%|██████▎   | 127/200 [55:50<27:21, 22.49s/it] 64%|██████▍   | 128/200 [56:12<27:03, 22.55s/it] 64%|██████▍   | 129/200 [56:35<26:43, 22.58s/it] 65%|██████▌   | 130/200 [56:58<26:22, 22.61s/it] 66%|██████▌   | 131/200 [57:21<26:01, 22.63s/it] 66%|██████▌   | 132/200 [57:43<25:39, 22.64s/it] 66%|██████▋   | 133/200 [58:06<25:16, 22.63s/it] 67%|██████▋   | 134/200 [58:29<24:55, 22.66s/it] 68%|██████▊   | 135/200 [58:51<24:33, 22.67s/it] 68%|██████▊   | 136/200 [59:27<28:14, 26.47s/it] 68%|██████▊   | 137/200 [59:49<26:35, 25.32s/it] 69%|██████▉   | 138/200 [1:00:10<24:41, 23.90s/it] 70%|██████▉   | 139/200 [1:00:28<22:28, 22.11s/it] 70%|███████   | 140/200 [1:00:50<22:16, 22.27s/it] 70%|███████   | 141/200 [1:01:13<22:00, 22.39s/it] 71%|███████   | 142/200 [1:01:36<21:43, 22.47s/it] 72%|███████▏  | 143/200 [1:01:58<21:23, 22.53s/it] 72%|███████▏  | 144/200 [1:02:21<21:03, 22.57s/it] 72%|███████▎  | 145/200 [1:02:44<20:41, 22.57s/it] 73%|███████▎  | 146/200 [1:03:06<20:20, 22.61s/it] 74%|███████▎  | 147/200 [1:03:29<19:59, 22.63s/it] 74%|███████▍  | 148/200 [1:03:52<19:37, 22.64s/it] 74%|███████▍  | 149/200 [1:04:14<19:14, 22.64s/it] 75%|███████▌  | 150/200 [1:04:37<18:51, 22.63s/it] 76%|███████▌  | 151/200 [1:05:12<21:34, 26.43s/it] 76%|███████▌  | 152/200 [1:05:35<20:14, 25.31s/it] 76%|███████▋  | 153/200 [1:05:55<18:43, 23.90s/it] 77%|███████▋  | 154/200 [1:06:13<16:56, 22.10s/it] 78%|███████▊  | 155/200 [1:06:36<16:40, 22.23s/it] 78%|███████▊  | 156/200 [1:06:59<16:23, 22.36s/it] 78%|███████▊  | 157/200 [1:07:21<16:06, 22.47s/it] 79%|███████▉  | 158/200 [1:07:44<15:46, 22.53s/it] 80%|███████▉  | 159/200 [1:08:07<15:25, 22.58s/it] 80%|████████  | 160/200 [1:08:29<15:04, 22.60s/it] 80%|████████  | 161/200 [1:08:52<14:42, 22.63s/it] 81%|████████  | 162/200 [1:09:15<14:21, 22.66s/it] 82%|████████▏ | 163/200 [1:09:37<13:58, 22.65s/it] 82%|████████▏ | 164/200 [1:10:00<13:35, 22.66s/it] 82%|████████▎ | 165/200 [1:10:23<13:13, 22.66s/it] 83%|████████▎ | 166/200 [1:10:58<15:02, 26.54s/it] 84%|████████▎ | 167/200 [1:11:21<13:56, 25.36s/it] 84%|████████▍ | 168/200 [1:11:41<12:45, 23.94s/it] 84%|████████▍ | 169/200 [1:12:00<11:27, 22.17s/it] 85%|████████▌ | 170/200 [1:12:22<11:06, 22.21s/it] 86%|████████▌ | 171/200 [1:12:45<10:48, 22.35s/it] 86%|████████▌ | 172/200 [1:13:07<10:28, 22.45s/it] 86%|████████▋ | 173/200 [1:13:30<10:08, 22.52s/it] 87%|████████▋ | 174/200 [1:13:53<09:46, 22.58s/it] 88%|████████▊ | 175/200 [1:14:15<09:24, 22.59s/it] 88%|████████▊ | 176/200 [1:14:38<09:02, 22.61s/it] 88%|████████▊ | 177/200 [1:15:00<08:39, 22.59s/it] 89%|████████▉ | 178/200 [1:15:23<08:17, 22.60s/it] 90%|████████▉ | 179/200 [1:15:46<07:55, 22.64s/it] 90%|█████████ | 180/200 [1:16:08<07:32, 22.65s/it] 90%|█████████ | 181/200 [1:16:44<08:21, 26.41s/it] 91%|█████████ | 182/200 [1:17:06<07:35, 25.29s/it] 92%|█████████▏| 183/200 [1:17:27<06:46, 23.92s/it] 92%|█████████▏| 184/200 [1:17:45<05:56, 22.25s/it] 92%|█████████▎| 185/200 [1:18:07<05:33, 22.21s/it] 93%|█████████▎| 186/200 [1:18:30<05:13, 22.37s/it] 94%|█████████▎| 187/200 [1:18:53<04:52, 22.48s/it] 94%|█████████▍| 188/200 [1:19:16<04:30, 22.53s/it] 94%|█████████▍| 189/200 [1:19:38<04:08, 22.58s/it] 95%|█████████▌| 190/200 [1:20:01<03:46, 22.62s/it] 96%|█████████▌| 191/200 [1:20:24<03:23, 22.64s/it] 96%|█████████▌| 192/200 [1:20:46<03:01, 22.65s/it] 96%|█████████▋| 193/200 [1:21:09<02:38, 22.67s/it] 97%|█████████▋| 194/200 [1:21:32<02:15, 22.67s/it] 98%|█████████▊| 195/200 [1:21:54<01:53, 22.66s/it] 98%|█████████▊| 196/200 [1:22:30<01:45, 26.44s/it] 98%|█████████▊| 197/200 [1:22:52<01:15, 25.31s/it] 99%|█████████▉| 198/200 [1:23:13<00:47, 23.91s/it]100%|█████████▉| 199/200 [1:23:32<00:22, 22.36s/it]100%|██████████| 200/200 [1:24:04<00:00, 25.26s/it]100%|██████████| 200/200 [1:24:04<00:00, 25.22s/it]
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.303, acc: 10.50, fv: 2.303, accv: 10.43, lr: 0.0010, time: 14.11
[000001] f: 2.301, acc: 10.50, fv: 2.301, accv: 11.52, lr: 0.0010, time: 13.31
[000063] f: 1.682, acc: 34.83, fv: 1.636, accv: 36.61, lr: 0.0010, time: 13.45
[000125] f: 1.580, acc: 42.35, fv: 1.626, accv: 41.32, lr: 0.0010, time: 13.51
[000187] f: 1.458, acc: 48.57, fv: 1.472, accv: 48.21, lr: 0.0010, time: 13.50
[000249] f: 1.445, acc: 46.50, fv: 1.493, accv: 44.71, lr: 0.0010, time: 13.45
[000250] f: 1.460, acc: 45.40, fv: 1.507, accv: 43.89, lr: 0.0010, time: 13.48
[000251] f: 1.426, acc: 46.89, fv: 1.454, accv: 46.10, lr: 0.0010, time: 13.48
[000313] f: 1.667, acc: 41.43, fv: 1.690, accv: 42.47, lr: 0.0010, time: 13.54
[000375] f: 1.151, acc: 59.16, fv: 1.163, accv: 57.91, lr: 0.0010, time: 13.53
[000437] f: 1.187, acc: 56.33, fv: 1.224, accv: 55.66, lr: 0.0010, time: 13.64
[000499] f: 1.313, acc: 52.95, fv: 1.442, accv: 50.01, lr: 0.0010, time: 13.53
[000501] f: 1.090, acc: 61.10, fv: 1.109, accv: 60.17, lr: 0.0010, time: 13.53
[000563] f: 1.241, acc: 55.28, fv: 1.246, accv: 55.74, lr: 0.0010, time: 13.58
[000625] f: 1.112, acc: 60.86, fv: 1.166, accv: 60.40, lr: 0.0010, time: 13.65
[000687] f: 1.177, acc: 57.64, fv: 1.237, accv: 56.50, lr: 0.0010, time: 13.66
[000749] f: 1.382, acc: 53.12, fv: 1.440, accv: 52.27, lr: 0.0010, time: 13.58
[000751] f: 1.149, acc: 59.34, fv: 1.200, accv: 58.14, lr: 0.0010, time: 13.67
[000813] f: 0.986, acc: 64.35, fv: 1.006, accv: 63.61, lr: 0.0010, time: 13.56
[000875] f: 1.080, acc: 61.57, fv: 1.133, accv: 60.32, lr: 0.0010, time: 13.69
[000937] f: 0.978, acc: 64.64, fv: 1.044, accv: 64.06, lr: 0.0010, time: 13.57
[000999] f: 0.876, acc: 69.11, fv: 0.920, accv: 67.84, lr: 0.0010, time: 13.54
[001001] f: 0.892, acc: 68.77, fv: 0.936, accv: 67.49, lr: 0.0010, time: 13.66
[001063] f: 0.893, acc: 67.76, fv: 0.933, accv: 66.57, lr: 0.0010, time: 13.59
[001125] f: 0.893, acc: 68.80, fv: 0.920, accv: 67.99, lr: 0.0010, time: 13.59
[001187] f: 0.930, acc: 67.97, fv: 0.968, accv: 67.33, lr: 0.0010, time: 13.61
[001249] f: 0.930, acc: 67.48, fv: 0.975, accv: 66.49, lr: 0.0010, time: 13.68
[001750] f: 0.806, acc: 71.89, fv: 0.881, accv: 69.82, lr: 0.0010, time: 13.81
[002250] f: 0.699, acc: 75.75, fv: 0.787, accv: 73.19, lr: 0.0010, time: 13.89
[002750] f: 0.613, acc: 78.63, fv: 0.731, accv: 75.02, lr: 0.0010, time: 13.75
[003250] f: 0.553, acc: 80.49, fv: 0.630, accv: 78.27, lr: 0.0010, time: 13.72
[003750] f: 0.698, acc: 75.51, fv: 0.800, accv: 73.55, lr: 0.0010, time: 13.65
[004000] f: 0.587, acc: 79.87, fv: 0.676, accv: 77.60, lr: 0.0010, time: 13.62
[004250] f: 0.645, acc: 77.96, fv: 0.762, accv: 75.44, lr: 0.0010, time: 13.76
[004750] f: 0.461, acc: 84.01, fv: 0.567, accv: 80.79, lr: 0.0010, time: 13.72
[005250] f: 0.534, acc: 81.18, fv: 0.653, accv: 78.29, lr: 0.0010, time: 13.75
[005750] f: 0.406, acc: 85.84, fv: 0.514, accv: 82.33, lr: 0.0010, time: 13.79
[006250] f: 0.526, acc: 81.29, fv: 0.667, accv: 78.16, lr: 0.0010, time: 13.91
[006750] f: 0.481, acc: 83.13, fv: 0.588, accv: 80.48, lr: 0.0010, time: 13.77
[007250] f: 0.401, acc: 86.16, fv: 0.535, accv: 82.21, lr: 0.0009, time: 13.83
[007750] f: 0.443, acc: 84.71, fv: 0.627, accv: 80.28, lr: 0.0009, time: 13.99
[008250] f: 0.404, acc: 85.68, fv: 0.568, accv: 81.64, lr: 0.0009, time: 13.97
[008750] f: 0.347, acc: 88.05, fv: 0.502, accv: 83.60, lr: 0.0009, time: 14.22
[011500] f: 0.334, acc: 88.38, fv: 0.549, accv: 83.15, lr: 0.0009, time: 14.17
[015250] f: 0.272, acc: 90.20, fv: 0.521, accv: 83.85, lr: 0.0008, time: 14.61
[019000] f: 0.241, acc: 91.28, fv: 0.545, accv: 83.72, lr: 0.0007, time: 14.57
[022750] f: 0.224, acc: 91.93, fv: 0.562, accv: 84.13, lr: 0.0006, time: 14.66
[026500] f: 0.174, acc: 93.84, fv: 0.538, accv: 85.61, lr: 0.0005, time: 14.78
[030250] f: 0.122, acc: 95.83, fv: 0.508, accv: 86.23, lr: 0.0003, time: 14.76
[034000] f: 0.093, acc: 97.08, fv: 0.498, accv: 86.54, lr: 0.0002, time: 14.54
[037750] f: 0.076, acc: 97.65, fv: 0.496, accv: 86.67, lr: 0.0001, time: 14.76
[041500] f: 0.064, acc: 98.27, fv: 0.496, accv: 86.79, lr: 0.0001, time: 14.65
[045250] f: 0.057, acc: 98.53, fv: 0.492, accv: 87.25, lr: 0.0000, time: 14.52
[049000] f: 0.057, acc: 98.51, fv: 0.489, accv: 87.18, lr: 0.0000, time: 14.69
[050000] f: 0.056, acc: 98.57, fv: 0.490, accv: 87.23, lr: 0.0000, time: 14.00
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:41<5:35:23, 101.12s/it]  1%|          | 2/200 [03:08<5:07:03, 93.05s/it]   2%|▏         | 3/200 [04:35<4:55:59, 90.15s/it]  2%|▏         | 4/200 [06:00<4:47:46, 88.09s/it]  2%|▎         | 5/200 [07:23<4:41:02, 86.47s/it]  3%|▎         | 6/200 [07:44<3:27:05, 64.05s/it]  4%|▎         | 7/200 [08:17<2:54:07, 54.13s/it]  4%|▍         | 8/200 [08:38<2:19:28, 43.58s/it]  4%|▍         | 9/200 [09:13<2:09:15, 40.61s/it]  5%|▌         | 10/200 [09:33<1:48:59, 34.42s/it]  6%|▌         | 11/200 [10:07<1:48:16, 34.37s/it]  6%|▌         | 12/200 [10:28<1:34:19, 30.10s/it]  6%|▋         | 13/200 [11:02<1:38:00, 31.45s/it]  7%|▋         | 14/200 [11:22<1:26:47, 27.99s/it]  8%|▊         | 15/200 [11:57<1:33:00, 30.16s/it]  8%|▊         | 16/200 [12:31<1:35:56, 31.29s/it]  8%|▊         | 17/200 [13:05<1:37:56, 32.11s/it]  9%|▉         | 18/200 [13:25<1:26:18, 28.45s/it] 10%|▉         | 19/200 [14:00<1:31:51, 30.45s/it] 10%|█         | 20/200 [14:21<1:22:02, 27.35s/it] 10%|█         | 21/200 [14:55<1:28:03, 29.52s/it] 11%|█         | 22/200 [15:16<1:19:32, 26.81s/it] 12%|█▏        | 23/200 [15:50<1:25:53, 29.12s/it] 12%|█▏        | 24/200 [16:10<1:17:37, 26.46s/it] 12%|█▎        | 25/200 [16:45<1:24:38, 29.02s/it] 13%|█▎        | 26/200 [17:05<1:15:55, 26.18s/it] 14%|█▎        | 27/200 [17:41<1:23:37, 29.00s/it] 14%|█▍        | 28/200 [17:59<1:14:16, 25.91s/it] 14%|█▍        | 29/200 [18:35<1:22:41, 29.02s/it] 15%|█▌        | 30/200 [18:53<1:12:23, 25.55s/it] 16%|█▌        | 31/200 [19:30<1:21:18, 28.87s/it] 16%|█▌        | 32/200 [19:47<1:10:51, 25.31s/it] 16%|█▋        | 33/200 [20:23<1:19:53, 28.70s/it] 17%|█▋        | 34/200 [20:39<1:08:37, 24.80s/it] 18%|█▊        | 35/200 [21:16<1:18:09, 28.42s/it] 18%|█▊        | 36/200 [21:35<1:10:31, 25.80s/it] 18%|█▊        | 37/200 [21:58<1:07:34, 24.87s/it] 19%|█▉        | 38/200 [22:21<1:05:22, 24.21s/it] 20%|█▉        | 39/200 [22:43<1:03:42, 23.74s/it] 20%|██        | 40/200 [23:06<1:02:25, 23.41s/it] 20%|██        | 41/200 [23:29<1:01:28, 23.20s/it] 21%|██        | 42/200 [23:51<1:00:40, 23.04s/it] 22%|██▏       | 43/200 [24:14<1:00:00, 22.93s/it] 22%|██▏       | 44/200 [24:36<58:52, 22.64s/it]   22%|██▎       | 45/200 [24:55<55:19, 21.42s/it] 23%|██▎       | 46/200 [25:32<1:06:50, 26.05s/it] 24%|██▎       | 47/200 [25:51<1:01:15, 24.02s/it] 24%|██▍       | 48/200 [26:13<59:48, 23.61s/it]   24%|██▍       | 49/200 [26:36<58:42, 23.33s/it] 25%|██▌       | 50/200 [26:59<57:49, 23.13s/it] 26%|██▌       | 51/200 [27:22<57:08, 23.01s/it] 26%|██▌       | 52/200 [27:44<56:28, 22.89s/it] 26%|██▋       | 53/200 [28:07<55:52, 22.81s/it] 27%|██▋       | 54/200 [28:29<55:23, 22.76s/it] 28%|██▊       | 55/200 [28:52<54:56, 22.73s/it] 28%|██▊       | 56/200 [29:15<54:30, 22.71s/it] 28%|██▊       | 57/200 [29:37<54:05, 22.69s/it] 29%|██▉       | 58/200 [30:00<53:41, 22.69s/it] 30%|██▉       | 59/200 [30:22<52:41, 22.42s/it] 30%|███       | 60/200 [30:41<49:57, 21.41s/it] 30%|███       | 61/200 [31:18<1:00:38, 26.17s/it] 31%|███       | 62/200 [31:37<55:16, 24.03s/it]   32%|███▏      | 63/200 [32:00<53:55, 23.62s/it] 32%|███▏      | 64/200 [32:23<52:53, 23.34s/it] 32%|███▎      | 65/200 [32:45<52:03, 23.14s/it] 33%|███▎      | 66/200 [33:08<51:20, 22.99s/it] 34%|███▎      | 67/200 [33:31<50:43, 22.89s/it] 34%|███▍      | 68/200 [33:53<50:12, 22.82s/it] 34%|███▍      | 69/200 [34:16<49:41, 22.76s/it] 35%|███▌      | 70/200 [34:39<49:16, 22.74s/it] 36%|███▌      | 71/200 [35:01<48:48, 22.70s/it] 36%|███▌      | 72/200 [35:24<48:23, 22.68s/it] 36%|███▋      | 73/200 [35:46<47:58, 22.67s/it] 37%|███▋      | 74/200 [36:08<46:51, 22.31s/it] 38%|███▊      | 75/200 [36:27<44:42, 21.46s/it] 38%|███▊      | 76/200 [37:05<54:09, 26.20s/it] 38%|███▊      | 77/200 [37:23<49:02, 23.92s/it] 39%|███▉      | 78/200 [37:46<47:52, 23.55s/it] 40%|███▉      | 79/200 [38:09<46:58, 23.29s/it] 40%|████      | 80/200 [38:31<46:11, 23.09s/it] 40%|████      | 81/200 [38:54<45:31, 22.96s/it] 41%|████      | 82/200 [39:17<44:58, 22.87s/it] 42%|████▏     | 83/200 [39:39<44:27, 22.80s/it] 42%|████▏     | 84/200 [40:02<44:00, 22.77s/it] 42%|████▎     | 85/200 [40:25<43:35, 22.74s/it] 43%|████▎     | 86/200 [40:47<43:09, 22.71s/it] 44%|████▎     | 87/200 [41:10<42:44, 22.70s/it] 44%|████▍     | 88/200 [41:33<42:21, 22.69s/it] 44%|████▍     | 89/200 [41:54<41:11, 22.26s/it] 45%|████▌     | 90/200 [42:13<39:23, 21.49s/it] 46%|████▌     | 91/200 [42:51<47:39, 26.23s/it] 46%|████▌     | 92/200 [43:09<42:53, 23.82s/it] 46%|████▋     | 93/200 [43:32<41:50, 23.46s/it] 47%|████▋     | 94/200 [43:54<40:59, 23.20s/it] 48%|████▊     | 95/200 [44:17<40:19, 23.05s/it] 48%|████▊     | 96/200 [44:39<39:42, 22.91s/it] 48%|████▊     | 97/200 [45:02<39:12, 22.84s/it] 49%|████▉     | 98/200 [45:25<38:45, 22.80s/it] 50%|████▉     | 99/200 [45:47<38:18, 22.75s/it] 50%|█████     | 100/200 [46:10<37:49, 22.70s/it] 50%|█████     | 101/200 [46:33<37:27, 22.70s/it] 51%|█████     | 102/200 [46:55<37:04, 22.70s/it] 52%|█████▏    | 103/200 [47:18<36:40, 22.69s/it] 52%|█████▏    | 104/200 [47:39<35:30, 22.19s/it] 52%|█████▎    | 105/200 [47:59<34:06, 21.54s/it] 53%|█████▎    | 106/200 [48:37<41:12, 26.31s/it] 54%|█████▎    | 107/200 [48:55<36:52, 23.79s/it] 54%|█████▍    | 108/200 [49:17<35:55, 23.43s/it] 55%|█████▍    | 109/200 [49:40<35:11, 23.20s/it] 55%|█████▌    | 110/200 [50:02<34:31, 23.02s/it] 56%|█████▌    | 111/200 [50:25<34:01, 22.93s/it] 56%|█████▌    | 112/200 [50:48<33:31, 22.86s/it] 56%|█████▋    | 113/200 [51:10<33:02, 22.79s/it] 57%|█████▋    | 114/200 [51:33<32:34, 22.72s/it] 57%|█████▊    | 115/200 [51:56<32:09, 22.70s/it] 58%|█████▊    | 116/200 [52:18<31:46, 22.69s/it] 58%|█████▊    | 117/200 [52:41<31:21, 22.67s/it] 59%|█████▉    | 118/200 [53:04<30:57, 22.65s/it] 60%|█████▉    | 119/200 [53:24<29:51, 22.12s/it] 60%|██████    | 120/200 [53:45<28:47, 21.60s/it] 60%|██████    | 121/200 [54:22<34:38, 26.31s/it] 61%|██████    | 122/200 [54:40<30:52, 23.75s/it] 62%|██████▏   | 123/200 [55:03<30:02, 23.41s/it] 62%|██████▏   | 124/200 [55:25<29:20, 23.17s/it] 62%|██████▎   | 125/200 [55:48<28:45, 23.01s/it] 63%|██████▎   | 126/200 [56:10<28:13, 22.89s/it] 64%|██████▎   | 127/200 [56:33<27:46, 22.83s/it] 64%|██████▍   | 128/200 [56:56<27:19, 22.77s/it] 64%|██████▍   | 129/200 [57:18<26:55, 22.75s/it] 65%|██████▌   | 130/200 [57:41<26:29, 22.71s/it] 66%|██████▌   | 131/200 [58:04<26:05, 22.69s/it] 66%|██████▌   | 132/200 [58:26<25:42, 22.69s/it] 66%|██████▋   | 133/200 [58:49<25:19, 22.68s/it] 67%|██████▋   | 134/200 [59:10<24:18, 22.10s/it] 68%|██████▊   | 135/200 [59:30<23:24, 21.60s/it] 68%|██████▊   | 136/200 [1:00:07<28:02, 26.29s/it] 68%|██████▊   | 137/200 [1:00:25<24:59, 23.81s/it] 69%|██████▉   | 138/200 [1:00:48<24:10, 23.39s/it] 70%|██████▉   | 139/200 [1:01:10<23:33, 23.16s/it] 70%|███████   | 140/200 [1:01:33<23:01, 23.02s/it] 70%|███████   | 141/200 [1:01:56<22:31, 22.91s/it] 71%|███████   | 142/200 [1:02:18<22:03, 22.82s/it] 72%|███████▏  | 143/200 [1:02:41<21:37, 22.76s/it] 72%|███████▏  | 144/200 [1:03:04<21:12, 22.72s/it] 72%|███████▎  | 145/200 [1:03:26<20:48, 22.70s/it] 73%|███████▎  | 146/200 [1:03:49<20:25, 22.70s/it] 74%|███████▎  | 147/200 [1:04:12<20:01, 22.68s/it] 74%|███████▍  | 148/200 [1:04:34<19:36, 22.63s/it] 74%|███████▍  | 149/200 [1:04:55<18:44, 22.05s/it] 75%|███████▌  | 150/200 [1:05:15<17:59, 21.58s/it] 76%|███████▌  | 151/200 [1:05:53<21:30, 26.33s/it] 76%|███████▌  | 152/200 [1:06:11<19:05, 23.87s/it] 76%|███████▋  | 153/200 [1:06:33<18:17, 23.35s/it] 77%|███████▋  | 154/200 [1:06:56<17:43, 23.12s/it] 78%|███████▊  | 155/200 [1:07:18<17:14, 22.98s/it] 78%|███████▊  | 156/200 [1:07:41<16:47, 22.89s/it] 78%|███████▊  | 157/200 [1:08:04<16:21, 22.83s/it] 79%|███████▉  | 158/200 [1:08:26<15:55, 22.76s/it] 80%|███████▉  | 159/200 [1:08:49<15:32, 22.73s/it] 80%|████████  | 160/200 [1:09:12<15:07, 22.70s/it] 80%|████████  | 161/200 [1:09:34<14:44, 22.67s/it] 81%|████████  | 162/200 [1:09:57<14:20, 22.66s/it] 82%|████████▏ | 163/200 [1:10:19<13:57, 22.64s/it] 82%|████████▏ | 164/200 [1:10:40<13:13, 22.05s/it] 82%|████████▎ | 165/200 [1:11:01<12:35, 21.59s/it] 83%|████████▎ | 166/200 [1:11:38<14:54, 26.30s/it] 84%|████████▎ | 167/200 [1:11:57<13:12, 24.02s/it] 84%|████████▍ | 168/200 [1:12:18<12:25, 23.30s/it] 84%|████████▍ | 169/200 [1:12:41<11:56, 23.11s/it] 85%|████████▌ | 170/200 [1:13:04<11:29, 22.98s/it] 86%|████████▌ | 171/200 [1:13:26<11:03, 22.89s/it] 86%|████████▌ | 172/200 [1:13:49<10:38, 22.81s/it] 86%|████████▋ | 173/200 [1:14:11<10:14, 22.74s/it] 87%|████████▋ | 174/200 [1:14:34<09:50, 22.72s/it] 88%|████████▊ | 175/200 [1:14:57<09:26, 22.68s/it] 88%|████████▊ | 176/200 [1:15:19<09:03, 22.66s/it] 88%|████████▊ | 177/200 [1:15:42<08:40, 22.65s/it] 89%|████████▉ | 178/200 [1:16:05<08:18, 22.66s/it] 90%|████████▉ | 179/200 [1:16:25<07:42, 22.02s/it] 90%|█████████ | 180/200 [1:16:46<07:12, 21.62s/it] 90%|█████████ | 181/200 [1:17:23<08:19, 26.28s/it] 91%|█████████ | 182/200 [1:17:42<07:15, 24.18s/it] 92%|█████████▏| 183/200 [1:18:03<06:34, 23.22s/it] 92%|█████████▏| 184/200 [1:18:26<06:08, 23.05s/it] 92%|█████████▎| 185/200 [1:18:48<05:43, 22.92s/it] 93%|█████████▎| 186/200 [1:19:11<05:19, 22.85s/it] 94%|█████████▎| 187/200 [1:19:34<04:56, 22.80s/it] 94%|█████████▍| 188/200 [1:19:56<04:32, 22.75s/it] 94%|█████████▍| 189/200 [1:20:19<04:10, 22.73s/it] 95%|█████████▌| 190/200 [1:20:42<03:47, 22.72s/it] 96%|█████████▌| 191/200 [1:21:04<03:24, 22.68s/it] 96%|█████████▌| 192/200 [1:21:27<03:01, 22.66s/it] 96%|█████████▋| 193/200 [1:21:50<02:38, 22.65s/it] 97%|█████████▋| 194/200 [1:22:10<02:12, 22.05s/it] 98%|█████████▊| 195/200 [1:22:31<01:48, 21.61s/it] 98%|█████████▊| 196/200 [1:23:08<01:45, 26.33s/it] 98%|█████████▊| 197/200 [1:23:28<01:13, 24.43s/it] 99%|█████████▉| 198/200 [1:23:46<00:44, 22.39s/it]100%|█████████▉| 199/200 [1:24:01<00:20, 20.34s/it]100%|██████████| 200/200 [1:24:29<00:00, 22.62s/it]100%|██████████| 200/200 [1:24:29<00:00, 25.35s/it]
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.302, acc: 11.09, fv: 2.301, accv: 11.92, lr: 0.0010, time: 14.04
[000001] f: 2.297, acc: 10.03, fv: 2.295, accv: 10.20, lr: 0.0010, time: 13.36
[000063] f: 1.784, acc: 33.34, fv: 1.807, accv: 32.17, lr: 0.0010, time: 13.47
[000125] f: 1.537, acc: 43.78, fv: 1.513, accv: 44.06, lr: 0.0010, time: 13.50
[000187] f: 1.396, acc: 49.92, fv: 1.391, accv: 49.07, lr: 0.0010, time: 13.53
[000249] f: 1.309, acc: 54.17, fv: 1.307, accv: 53.27, lr: 0.0010, time: 13.50
[000250] f: 1.265, acc: 55.40, fv: 1.262, accv: 54.51, lr: 0.0010, time: 13.49
[000251] f: 1.292, acc: 54.25, fv: 1.292, accv: 53.84, lr: 0.0010, time: 13.49
[000313] f: 1.288, acc: 52.09, fv: 1.312, accv: 52.02, lr: 0.0010, time: 13.63
[000375] f: 1.282, acc: 51.68, fv: 1.314, accv: 50.74, lr: 0.0010, time: 13.60
[000437] f: 1.216, acc: 56.74, fv: 1.270, accv: 54.67, lr: 0.0010, time: 13.65
[000499] f: 1.320, acc: 52.56, fv: 1.387, accv: 51.98, lr: 0.0010, time: 13.55
[000501] f: 1.293, acc: 53.03, fv: 1.333, accv: 53.15, lr: 0.0010, time: 13.52
[000563] f: 1.047, acc: 62.62, fv: 1.057, accv: 62.24, lr: 0.0010, time: 13.63
[000625] f: 1.104, acc: 61.25, fv: 1.140, accv: 60.31, lr: 0.0010, time: 13.69
[000687] f: 1.129, acc: 59.14, fv: 1.164, accv: 58.60, lr: 0.0010, time: 13.68
[000749] f: 1.160, acc: 60.20, fv: 1.249, accv: 59.27, lr: 0.0010, time: 13.66
[000751] f: 0.993, acc: 64.52, fv: 1.051, accv: 63.01, lr: 0.0010, time: 13.48
[000813] f: 0.962, acc: 64.93, fv: 1.003, accv: 63.38, lr: 0.0010, time: 13.72
[000875] f: 1.023, acc: 62.61, fv: 1.094, accv: 61.16, lr: 0.0010, time: 13.78
[000937] f: 0.962, acc: 65.38, fv: 1.008, accv: 64.36, lr: 0.0010, time: 13.72
[000999] f: 0.860, acc: 69.97, fv: 0.854, accv: 69.87, lr: 0.0010, time: 13.80
[001001] f: 0.835, acc: 71.10, fv: 0.835, accv: 70.40, lr: 0.0010, time: 13.44
[001063] f: 0.918, acc: 67.60, fv: 0.950, accv: 66.65, lr: 0.0010, time: 13.80
[001125] f: 0.811, acc: 71.16, fv: 0.814, accv: 70.81, lr: 0.0010, time: 13.77
[001187] f: 0.992, acc: 64.18, fv: 1.067, accv: 63.62, lr: 0.0010, time: 13.76
[001249] f: 0.883, acc: 68.23, fv: 0.960, accv: 66.72, lr: 0.0010, time: 13.71
[001750] f: 0.854, acc: 69.67, fv: 0.889, accv: 69.26, lr: 0.0010, time: 14.00
[002250] f: 0.895, acc: 69.44, fv: 1.026, accv: 67.28, lr: 0.0010, time: 14.54
[002750] f: 0.618, acc: 78.58, fv: 0.669, accv: 76.87, lr: 0.0010, time: 14.45
[003250] f: 0.584, acc: 79.74, fv: 0.668, accv: 76.95, lr: 0.0010, time: 14.67
[003750] f: 0.715, acc: 75.25, fv: 0.834, accv: 72.48, lr: 0.0010, time: 14.40
[004000] f: 0.589, acc: 79.46, fv: 0.719, accv: 75.70, lr: 0.0010, time: 13.80
[004250] f: 0.692, acc: 75.92, fv: 0.812, accv: 73.43, lr: 0.0010, time: 13.77
[004750] f: 0.711, acc: 74.99, fv: 0.865, accv: 71.59, lr: 0.0010, time: 14.45
[005250] f: 0.666, acc: 76.19, fv: 0.804, accv: 73.70, lr: 0.0010, time: 14.28
[005750] f: 0.444, acc: 84.82, fv: 0.555, accv: 81.58, lr: 0.0010, time: 14.08
[006250] f: 0.473, acc: 83.30, fv: 0.586, accv: 80.00, lr: 0.0010, time: 14.02
[006750] f: 0.462, acc: 83.80, fv: 0.580, accv: 81.13, lr: 0.0010, time: 13.92
[007250] f: 0.448, acc: 84.50, fv: 0.596, accv: 80.72, lr: 0.0009, time: 13.92
[007750] f: 0.451, acc: 84.45, fv: 0.617, accv: 80.11, lr: 0.0009, time: 13.68
[008250] f: 0.364, acc: 87.41, fv: 0.504, accv: 83.41, lr: 0.0009, time: 13.65
[008750] f: 0.568, acc: 80.60, fv: 0.760, accv: 76.51, lr: 0.0009, time: 13.72
[011500] f: 0.393, acc: 86.44, fv: 0.618, accv: 80.48, lr: 0.0009, time: 14.42
[015250] f: 0.280, acc: 89.98, fv: 0.519, accv: 83.54, lr: 0.0008, time: 14.46
[019000] f: 0.219, acc: 92.19, fv: 0.472, accv: 85.26, lr: 0.0007, time: 14.44
[022750] f: 0.253, acc: 90.78, fv: 0.562, accv: 83.48, lr: 0.0006, time: 14.44
[026500] f: 0.147, acc: 94.86, fv: 0.490, accv: 85.87, lr: 0.0005, time: 14.45
[030250] f: 0.163, acc: 94.15, fv: 0.569, accv: 84.68, lr: 0.0003, time: 14.40
[034000] f: 0.107, acc: 96.43, fv: 0.516, accv: 86.08, lr: 0.0002, time: 14.87
[037750] f: 0.082, acc: 97.54, fv: 0.499, accv: 86.54, lr: 0.0001, time: 14.92
[041500] f: 0.074, acc: 97.88, fv: 0.504, accv: 86.53, lr: 0.0001, time: 14.62
[045250] f: 0.065, acc: 98.26, fv: 0.491, accv: 86.97, lr: 0.0000, time: 14.86
[049000] f: 0.063, acc: 98.33, fv: 0.493, accv: 86.80, lr: 0.0000, time: 14.54
[050000] f: 0.063, acc: 98.26, fv: 0.494, accv: 86.87, lr: 0.0000, time: 14.33
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:40<5:33:57, 100.69s/it]  1%|          | 2/200 [03:07<5:05:48, 92.67s/it]   2%|▏         | 3/200 [04:33<4:53:35, 89.42s/it]  2%|▏         | 4/200 [05:56<4:43:52, 86.90s/it]  2%|▎         | 5/200 [07:18<4:36:20, 85.03s/it]  3%|▎         | 6/200 [07:35<3:20:09, 61.91s/it]  4%|▎         | 7/200 [08:09<2:50:15, 52.93s/it]  4%|▍         | 8/200 [08:26<2:12:50, 41.51s/it]  4%|▍         | 9/200 [09:01<2:05:44, 39.50s/it]  5%|▌         | 10/200 [09:16<1:41:02, 31.91s/it]  6%|▌         | 11/200 [09:51<1:43:26, 32.84s/it]  6%|▌         | 12/200 [10:05<1:25:10, 27.18s/it]  6%|▋         | 13/200 [10:39<1:31:01, 29.21s/it]  7%|▋         | 14/200 [10:56<1:19:10, 25.54s/it]  8%|▊         | 15/200 [11:28<1:24:07, 27.28s/it]  8%|▊         | 16/200 [12:01<1:29:31, 29.19s/it]  8%|▊         | 17/200 [12:35<1:32:52, 30.45s/it]  9%|▉         | 18/200 [12:54<1:22:37, 27.24s/it] 10%|▉         | 19/200 [13:23<1:23:23, 27.64s/it] 10%|█         | 20/200 [13:45<1:17:31, 25.84s/it] 10%|█         | 21/200 [14:13<1:19:17, 26.58s/it] 11%|█         | 22/200 [14:34<1:14:26, 25.10s/it] 12%|█▏        | 23/200 [15:04<1:18:23, 26.57s/it] 12%|█▏        | 24/200 [15:26<1:13:24, 25.02s/it] 12%|█▎        | 25/200 [15:58<1:18:57, 27.07s/it] 13%|█▎        | 26/200 [16:19<1:13:15, 25.26s/it] 14%|█▎        | 27/200 [16:52<1:19:38, 27.62s/it] 14%|█▍        | 28/200 [17:12<1:13:01, 25.47s/it] 14%|█▍        | 29/200 [17:46<1:20:00, 28.07s/it] 15%|█▌        | 30/200 [18:07<1:13:10, 25.83s/it] 16%|█▌        | 31/200 [18:41<1:19:54, 28.37s/it] 16%|█▌        | 32/200 [19:03<1:13:24, 26.22s/it] 16%|█▋        | 33/200 [19:37<1:19:29, 28.56s/it] 17%|█▋        | 34/200 [19:58<1:13:12, 26.46s/it] 18%|█▊        | 35/200 [20:32<1:18:49, 28.67s/it] 18%|█▊        | 36/200 [20:54<1:12:42, 26.60s/it] 18%|█▊        | 37/200 [21:13<1:06:33, 24.50s/it] 19%|█▉        | 38/200 [21:31<1:00:40, 22.47s/it] 20%|█▉        | 39/200 [21:54<1:00:19, 22.48s/it] 20%|██        | 40/200 [22:16<1:00:02, 22.52s/it] 20%|██        | 41/200 [22:39<59:43, 22.54s/it]   21%|██        | 42/200 [23:01<59:20, 22.54s/it] 22%|██▏       | 43/200 [23:24<59:01, 22.56s/it] 22%|██▏       | 44/200 [23:46<58:37, 22.55s/it] 22%|██▎       | 45/200 [24:09<58:13, 22.54s/it] 23%|██▎       | 46/200 [24:46<1:09:03, 26.91s/it] 24%|██▎       | 47/200 [25:08<1:04:35, 25.33s/it] 24%|██▍       | 48/200 [25:28<1:00:19, 23.81s/it] 24%|██▍       | 49/200 [25:45<54:55, 21.83s/it]   25%|██▌       | 50/200 [26:08<55:08, 22.06s/it] 26%|██▌       | 51/200 [26:30<55:08, 22.21s/it] 26%|██▌       | 52/200 [26:53<55:01, 22.30s/it] 26%|██▋       | 53/200 [27:15<54:47, 22.36s/it] 27%|██▋       | 54/200 [27:38<54:36, 22.45s/it] 28%|██▊       | 55/200 [28:01<54:19, 22.48s/it] 28%|██▊       | 56/200 [28:23<53:57, 22.48s/it] 28%|██▊       | 57/200 [28:46<53:37, 22.50s/it] 29%|██▉       | 58/200 [29:08<53:17, 22.52s/it] 30%|██▉       | 59/200 [29:31<52:58, 22.54s/it] 30%|███       | 60/200 [29:53<52:35, 22.54s/it] 30%|███       | 61/200 [30:30<1:02:17, 26.89s/it] 31%|███       | 62/200 [30:52<58:07, 25.27s/it]   32%|███▏      | 63/200 [31:13<54:46, 23.99s/it] 32%|███▏      | 64/200 [31:29<49:16, 21.74s/it] 32%|███▎      | 65/200 [31:52<49:23, 21.95s/it] 33%|███▎      | 66/200 [32:14<49:27, 22.15s/it] 34%|███▎      | 67/200 [32:37<49:23, 22.28s/it] 34%|███▍      | 68/200 [32:59<49:11, 22.36s/it] 34%|███▍      | 69/200 [33:22<48:55, 22.41s/it] 35%|███▌      | 70/200 [33:45<48:40, 22.46s/it] 36%|███▌      | 71/200 [34:07<48:21, 22.49s/it] 36%|███▌      | 72/200 [34:30<47:59, 22.50s/it] 36%|███▋      | 73/200 [34:52<47:40, 22.53s/it] 37%|███▋      | 74/200 [35:15<47:18, 22.53s/it] 38%|███▊      | 75/200 [35:37<46:56, 22.53s/it] 38%|███▊      | 76/200 [36:14<55:28, 26.85s/it] 38%|███▊      | 77/200 [36:36<51:35, 25.17s/it] 39%|███▉      | 78/200 [36:57<49:01, 24.11s/it] 40%|███▉      | 79/200 [37:14<44:09, 21.89s/it] 40%|████      | 80/200 [37:36<43:47, 21.90s/it] 40%|████      | 81/200 [37:58<43:51, 22.12s/it] 41%|████      | 82/200 [38:21<43:45, 22.25s/it] 42%|████▏     | 83/200 [38:44<43:35, 22.35s/it] 42%|████▏     | 84/200 [39:06<43:20, 22.42s/it] 42%|████▎     | 85/200 [39:29<43:03, 22.47s/it] 43%|████▎     | 86/200 [39:51<42:43, 22.49s/it] 44%|████▎     | 87/200 [40:14<42:23, 22.51s/it] 44%|████▍     | 88/200 [40:36<42:02, 22.53s/it] 44%|████▍     | 89/200 [40:59<41:37, 22.50s/it] 45%|████▌     | 90/200 [41:21<41:19, 22.54s/it] 46%|████▌     | 91/200 [41:59<48:51, 26.89s/it] 46%|████▌     | 92/200 [42:20<45:18, 25.17s/it] 46%|████▋     | 93/200 [42:41<43:05, 24.17s/it] 47%|████▋     | 94/200 [42:59<39:05, 22.12s/it] 48%|████▊     | 95/200 [43:20<38:12, 21.83s/it] 48%|████▊     | 96/200 [43:43<38:11, 22.04s/it] 48%|████▊     | 97/200 [44:05<38:06, 22.19s/it] 49%|████▉     | 98/200 [44:28<37:54, 22.30s/it] 50%|████▉     | 99/200 [44:50<37:40, 22.38s/it] 50%|█████     | 100/200 [45:13<37:23, 22.43s/it] 50%|█████     | 101/200 [45:35<37:05, 22.48s/it] 51%|█████     | 102/200 [45:58<36:45, 22.50s/it] 52%|█████▏    | 103/200 [46:20<36:24, 22.52s/it] 52%|█████▏    | 104/200 [46:43<36:00, 22.50s/it] 52%|█████▎    | 105/200 [47:06<35:41, 22.54s/it] 53%|█████▎    | 106/200 [47:43<42:08, 26.90s/it] 54%|█████▎    | 107/200 [48:03<38:51, 25.07s/it] 54%|█████▍    | 108/200 [48:25<37:03, 24.17s/it] 55%|█████▍    | 109/200 [48:44<33:59, 22.41s/it] 55%|█████▌    | 110/200 [49:04<32:38, 21.76s/it] 56%|█████▌    | 111/200 [49:27<32:39, 22.01s/it] 56%|█████▌    | 112/200 [49:49<32:32, 22.18s/it] 56%|█████▋    | 113/200 [50:12<32:19, 22.29s/it] 57%|█████▋    | 114/200 [50:34<32:05, 22.39s/it] 57%|█████▊    | 115/200 [50:57<31:44, 22.41s/it] 58%|█████▊    | 116/200 [51:19<31:25, 22.45s/it] 58%|█████▊    | 117/200 [51:42<31:07, 22.50s/it] 59%|█████▉    | 118/200 [52:05<30:47, 22.53s/it] 60%|█████▉    | 119/200 [52:27<30:26, 22.54s/it] 60%|██████    | 120/200 [52:50<30:03, 22.55s/it] 60%|██████    | 121/200 [53:27<35:22, 26.87s/it] 61%|██████    | 122/200 [53:47<32:29, 25.00s/it] 62%|██████▏   | 123/200 [54:10<31:01, 24.17s/it] 62%|██████▏   | 124/200 [54:29<28:40, 22.64s/it] 62%|██████▎   | 125/200 [54:48<27:04, 21.66s/it] 63%|██████▎   | 126/200 [55:11<27:02, 21.93s/it] 64%|██████▎   | 127/200 [55:33<26:56, 22.14s/it] 64%|██████▍   | 128/200 [55:56<26:42, 22.26s/it] 64%|██████▍   | 129/200 [56:18<26:27, 22.36s/it] 65%|██████▌   | 130/200 [56:41<26:10, 22.44s/it] 66%|██████▌   | 131/200 [57:04<25:52, 22.49s/it] 66%|██████▌   | 132/200 [57:26<25:31, 22.52s/it] 66%|██████▋   | 133/200 [57:49<25:10, 22.54s/it] 67%|██████▋   | 134/200 [58:11<24:49, 22.57s/it] 68%|██████▊   | 135/200 [58:34<24:27, 22.58s/it] 68%|██████▊   | 136/200 [59:11<28:51, 27.06s/it] 68%|██████▊   | 137/200 [59:32<26:21, 25.10s/it] 69%|██████▉   | 138/200 [59:54<25:05, 24.29s/it] 70%|██████▉   | 139/200 [1:00:14<23:19, 22.95s/it] 70%|███████   | 140/200 [1:00:33<21:38, 21.65s/it] 70%|███████   | 141/200 [1:00:55<21:33, 21.93s/it] 71%|███████   | 142/200 [1:01:18<21:21, 22.09s/it] 72%|███████▏  | 143/200 [1:01:40<21:07, 22.23s/it] 72%|███████▏  | 144/200 [1:02:03<20:51, 22.35s/it] 72%|███████▎  | 145/200 [1:02:26<20:33, 22.43s/it] 73%|███████▎  | 146/200 [1:02:48<20:15, 22.50s/it] 74%|███████▎  | 147/200 [1:03:11<19:53, 22.52s/it] 74%|███████▍  | 148/200 [1:03:33<19:30, 22.52s/it] 74%|███████▍  | 149/200 [1:03:56<19:09, 22.55s/it] 75%|███████▌  | 150/200 [1:04:19<18:48, 22.57s/it] 76%|███████▌  | 151/200 [1:04:56<22:06, 27.06s/it] 76%|███████▌  | 152/200 [1:05:17<20:04, 25.09s/it] 76%|███████▋  | 153/200 [1:05:39<19:02, 24.30s/it] 77%|███████▋  | 154/200 [1:05:59<17:41, 23.08s/it] 78%|███████▊  | 155/200 [1:06:18<16:11, 21.59s/it] 78%|███████▊  | 156/200 [1:06:40<16:02, 21.87s/it] 78%|███████▊  | 157/200 [1:07:03<15:50, 22.09s/it] 79%|███████▉  | 158/200 [1:07:25<15:32, 22.21s/it] 80%|███████▉  | 159/200 [1:07:48<15:15, 22.33s/it] 80%|████████  | 160/200 [1:08:10<14:56, 22.41s/it] 80%|████████  | 161/200 [1:08:33<14:36, 22.47s/it] 81%|████████  | 162/200 [1:08:56<14:14, 22.50s/it] 82%|████████▏ | 163/200 [1:09:18<13:53, 22.52s/it] 82%|████████▏ | 164/200 [1:09:41<13:31, 22.55s/it] 82%|████████▎ | 165/200 [1:10:03<13:09, 22.56s/it] 83%|████████▎ | 166/200 [1:10:41<15:16, 26.96s/it] 84%|████████▎ | 167/200 [1:11:01<13:45, 25.02s/it] 84%|████████▍ | 168/200 [1:11:24<12:56, 24.27s/it] 84%|████████▍ | 169/200 [1:11:44<11:58, 23.17s/it] 85%|████████▌ | 170/200 [1:12:02<10:46, 21.56s/it] 86%|████████▌ | 171/200 [1:12:24<10:33, 21.84s/it] 86%|████████▌ | 172/200 [1:12:47<10:17, 22.05s/it] 86%|████████▋ | 173/200 [1:13:10<09:59, 22.20s/it] 87%|████████▋ | 174/200 [1:13:32<09:39, 22.31s/it] 88%|████████▊ | 175/200 [1:13:55<09:19, 22.38s/it] 88%|████████▊ | 176/200 [1:14:17<08:58, 22.44s/it] 88%|████████▊ | 177/200 [1:14:40<08:36, 22.45s/it] 89%|████████▉ | 178/200 [1:15:02<08:14, 22.50s/it] 90%|████████▉ | 179/200 [1:15:25<07:53, 22.53s/it] 90%|█████████ | 180/200 [1:15:48<07:31, 22.56s/it] 90%|█████████ | 181/200 [1:16:25<08:33, 27.02s/it] 91%|█████████ | 182/200 [1:16:45<07:31, 25.08s/it] 92%|█████████▏| 183/200 [1:17:08<06:53, 24.34s/it] 92%|█████████▏| 184/200 [1:17:29<06:11, 23.20s/it] 92%|█████████▎| 185/200 [1:17:47<05:24, 21.65s/it] 93%|█████████▎| 186/200 [1:18:09<05:05, 21.81s/it] 94%|█████████▎| 187/200 [1:18:31<04:46, 22.03s/it] 94%|█████████▍| 188/200 [1:18:54<04:26, 22.18s/it] 94%|█████████▍| 189/200 [1:19:17<04:05, 22.30s/it] 95%|█████████▌| 190/200 [1:19:39<03:43, 22.34s/it] 96%|█████████▌| 191/200 [1:20:01<03:21, 22.40s/it] 96%|█████████▌| 192/200 [1:20:24<02:59, 22.45s/it] 96%|█████████▋| 193/200 [1:20:47<02:37, 22.49s/it] 97%|█████████▋| 194/200 [1:21:09<02:15, 22.52s/it] 98%|█████████▊| 195/200 [1:21:32<01:52, 22.55s/it] 98%|█████████▊| 196/200 [1:22:09<01:47, 26.92s/it] 98%|█████████▊| 197/200 [1:22:30<01:15, 25.04s/it] 99%|█████████▉| 198/200 [1:22:52<00:48, 24.30s/it]100%|█████████▉| 199/200 [1:23:13<00:23, 23.18s/it]100%|██████████| 200/200 [1:23:46<00:00, 26.18s/it]100%|██████████| 200/200 [1:23:46<00:00, 25.13s/it]
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.306, acc: 10.00, fv: 2.306, accv: 10.00, lr: 0.0010, time: 14.67
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:38<5:25:28, 98.13s/it]  1%|          | 2/200 [03:03<4:59:53, 90.88s/it]  2%|▏         | 3/200 [04:27<4:47:31, 87.57s/it]  2%|▏         | 4/200 [05:49<4:38:45, 85.33s/it]  2%|▎         | 5/200 [07:15<4:37:54, 85.51s/it]  3%|▎         | 6/200 [07:32<3:20:57, 62.15s/it]  4%|▎         | 7/200 [08:04<2:48:26, 52.36s/it]  4%|▍         | 8/200 [08:23<2:13:35, 41.75s/it]  4%|▍         | 9/200 [08:52<2:00:39, 37.90s/it]  5%|▌         | 10/200 [09:13<1:43:11, 32.59s/it]  6%|▌         | 11/200 [09:42<1:39:15, 31.51s/it]  6%|▌         | 12/200 [10:03<1:28:19, 28.19s/it]  6%|▋         | 13/200 [10:33<1:30:15, 28.96s/it]  7%|▋         | 14/200 [10:54<1:22:03, 26.47s/it]  8%|▊         | 15/200 [11:26<1:26:45, 28.14s/it]  8%|▊         | 16/200 [11:59<1:30:19, 29.45s/it]  8%|▊         | 17/200 [12:32<1:33:08, 30.54s/it]  9%|▉         | 18/200 [12:52<1:23:44, 27.60s/it] 10%|▉         | 19/200 [13:26<1:28:23, 29.30s/it] 10%|█         | 20/200 [13:47<1:20:51, 26.95s/it] 10%|█         | 21/200 [14:20<1:26:00, 28.83s/it] 11%|█         | 22/200 [14:42<1:19:06, 26.67s/it] 12%|█▏        | 23/200 [15:16<1:24:48, 28.75s/it] 12%|█▏        | 24/200 [15:37<1:17:37, 26.46s/it] 12%|█▎        | 25/200 [16:11<1:24:08, 28.85s/it] 13%|█▎        | 26/200 [16:32<1:16:27, 26.36s/it] 14%|█▎        | 27/200 [17:07<1:23:36, 29.00s/it] 14%|█▍        | 28/200 [17:27<1:15:54, 26.48s/it] 14%|█▍        | 29/200 [18:02<1:22:40, 29.01s/it] 15%|█▌        | 30/200 [18:23<1:15:25, 26.62s/it] 16%|█▌        | 31/200 [18:58<1:21:35, 28.97s/it] 16%|█▌        | 32/200 [19:19<1:14:52, 26.74s/it] 16%|█▋        | 33/200 [19:54<1:20:37, 28.97s/it] 17%|█▋        | 34/200 [20:15<1:14:17, 26.85s/it] 18%|█▊        | 35/200 [20:50<1:19:50, 29.03s/it] 18%|█▊        | 36/200 [21:12<1:14:08, 27.13s/it] 18%|█▊        | 37/200 [21:35<1:10:02, 25.78s/it] 19%|█▉        | 38/200 [21:58<1:07:03, 24.84s/it] 20%|█▉        | 39/200 [22:20<1:04:55, 24.20s/it] 20%|██        | 40/200 [22:43<1:03:15, 23.72s/it] 20%|██        | 41/200 [23:06<1:02:02, 23.41s/it] 21%|██        | 42/200 [23:28<1:01:04, 23.20s/it] 22%|██▏       | 43/200 [23:51<1:00:17, 23.04s/it] 22%|██▏       | 44/200 [24:09<56:25, 21.70s/it]   22%|██▎       | 45/200 [24:32<56:23, 21.83s/it] 23%|██▎       | 46/200 [25:06<1:05:40, 25.59s/it] 24%|██▎       | 47/200 [25:29<1:02:58, 24.69s/it] 24%|██▍       | 48/200 [25:51<1:00:56, 24.05s/it] 24%|██▍       | 49/200 [26:14<59:28, 23.63s/it]   25%|██▌       | 50/200 [26:37<58:24, 23.36s/it] 26%|██▌       | 51/200 [26:59<57:28, 23.14s/it] 26%|██▌       | 52/200 [27:22<56:46, 23.01s/it] 26%|██▋       | 53/200 [27:45<56:06, 22.90s/it] 27%|██▋       | 54/200 [28:07<55:31, 22.82s/it] 28%|██▊       | 55/200 [28:30<54:59, 22.76s/it] 28%|██▊       | 56/200 [28:52<54:28, 22.70s/it] 28%|██▊       | 57/200 [29:15<54:02, 22.68s/it] 29%|██▉       | 58/200 [29:38<53:38, 22.67s/it] 30%|██▉       | 59/200 [29:56<50:20, 21.42s/it] 30%|███       | 60/200 [30:18<50:32, 21.66s/it] 30%|███       | 61/200 [30:53<58:54, 25.43s/it] 31%|███       | 62/200 [31:15<56:35, 24.61s/it] 32%|███▏      | 63/200 [31:38<54:47, 24.00s/it] 32%|███▏      | 64/200 [32:00<53:29, 23.60s/it] 32%|███▎      | 65/200 [32:23<52:27, 23.31s/it] 33%|███▎      | 66/200 [32:46<51:41, 23.14s/it] 34%|███▎      | 67/200 [33:08<50:57, 22.99s/it] 34%|███▍      | 68/200 [33:31<50:25, 22.92s/it] 34%|███▍      | 69/200 [33:54<49:54, 22.86s/it] 35%|███▌      | 70/200 [34:17<49:24, 22.80s/it] 36%|███▌      | 71/200 [34:39<48:57, 22.78s/it] 36%|███▌      | 72/200 [35:02<48:33, 22.76s/it] 36%|███▋      | 73/200 [35:24<47:55, 22.65s/it] 37%|███▋      | 74/200 [35:43<45:11, 21.52s/it] 38%|███▊      | 75/200 [36:06<45:25, 21.81s/it] 38%|███▊      | 76/200 [36:40<52:48, 25.55s/it] 38%|███▊      | 77/200 [37:03<50:36, 24.68s/it] 39%|███▉      | 78/200 [37:25<48:55, 24.06s/it] 40%|███▉      | 79/200 [37:48<47:40, 23.64s/it] 40%|████      | 80/200 [38:11<46:40, 23.34s/it] 40%|████      | 81/200 [38:33<45:52, 23.13s/it] 41%|████      | 82/200 [38:56<45:12, 22.99s/it] 42%|████▏     | 83/200 [39:19<44:38, 22.89s/it] 42%|████▏     | 84/200 [39:41<44:06, 22.82s/it] 42%|████▎     | 85/200 [40:04<43:41, 22.79s/it] 43%|████▎     | 86/200 [40:27<43:12, 22.74s/it] 44%|████▎     | 87/200 [40:49<42:44, 22.70s/it] 44%|████▍     | 88/200 [41:11<41:54, 22.45s/it] 44%|████▍     | 89/200 [41:30<39:39, 21.43s/it] 45%|████▌     | 90/200 [41:53<39:54, 21.76s/it] 46%|████▌     | 91/200 [42:27<46:23, 25.54s/it] 46%|████▌     | 92/200 [42:50<44:24, 24.67s/it] 46%|████▋     | 93/200 [43:12<42:57, 24.09s/it] 47%|████▋     | 94/200 [43:35<41:48, 23.67s/it] 48%|████▊     | 95/200 [43:58<40:52, 23.36s/it] 48%|████▊     | 96/200 [44:20<40:10, 23.17s/it] 48%|████▊     | 97/200 [44:43<39:30, 23.01s/it] 49%|████▉     | 98/200 [45:06<38:56, 22.91s/it] 50%|████▉     | 99/200 [45:29<38:28, 22.86s/it] 50%|█████     | 100/200 [45:51<37:58, 22.78s/it] 50%|█████     | 101/200 [46:14<37:30, 22.73s/it] 51%|█████     | 102/200 [46:36<37:05, 22.71s/it] 52%|█████▏    | 103/200 [46:58<36:09, 22.37s/it] 52%|█████▏    | 104/200 [47:18<34:28, 21.54s/it] 52%|█████▎    | 105/200 [47:40<34:37, 21.87s/it] 53%|█████▎    | 106/200 [48:15<40:10, 25.64s/it] 54%|█████▎    | 107/200 [48:37<38:18, 24.71s/it] 54%|█████▍    | 108/200 [49:00<36:57, 24.10s/it] 55%|█████▍    | 109/200 [49:23<35:52, 23.66s/it] 55%|█████▌    | 110/200 [49:45<35:00, 23.33s/it] 56%|█████▌    | 111/200 [50:08<34:18, 23.13s/it] 56%|█████▌    | 112/200 [50:30<33:41, 22.97s/it] 56%|█████▋    | 113/200 [50:53<33:10, 22.88s/it] 57%|█████▋    | 114/200 [51:16<32:40, 22.80s/it] 57%|█████▊    | 115/200 [51:38<32:12, 22.73s/it] 58%|█████▊    | 116/200 [52:01<31:47, 22.71s/it] 58%|█████▊    | 117/200 [52:24<31:23, 22.69s/it] 59%|█████▉    | 118/200 [52:45<30:21, 22.22s/it] 60%|█████▉    | 119/200 [53:05<29:07, 21.58s/it] 60%|██████    | 120/200 [53:27<29:11, 21.89s/it] 60%|██████    | 121/200 [54:02<33:49, 25.69s/it] 61%|██████    | 122/200 [54:25<32:12, 24.77s/it] 62%|██████▏   | 123/200 [54:47<31:00, 24.16s/it] 62%|██████▏   | 124/200 [55:10<30:02, 23.71s/it] 62%|██████▎   | 125/200 [55:33<29:15, 23.41s/it] 63%|██████▎   | 126/200 [55:55<28:35, 23.18s/it] 64%|██████▎   | 127/200 [56:18<28:01, 23.04s/it] 64%|██████▍   | 128/200 [56:41<27:29, 22.91s/it] 64%|██████▍   | 129/200 [57:03<27:02, 22.85s/it] 65%|██████▌   | 130/200 [57:26<26:35, 22.80s/it] 66%|██████▌   | 131/200 [57:49<26:09, 22.74s/it] 66%|██████▌   | 132/200 [58:11<25:43, 22.70s/it] 66%|██████▋   | 133/200 [58:32<24:40, 22.09s/it] 67%|██████▋   | 134/200 [58:52<23:46, 21.62s/it] 68%|██████▊   | 135/200 [59:15<23:44, 21.92s/it] 68%|██████▊   | 136/200 [59:50<27:27, 25.74s/it] 68%|██████▊   | 137/200 [1:00:12<26:02, 24.80s/it] 69%|██████▉   | 138/200 [1:00:35<24:57, 24.15s/it] 70%|██████▉   | 139/200 [1:00:57<24:04, 23.69s/it] 70%|███████   | 140/200 [1:01:20<23:21, 23.36s/it] 70%|███████   | 141/200 [1:01:43<22:44, 23.13s/it] 71%|███████   | 142/200 [1:02:05<22:13, 22.99s/it] 72%|███████▏  | 143/200 [1:02:28<21:43, 22.87s/it] 72%|███████▏  | 144/200 [1:02:51<21:16, 22.80s/it] 72%|███████▎  | 145/200 [1:03:13<20:50, 22.73s/it] 73%|███████▎  | 146/200 [1:03:36<20:25, 22.69s/it] 74%|███████▎  | 147/200 [1:03:58<20:00, 22.66s/it] 74%|███████▍  | 148/200 [1:04:19<19:04, 22.01s/it] 74%|███████▍  | 149/200 [1:04:39<18:20, 21.59s/it] 75%|███████▌  | 150/200 [1:05:02<18:10, 21.81s/it] 76%|███████▌  | 151/200 [1:05:36<20:58, 25.68s/it] 76%|███████▌  | 152/200 [1:05:59<19:48, 24.77s/it] 76%|███████▋  | 153/200 [1:06:22<18:54, 24.15s/it] 77%|███████▋  | 154/200 [1:06:44<18:10, 23.71s/it] 78%|███████▊  | 155/200 [1:07:07<17:32, 23.38s/it] 78%|███████▊  | 156/200 [1:07:30<16:59, 23.17s/it] 78%|███████▊  | 157/200 [1:07:52<16:29, 23.00s/it] 79%|███████▉  | 158/200 [1:08:15<16:01, 22.90s/it] 80%|███████▉  | 159/200 [1:08:38<15:35, 22.82s/it] 80%|████████  | 160/200 [1:09:00<15:10, 22.77s/it] 80%|████████  | 161/200 [1:09:23<14:46, 22.74s/it] 81%|████████  | 162/200 [1:09:46<14:23, 22.72s/it] 82%|████████▏ | 163/200 [1:10:06<13:36, 22.07s/it] 82%|████████▏ | 164/200 [1:10:27<13:00, 21.67s/it] 82%|████████▎ | 165/200 [1:10:49<12:43, 21.82s/it] 83%|████████▎ | 166/200 [1:11:24<14:38, 25.84s/it] 84%|████████▎ | 167/200 [1:11:47<13:40, 24.86s/it] 84%|████████▍ | 168/200 [1:12:10<12:54, 24.20s/it] 84%|████████▍ | 169/200 [1:12:32<12:15, 23.72s/it] 85%|████████▌ | 170/200 [1:12:55<11:42, 23.41s/it] 86%|████████▌ | 171/200 [1:13:17<11:11, 23.17s/it] 86%|████████▌ | 172/200 [1:13:40<10:43, 23.00s/it] 86%|████████▋ | 173/200 [1:14:03<10:18, 22.91s/it] 87%|████████▋ | 174/200 [1:14:25<09:53, 22.83s/it] 88%|████████▊ | 175/200 [1:14:48<09:29, 22.77s/it] 88%|████████▊ | 176/200 [1:15:11<09:04, 22.70s/it] 88%|████████▊ | 177/200 [1:15:33<08:40, 22.61s/it] 89%|████████▉ | 178/200 [1:15:53<08:02, 21.94s/it] 90%|████████▉ | 179/200 [1:16:14<07:34, 21.64s/it] 90%|█████████ | 180/200 [1:16:36<07:13, 21.69s/it] 90%|█████████ | 181/200 [1:17:12<08:10, 25.82s/it] 91%|█████████ | 182/200 [1:17:34<07:27, 24.86s/it] 92%|█████████▏| 183/200 [1:17:57<06:51, 24.19s/it] 92%|█████████▏| 184/200 [1:18:20<06:20, 23.75s/it] 92%|█████████▎| 185/200 [1:18:42<05:51, 23.43s/it] 93%|█████████▎| 186/200 [1:19:05<05:24, 23.21s/it] 94%|█████████▎| 187/200 [1:19:28<04:59, 23.05s/it] 94%|█████████▍| 188/200 [1:19:50<04:35, 22.95s/it] 94%|█████████▍| 189/200 [1:20:13<04:11, 22.87s/it] 95%|█████████▌| 190/200 [1:20:36<03:48, 22.83s/it] 96%|█████████▌| 191/200 [1:20:58<03:25, 22.79s/it] 96%|█████████▌| 192/200 [1:21:21<03:00, 22.59s/it] 96%|█████████▋| 193/200 [1:21:41<02:34, 22.03s/it] 97%|█████████▋| 194/200 [1:22:03<02:10, 21.81s/it] 98%|█████████▊| 195/200 [1:22:24<01:48, 21.69s/it] 98%|█████████▊| 196/200 [1:23:00<01:43, 25.87s/it] 98%|█████████▊| 197/200 [1:23:17<01:10, 23.39s/it] 99%|█████████▉| 198/200 [1:23:32<00:41, 20.69s/it]100%|█████████▉| 199/200 [1:23:45<00:18, 18.52s/it]100%|██████████| 200/200 [1:24:14<00:00, 21.56s/it][000001] f: 2.298, acc: 11.18, fv: 2.297, accv: 11.85, lr: 0.0010, time: 13.73
[000063] f: 1.687, acc: 37.18, fv: 1.668, accv: 36.75, lr: 0.0010, time: 13.60
[000125] f: 1.587, acc: 40.58, fv: 1.640, accv: 38.71, lr: 0.0010, time: 13.48
[000187] f: 1.551, acc: 44.47, fv: 1.553, accv: 45.12, lr: 0.0010, time: 13.44
[000249] f: 1.427, acc: 47.44, fv: 1.417, accv: 47.68, lr: 0.0010, time: 13.89
[000250] f: 1.381, acc: 49.61, fv: 1.368, accv: 49.14, lr: 0.0010, time: 13.80
[000251] f: 1.364, acc: 50.28, fv: 1.352, accv: 49.90, lr: 0.0010, time: 13.62
[000313] f: 1.410, acc: 47.55, fv: 1.486, accv: 46.00, lr: 0.0010, time: 13.70
[000375] f: 1.260, acc: 55.29, fv: 1.309, accv: 53.69, lr: 0.0010, time: 13.64
[000437] f: 1.198, acc: 57.08, fv: 1.224, accv: 56.09, lr: 0.0010, time: 13.72
[000499] f: 1.262, acc: 54.38, fv: 1.292, accv: 54.12, lr: 0.0010, time: 13.63
[000501] f: 1.221, acc: 56.34, fv: 1.233, accv: 55.76, lr: 0.0010, time: 13.80
[000563] f: 1.181, acc: 58.66, fv: 1.192, accv: 58.90, lr: 0.0010, time: 13.85
[000625] f: 1.313, acc: 54.48, fv: 1.388, accv: 53.93, lr: 0.0010, time: 13.88
[000687] f: 1.264, acc: 54.57, fv: 1.334, accv: 53.64, lr: 0.0010, time: 13.51
[000749] f: 1.098, acc: 60.90, fv: 1.113, accv: 60.62, lr: 0.0010, time: 13.65
[000751] f: 1.127, acc: 60.90, fv: 1.147, accv: 60.51, lr: 0.0010, time: 13.87
[000813] f: 1.137, acc: 57.34, fv: 1.230, accv: 54.72, lr: 0.0010, time: 13.80
[000875] f: 0.929, acc: 66.68, fv: 0.975, accv: 64.94, lr: 0.0010, time: 13.81
[000937] f: 1.028, acc: 64.03, fv: 1.035, accv: 64.38, lr: 0.0010, time: 13.44
[000999] f: 0.975, acc: 64.95, fv: 1.006, accv: 64.17, lr: 0.0010, time: 13.76
[001001] f: 1.396, acc: 52.82, fv: 1.444, accv: 53.86, lr: 0.0010, time: 13.75
[001063] f: 0.917, acc: 66.75, fv: 0.962, accv: 65.99, lr: 0.0010, time: 13.74
[001125] f: 0.867, acc: 69.03, fv: 0.907, accv: 67.92, lr: 0.0010, time: 13.72
[001187] f: 0.990, acc: 65.46, fv: 1.068, accv: 63.97, lr: 0.0010, time: 14.38
[001249] f: 0.968, acc: 66.01, fv: 1.005, accv: 66.09, lr: 0.0010, time: 14.73
[001750] f: 0.854, acc: 70.44, fv: 0.907, accv: 69.58, lr: 0.0010, time: 14.54
[002250] f: 0.670, acc: 76.45, fv: 0.751, accv: 74.24, lr: 0.0010, time: 14.63
[002750] f: 0.618, acc: 78.31, fv: 0.706, accv: 75.31, lr: 0.0010, time: 14.85
[003250] f: 0.690, acc: 76.19, fv: 0.733, accv: 75.23, lr: 0.0010, time: 14.37
[003750] f: 0.672, acc: 77.28, fv: 0.752, accv: 75.12, lr: 0.0010, time: 14.11
[004000] f: 0.570, acc: 79.92, fv: 0.662, accv: 77.51, lr: 0.0010, time: 13.96
[004250] f: 0.577, acc: 79.99, fv: 0.717, accv: 76.65, lr: 0.0010, time: 14.07
[004750] f: 0.587, acc: 79.12, fv: 0.710, accv: 75.90, lr: 0.0010, time: 13.93
[005250] f: 0.613, acc: 78.52, fv: 0.756, accv: 75.69, lr: 0.0010, time: 13.86
[005750] f: 0.551, acc: 80.67, fv: 0.679, accv: 78.43, lr: 0.0010, time: 13.89
[006250] f: 0.528, acc: 81.56, fv: 0.704, accv: 77.26, lr: 0.0010, time: 13.94
[006750] f: 0.463, acc: 83.98, fv: 0.575, accv: 81.26, lr: 0.0010, time: 13.93
[007250] f: 0.475, acc: 83.63, fv: 0.624, accv: 79.87, lr: 0.0009, time: 13.68
[007750] f: 0.665, acc: 78.61, fv: 0.946, accv: 73.61, lr: 0.0009, time: 13.70
[008250] f: 0.447, acc: 84.67, fv: 0.599, accv: 80.77, lr: 0.0009, time: 13.77
[008750] f: 0.389, acc: 86.50, fv: 0.544, accv: 82.44, lr: 0.0009, time: 14.51
[011500] f: 0.408, acc: 85.58, fv: 0.600, accv: 81.38, lr: 0.0009, time: 14.59
[015250] f: 0.271, acc: 90.51, fv: 0.519, accv: 84.03, lr: 0.0008, time: 14.43
[019000] f: 0.255, acc: 90.74, fv: 0.529, accv: 83.67, lr: 0.0007, time: 14.48
[022750] f: 0.176, acc: 93.90, fv: 0.481, accv: 85.84, lr: 0.0006, time: 14.52
[026500] f: 0.170, acc: 94.05, fv: 0.521, accv: 84.79, lr: 0.0005, time: 14.58
[030250] f: 0.150, acc: 94.62, fv: 0.532, accv: 85.39, lr: 0.0003, time: 14.57
[034000] f: 0.100, acc: 96.77, fv: 0.524, accv: 86.11, lr: 0.0002, time: 14.52
[037750] f: 0.081, acc: 97.56, fv: 0.503, accv: 86.33, lr: 0.0001, time: 14.51
[041500] f: 0.071, acc: 97.93, fv: 0.512, accv: 86.56, lr: 0.0001, time: 14.59
[045250] f: 0.063, acc: 98.34, fv: 0.507, accv: 86.86, lr: 0.0000, time: 14.59
[049000] f: 0.063, acc: 98.30, fv: 0.506, accv: 86.93, lr: 0.0000, time: 14.47
[050000] f: 0.061, acc: 98.43, fv: 0.504, accv: 86.83, lr: 0.0000, time: 13.72
100%|██████████| 200/200 [1:24:14<00:00, 25.27s/it]
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"simple","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='simple', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-simple.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 117, in main
    drop=args.model_args['dropout_rate'],
KeyError: 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.304, acc: 10.00, fv: 2.303, accv: 10.39, lr: 0.0010, time: 8.46
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:54<3:02:00, 54.88s/it]  1%|          | 2/200 [01:49<3:00:39, 54.75s/it]  2%|▏         | 3/200 [02:45<3:02:11, 55.49s/it]  2%|▏         | 4/200 [03:42<3:02:16, 55.80s/it]  2%|▎         | 5/200 [04:38<3:02:04, 56.02s/it]  3%|▎         | 6/200 [04:57<2:20:19, 43.40s/it]  4%|▎         | 7/200 [05:25<2:03:27, 38.38s/it]  4%|▍         | 8/200 [05:46<1:45:26, 32.95s/it]  4%|▍         | 9/200 [06:17<1:42:17, 32.13s/it]  5%|▌         | 10/200 [06:39<1:31:46, 28.98s/it]  6%|▌         | 11/200 [07:09<1:32:23, 29.33s/it]  6%|▌         | 12/200 [07:31<1:24:46, 27.05s/it]  6%|▋         | 13/200 [08:01<1:27:19, 28.02s/it]  7%|▋         | 14/200 [08:23<1:21:08, 26.18s/it]  8%|▊         | 15/200 [08:53<1:24:30, 27.41s/it]  8%|▊         | 16/200 [09:24<1:26:55, 28.34s/it]  8%|▊         | 17/200 [09:53<1:27:55, 28.83s/it]  9%|▉         | 18/200 [10:14<1:19:56, 26.36s/it] 10%|▉         | 19/200 [10:44<1:22:36, 27.39s/it] 10%|█         | 20/200 [11:06<1:17:12, 25.74s/it] 10%|█         | 21/200 [11:36<1:20:53, 27.11s/it] 11%|█         | 22/200 [11:58<1:15:45, 25.54s/it] 12%|█▏        | 23/200 [12:28<1:19:25, 26.92s/it] 12%|█▏        | 24/200 [12:50<1:14:31, 25.40s/it] 12%|█▎        | 25/200 [13:20<1:18:10, 26.81s/it] 13%|█▎        | 26/200 [13:42<1:13:27, 25.33s/it] 14%|█▎        | 27/200 [14:12<1:17:09, 26.76s/it] 14%|█▍        | 28/200 [14:34<1:12:29, 25.29s/it] 14%|█▍        | 29/200 [15:04<1:16:21, 26.79s/it] 15%|█▌        | 30/200 [15:26<1:11:41, 25.30s/it] 16%|█▌        | 31/200 [15:56<1:15:28, 26.80s/it] 16%|█▌        | 32/200 [16:18<1:10:54, 25.33s/it] 16%|█▋        | 33/200 [16:48<1:14:32, 26.78s/it] 17%|█▋        | 34/200 [17:10<1:09:58, 25.29s/it] 18%|█▊        | 35/200 [17:40<1:13:36, 26.77s/it] 18%|█▊        | 36/200 [18:02<1:09:06, 25.28s/it] 18%|█▊        | 37/200 [18:24<1:05:49, 24.23s/it] 19%|█▉        | 38/200 [18:46<1:03:56, 23.68s/it] 20%|█▉        | 39/200 [19:09<1:02:22, 23.25s/it] 20%|██        | 40/200 [19:31<1:01:33, 23.09s/it] 20%|██        | 41/200 [19:54<1:00:49, 22.95s/it] 21%|██        | 42/200 [20:17<1:00:11, 22.86s/it] 22%|██▏       | 43/200 [20:39<59:38, 22.79s/it]   22%|██▏       | 44/200 [21:02<59:10, 22.76s/it] 22%|██▎       | 45/200 [21:25<58:43, 22.73s/it] 23%|██▎       | 46/200 [21:56<1:04:47, 25.25s/it] 24%|██▎       | 47/200 [22:18<1:02:00, 24.32s/it] 24%|██▍       | 48/200 [22:40<59:38, 23.54s/it]   24%|██▍       | 49/200 [23:02<58:31, 23.26s/it] 25%|██▌       | 50/200 [23:24<57:22, 22.95s/it] 26%|██▌       | 51/200 [23:47<56:49, 22.88s/it] 26%|██▌       | 52/200 [24:10<56:16, 22.82s/it] 26%|██▋       | 53/200 [24:32<55:46, 22.77s/it] 27%|██▋       | 54/200 [24:55<55:20, 22.74s/it] 28%|██▊       | 55/200 [25:18<54:54, 22.72s/it] 28%|██▊       | 56/200 [25:40<54:29, 22.71s/it] 28%|██▊       | 57/200 [26:03<54:03, 22.68s/it] 29%|██▉       | 58/200 [26:26<53:39, 22.67s/it] 30%|██▉       | 59/200 [26:48<53:18, 22.68s/it] 30%|███       | 60/200 [27:11<52:54, 22.68s/it] 30%|███       | 61/200 [27:42<58:28, 25.24s/it] 31%|███       | 62/200 [28:04<55:46, 24.25s/it] 32%|███▏      | 63/200 [28:26<53:47, 23.56s/it] 32%|███▏      | 64/200 [28:49<52:45, 23.28s/it] 32%|███▎      | 65/200 [29:11<51:41, 22.97s/it] 33%|███▎      | 66/200 [29:34<51:06, 22.88s/it] 34%|███▎      | 67/200 [29:56<50:36, 22.83s/it] 34%|███▍      | 68/200 [30:19<50:06, 22.78s/it] 34%|███▍      | 69/200 [30:42<49:34, 22.70s/it] 35%|███▌      | 70/200 [31:04<49:09, 22.69s/it] 36%|███▌      | 71/200 [31:27<48:42, 22.65s/it] 36%|███▌      | 72/200 [31:50<48:22, 22.67s/it] 36%|███▋      | 73/200 [32:12<48:00, 22.68s/it] 37%|███▋      | 74/200 [32:35<47:36, 22.67s/it] 38%|███▊      | 75/200 [32:58<47:12, 22.66s/it] 38%|███▊      | 76/200 [33:29<52:04, 25.20s/it] 38%|███▊      | 77/200 [33:51<49:38, 24.22s/it] 39%|███▉      | 78/200 [34:13<47:54, 23.56s/it] 40%|███▉      | 79/200 [34:35<46:56, 23.28s/it] 40%|████      | 80/200 [34:58<45:55, 22.96s/it] 40%|████      | 81/200 [35:20<45:20, 22.86s/it] 41%|████      | 82/200 [35:43<44:51, 22.81s/it] 42%|████▏     | 83/200 [36:06<44:25, 22.78s/it] 42%|████▏     | 84/200 [36:28<44:01, 22.77s/it] 42%|████▎     | 85/200 [36:51<43:36, 22.75s/it] 43%|████▎     | 86/200 [37:14<43:12, 22.74s/it] 44%|████▎     | 87/200 [37:36<42:47, 22.72s/it] 44%|████▍     | 88/200 [37:59<42:22, 22.70s/it] 44%|████▍     | 89/200 [38:22<41:58, 22.69s/it] 45%|████▌     | 90/200 [38:44<41:36, 22.69s/it] 46%|████▌     | 91/200 [39:16<45:52, 25.25s/it] 46%|████▌     | 92/200 [39:37<43:33, 24.20s/it] 46%|████▋     | 93/200 [40:00<42:05, 23.61s/it] 47%|████▋     | 94/200 [40:22<41:14, 23.35s/it] 48%|████▊     | 95/200 [40:45<40:17, 23.02s/it] 48%|████▊     | 96/200 [41:07<39:43, 22.92s/it] 48%|████▊     | 97/200 [41:30<39:14, 22.86s/it] 49%|████▉     | 98/200 [41:53<38:44, 22.79s/it] 50%|████▉     | 99/200 [42:15<38:20, 22.77s/it] 50%|█████     | 100/200 [42:38<37:52, 22.72s/it] 50%|█████     | 101/200 [43:01<37:30, 22.73s/it] 51%|█████     | 102/200 [43:23<37:08, 22.74s/it] 52%|█████▏    | 103/200 [43:46<36:45, 22.74s/it] 52%|█████▏    | 104/200 [44:09<36:21, 22.72s/it] 52%|█████▎    | 105/200 [44:32<35:57, 22.71s/it] 53%|█████▎    | 106/200 [45:03<39:40, 25.33s/it] 54%|█████▎    | 107/200 [45:25<37:37, 24.27s/it] 54%|█████▍    | 108/200 [45:47<36:14, 23.64s/it] 55%|█████▍    | 109/200 [46:10<35:24, 23.35s/it] 55%|█████▌    | 110/200 [46:32<34:31, 23.01s/it] 56%|█████▌    | 111/200 [46:55<34:01, 22.93s/it] 56%|█████▌    | 112/200 [47:17<33:32, 22.87s/it] 56%|█████▋    | 113/200 [47:40<33:06, 22.83s/it] 57%|█████▋    | 114/200 [48:03<32:38, 22.77s/it] 57%|█████▊    | 115/200 [48:25<32:13, 22.75s/it] 58%|█████▊    | 116/200 [48:48<31:48, 22.72s/it] 58%|█████▊    | 117/200 [49:11<31:25, 22.72s/it] 59%|█████▉    | 118/200 [49:33<31:02, 22.71s/it] 60%|█████▉    | 119/200 [49:56<30:40, 22.72s/it] 60%|██████    | 120/200 [50:19<30:17, 22.72s/it] 60%|██████    | 121/200 [50:50<33:20, 25.33s/it] 61%|██████    | 122/200 [51:12<31:30, 24.23s/it] 62%|██████▏   | 123/200 [51:34<30:22, 23.67s/it] 62%|██████▏   | 124/200 [51:57<29:37, 23.38s/it] 62%|██████▎   | 125/200 [52:19<28:47, 23.04s/it] 63%|██████▎   | 126/200 [52:42<28:17, 22.94s/it] 64%|██████▎   | 127/200 [53:05<27:50, 22.88s/it] 64%|██████▍   | 128/200 [53:27<27:23, 22.83s/it] 64%|██████▍   | 129/200 [53:50<26:57, 22.78s/it] 65%|██████▌   | 130/200 [54:13<26:32, 22.76s/it] 66%|██████▌   | 131/200 [54:36<26:10, 22.75s/it] 66%|██████▌   | 132/200 [54:58<25:45, 22.73s/it] 66%|██████▋   | 133/200 [55:21<25:22, 22.73s/it] 67%|██████▋   | 134/200 [55:44<24:59, 22.73s/it] 68%|██████▊   | 135/200 [56:06<24:37, 22.73s/it] 68%|██████▊   | 136/200 [56:38<26:57, 25.27s/it] 68%|██████▊   | 137/200 [56:59<25:20, 24.14s/it] 69%|██████▉   | 138/200 [57:22<24:29, 23.70s/it] 70%|██████▉   | 139/200 [57:45<23:47, 23.40s/it] 70%|███████   | 140/200 [58:07<23:04, 23.07s/it] 70%|███████   | 141/200 [58:30<22:34, 22.96s/it] 71%|███████   | 142/200 [58:52<22:07, 22.88s/it] 72%|███████▏  | 143/200 [59:15<21:41, 22.84s/it] 72%|███████▏  | 144/200 [59:38<21:16, 22.80s/it] 72%|███████▎  | 145/200 [1:00:00<20:52, 22.78s/it] 73%|███████▎  | 146/200 [1:00:23<20:29, 22.76s/it] 74%|███████▎  | 147/200 [1:00:46<20:05, 22.75s/it] 74%|███████▍  | 148/200 [1:01:09<19:42, 22.73s/it] 74%|███████▍  | 149/200 [1:01:31<19:18, 22.71s/it] 75%|███████▌  | 150/200 [1:01:54<18:54, 22.70s/it] 76%|███████▌  | 151/200 [1:02:25<20:33, 25.18s/it] 76%|███████▌  | 152/200 [1:02:47<19:18, 24.13s/it] 76%|███████▋  | 153/200 [1:03:09<18:33, 23.68s/it] 77%|███████▋  | 154/200 [1:03:32<17:56, 23.40s/it] 78%|███████▊  | 155/200 [1:03:54<17:17, 23.05s/it] 78%|███████▊  | 156/200 [1:04:17<16:49, 22.95s/it] 78%|███████▊  | 157/200 [1:04:40<16:23, 22.88s/it] 79%|███████▉  | 158/200 [1:05:02<15:59, 22.84s/it] 80%|███████▉  | 159/200 [1:05:25<15:34, 22.79s/it] 80%|████████  | 160/200 [1:05:48<15:10, 22.77s/it] 80%|████████  | 161/200 [1:06:10<14:47, 22.75s/it] 81%|████████  | 162/200 [1:06:33<14:24, 22.74s/it] 82%|████████▏ | 163/200 [1:06:56<14:01, 22.74s/it] 82%|████████▏ | 164/200 [1:07:19<13:38, 22.73s/it] 82%|████████▎ | 165/200 [1:07:41<13:15, 22.74s/it] 83%|████████▎ | 166/200 [1:08:12<14:13, 25.09s/it] 84%|████████▎ | 167/200 [1:08:34<13:17, 24.16s/it] 84%|████████▍ | 168/200 [1:08:57<12:39, 23.73s/it] 84%|████████▍ | 169/200 [1:09:19<12:06, 23.42s/it] 85%|████████▌ | 170/200 [1:09:42<11:31, 23.05s/it] 86%|████████▌ | 171/200 [1:10:04<11:05, 22.94s/it] 86%|████████▌ | 172/200 [1:10:27<10:40, 22.87s/it] 86%|████████▋ | 173/200 [1:10:50<10:16, 22.85s/it] 87%|████████▋ | 174/200 [1:11:12<09:52, 22.79s/it] 88%|████████▊ | 175/200 [1:11:35<09:28, 22.75s/it] 88%|████████▊ | 176/200 [1:11:58<09:05, 22.72s/it] 88%|████████▊ | 177/200 [1:12:20<08:42, 22.72s/it] 89%|████████▉ | 178/200 [1:12:43<08:19, 22.71s/it] 90%|████████▉ | 179/200 [1:13:06<07:56, 22.70s/it] 90%|█████████ | 180/200 [1:13:28<07:33, 22.69s/it] 90%|█████████ | 181/200 [1:13:59<07:56, 25.10s/it] 91%|█████████ | 182/200 [1:14:21<07:14, 24.15s/it] 92%|█████████▏| 183/200 [1:14:44<06:43, 23.72s/it] 92%|█████████▏| 184/200 [1:15:07<06:14, 23.42s/it] 92%|█████████▎| 185/200 [1:15:29<05:46, 23.09s/it] 93%|█████████▎| 186/200 [1:15:52<05:21, 22.96s/it] 94%|█████████▎| 187/200 [1:16:14<04:57, 22.89s/it] 94%|█████████▍| 188/200 [1:16:37<04:34, 22.84s/it] 94%|█████████▍| 189/200 [1:17:00<04:10, 22.81s/it] 95%|█████████▌| 190/200 [1:17:22<03:47, 22.76s/it] 96%|█████████▌| 191/200 [1:17:45<03:24, 22.74s/it] 96%|█████████▌| 192/200 [1:18:08<03:01, 22.74s/it] 96%|█████████▋| 193/200 [1:18:30<02:39, 22.73s/it] 97%|█████████▋| 194/200 [1:18:53<02:16, 22.73s/it] 98%|█████████▊| 195/200 [1:19:16<01:53, 22.72s/it] 98%|█████████▊| 196/200 [1:19:47<01:40, 25.09s/it] 98%|█████████▊| 197/200 [1:20:09<01:12, 24.16s/it] 99%|█████████▉| 198/200 [1:20:31<00:47, 23.71s/it][000001] f: 2.304, acc: 10.00, fv: 2.300, accv: 12.40, lr: 0.0010, time: 7.60
[000063] f: 1.643, acc: 39.18, fv: 3.195, accv: 18.90, lr: 0.0010, time: 7.28
[000125] f: 1.511, acc: 42.89, fv: 3.068, accv: 19.21, lr: 0.0010, time: 7.15
[000187] f: 1.573, acc: 45.01, fv: 2.742, accv: 24.80, lr: 0.0010, time: 7.07
[000249] f: 1.233, acc: 54.59, fv: 3.198, accv: 25.08, lr: 0.0010, time: 7.43
[000250] f: 1.235, acc: 54.53, fv: 2.907, accv: 26.64, lr: 0.0010, time: 7.22
[000251] f: 1.235, acc: 54.10, fv: 2.826, accv: 27.34, lr: 0.0010, time: 7.53
[000313] f: 1.211, acc: 56.71, fv: 2.450, accv: 25.94, lr: 0.0010, time: 7.92
[000375] f: 1.386, acc: 51.67, fv: 2.046, accv: 35.36, lr: 0.0010, time: 7.89
[000437] f: 1.321, acc: 54.34, fv: 2.731, accv: 28.55, lr: 0.0010, time: 7.68
[000499] f: 0.979, acc: 65.87, fv: 1.952, accv: 38.62, lr: 0.0010, time: 7.90
[000501] f: 0.950, acc: 66.18, fv: 2.095, accv: 37.56, lr: 0.0010, time: 7.33
[000563] f: 0.979, acc: 65.31, fv: 3.740, accv: 26.70, lr: 0.0010, time: 7.69
[000625] f: 1.115, acc: 59.94, fv: 2.585, accv: 31.05, lr: 0.0010, time: 7.66
[000687] f: 1.175, acc: 57.82, fv: 2.350, accv: 31.39, lr: 0.0010, time: 7.56
[000749] f: 1.131, acc: 59.09, fv: 2.508, accv: 28.57, lr: 0.0010, time: 7.77
[000751] f: 1.078, acc: 60.09, fv: 2.280, accv: 32.27, lr: 0.0010, time: 7.64
[000813] f: 0.977, acc: 65.85, fv: 2.623, accv: 29.44, lr: 0.0010, time: 7.68
[000875] f: 0.889, acc: 68.73, fv: 2.896, accv: 31.21, lr: 0.0010, time: 7.69
[000937] f: 0.983, acc: 65.18, fv: 3.245, accv: 29.41, lr: 0.0010, time: 7.57
[000999] f: 0.878, acc: 68.44, fv: 2.982, accv: 29.56, lr: 0.0010, time: 7.71
[001001] f: 0.874, acc: 68.07, fv: 2.436, accv: 33.62, lr: 0.0010, time: 7.71
[001063] f: 1.144, acc: 59.43, fv: 2.354, accv: 31.56, lr: 0.0010, time: 7.83
[001125] f: 1.007, acc: 63.21, fv: 2.700, accv: 34.64, lr: 0.0010, time: 7.75
[001187] f: 0.798, acc: 70.81, fv: 2.481, accv: 36.12, lr: 0.0010, time: 7.56
[001249] f: 1.106, acc: 64.61, fv: 4.132, accv: 29.24, lr: 0.0010, time: 7.84
[001750] f: 0.855, acc: 69.89, fv: 3.511, accv: 30.26, lr: 0.0010, time: 7.89
[002250] f: 0.702, acc: 75.36, fv: 2.705, accv: 35.25, lr: 0.0010, time: 8.20
[002750] f: 0.507, acc: 82.24, fv: 2.400, accv: 38.91, lr: 0.0010, time: 8.12
[003250] f: 0.393, acc: 86.36, fv: 2.892, accv: 38.39, lr: 0.0010, time: 8.25
[003750] f: 0.339, acc: 87.98, fv: 2.954, accv: 36.33, lr: 0.0010, time: 8.31
[004000] f: 0.397, acc: 85.63, fv: 3.830, accv: 32.36, lr: 0.0010, time: 8.54
[004250] f: 0.321, acc: 88.45, fv: 2.529, accv: 40.88, lr: 0.0010, time: 8.66
[004750] f: 0.269, acc: 90.60, fv: 3.443, accv: 36.03, lr: 0.0010, time: 8.08
[005250] f: 0.451, acc: 83.18, fv: 4.033, accv: 35.02, lr: 0.0010, time: 8.28
[005750] f: 0.171, acc: 93.88, fv: 3.968, accv: 32.41, lr: 0.0010, time: 8.28
[006250] f: 0.274, acc: 90.20, fv: 5.671, accv: 27.91, lr: 0.0010, time: 8.22
[006750] f: 0.093, acc: 97.32, fv: 3.394, accv: 37.02, lr: 0.0010, time: 8.24
[007250] f: 0.304, acc: 89.31, fv: 6.005, accv: 28.99, lr: 0.0009, time: 8.28
[007750] f: 0.176, acc: 93.56, fv: 4.886, accv: 34.54, lr: 0.0009, time: 8.33
[008250] f: 0.206, acc: 92.15, fv: 4.218, accv: 39.39, lr: 0.0009, time: 8.29
[008750] f: 0.105, acc: 96.35, fv: 4.716, accv: 35.20, lr: 0.0009, time: 8.36
[011500] f: 0.002, acc: 100.00, fv: 4.833, accv: 35.06, lr: 0.0009, time: 8.48
[015250] f: 0.002, acc: 100.00, fv: 6.072, accv: 32.40, lr: 0.0008, time: 8.55
[019000] f: 0.002, acc: 100.00, fv: 5.553, accv: 33.67, lr: 0.0007, time: 8.44
[022750] f: 0.062, acc: 97.88, fv: 5.398, accv: 33.59, lr: 0.0006, time: 8.55
[026500] f: 0.000, acc: 100.00, fv: 6.784, accv: 31.77, lr: 0.0005, time: 8.67
[030250] f: 0.000, acc: 100.00, fv: 6.794, accv: 32.15, lr: 0.0003, time: 8.74
[034000] f: 0.000, acc: 100.00, fv: 7.343, accv: 31.50, lr: 0.0002, time: 8.53
[037750] f: 0.000, acc: 100.00, fv: 7.501, accv: 31.14, lr: 0.0001, time: 8.30
[041500] f: 0.000, acc: 100.00, fv: 7.625, accv: 31.74, lr: 0.0001, time: 8.08
[045250] f: 0.000, acc: 100.00, fv: 7.388, accv: 32.62, lr: 0.0000, time: 8.31
[049000] f: 0.000, acc: 100.00, fv: 7.460, accv: 32.51, lr: 0.0000, time: 8.49
[050000] f: 0.000, acc: 100.00, fv: 7.394, accv: 32.75, lr: 0.0000, time: 7.56
100%|█████████▉| 199/200 [1:20:54<00:23, 23.42s/it]100%|██████████| 200/200 [1:21:22<00:00, 24.95s/it]100%|██████████| 200/200 [1:21:22<00:00, 24.41s/it]
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.302, acc: 10.04, fv: 2.296, accv: 12.65, lr: 0.0010, time: 9.14
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:53<2:58:58, 53.96s/it]  1%|          | 2/200 [01:49<3:01:14, 54.92s/it]  2%|▏         | 3/200 [02:44<3:00:32, 54.99s/it]  2%|▏         | 4/200 [03:40<3:00:34, 55.28s/it]  2%|▎         | 5/200 [04:36<3:00:20, 55.49s/it]  3%|▎         | 6/200 [04:55<2:19:59, 43.30s/it]  4%|▎         | 7/200 [05:23<2:02:36, 38.12s/it]  4%|▍         | 8/200 [05:45<1:45:23, 32.93s/it]  4%|▍         | 9/200 [06:14<1:41:41, 31.94s/it]  5%|▌         | 10/200 [06:37<1:31:33, 28.91s/it]  6%|▌         | 11/200 [07:06<1:32:01, 29.22s/it]  6%|▌         | 12/200 [07:29<1:24:50, 27.08s/it]  6%|▋         | 13/200 [07:59<1:27:08, 27.96s/it]  7%|▋         | 14/200 [08:21<1:21:17, 26.22s/it]  8%|▊         | 15/200 [08:51<1:24:13, 27.32s/it]  8%|▊         | 16/200 [09:21<1:26:36, 28.24s/it]  8%|▊         | 17/200 [09:51<1:27:52, 28.81s/it]  9%|▉         | 18/200 [10:12<1:20:30, 26.54s/it] 10%|▉         | 19/200 [10:42<1:22:39, 27.40s/it] 10%|█         | 20/200 [11:04<1:17:26, 25.81s/it] 10%|█         | 21/200 [11:34<1:20:46, 27.07s/it] 11%|█         | 22/200 [11:56<1:15:55, 25.59s/it] 12%|█▏        | 23/200 [12:26<1:19:21, 26.90s/it] 12%|█▏        | 24/200 [12:48<1:14:40, 25.45s/it] 12%|█▎        | 25/200 [13:18<1:18:02, 26.76s/it] 13%|█▎        | 26/200 [13:40<1:13:29, 25.34s/it] 14%|█▎        | 27/200 [14:10<1:16:55, 26.68s/it] 14%|█▍        | 28/200 [14:32<1:12:34, 25.32s/it] 14%|█▍        | 29/200 [15:02<1:16:05, 26.70s/it] 15%|█▌        | 30/200 [15:24<1:11:45, 25.33s/it] 16%|█▌        | 31/200 [15:54<1:15:15, 26.72s/it] 16%|█▌        | 32/200 [16:16<1:10:55, 25.33s/it] 16%|█▋        | 33/200 [16:46<1:14:29, 26.77s/it] 17%|█▋        | 34/200 [17:08<1:10:03, 25.32s/it] 18%|█▊        | 35/200 [17:38<1:13:31, 26.74s/it] 18%|█▊        | 36/200 [18:00<1:09:11, 25.32s/it] 18%|█▊        | 37/200 [18:22<1:05:58, 24.29s/it] 19%|█▉        | 38/200 [18:45<1:04:09, 23.76s/it] 20%|█▉        | 39/200 [19:07<1:02:39, 23.35s/it] 20%|██        | 40/200 [19:30<1:01:42, 23.14s/it] 20%|██        | 41/200 [19:52<1:00:57, 23.01s/it] 21%|██        | 42/200 [20:15<1:00:19, 22.91s/it] 22%|██▏       | 43/200 [20:38<59:45, 22.84s/it]   22%|██▏       | 44/200 [21:00<59:17, 22.80s/it] 22%|██▎       | 45/200 [21:23<58:48, 22.76s/it] 23%|██▎       | 46/200 [21:54<1:04:43, 25.22s/it] 24%|██▎       | 47/200 [22:16<1:02:03, 24.34s/it] 24%|██▍       | 48/200 [22:38<59:54, 23.65s/it]   24%|██▍       | 49/200 [23:01<58:39, 23.31s/it] 25%|██▌       | 50/200 [23:23<57:32, 23.02s/it] 26%|██▌       | 51/200 [23:46<56:53, 22.91s/it] 26%|██▌       | 52/200 [24:08<56:20, 22.84s/it] 26%|██▋       | 53/200 [24:31<55:45, 22.76s/it] 27%|██▋       | 54/200 [24:54<55:17, 22.72s/it] 28%|██▊       | 55/200 [25:16<54:55, 22.73s/it] 28%|██▊       | 56/200 [25:39<54:30, 22.71s/it] 28%|██▊       | 57/200 [26:02<54:10, 22.73s/it] 29%|██▉       | 58/200 [26:25<53:46, 22.72s/it] 30%|██▉       | 59/200 [26:47<53:17, 22.68s/it] 30%|███       | 60/200 [27:10<52:54, 22.68s/it] 30%|███       | 61/200 [27:41<58:07, 25.09s/it] 31%|███       | 62/200 [28:03<55:38, 24.19s/it] 32%|███▏      | 63/200 [28:25<53:50, 23.58s/it] 32%|███▏      | 64/200 [28:47<52:48, 23.30s/it] 32%|███▎      | 65/200 [29:10<51:48, 23.02s/it] 33%|███▎      | 66/200 [29:32<51:07, 22.89s/it] 34%|███▎      | 67/200 [29:55<50:36, 22.83s/it] 34%|███▍      | 68/200 [30:18<50:09, 22.80s/it] 34%|███▍      | 69/200 [30:41<49:43, 22.78s/it] 35%|███▌      | 70/200 [31:03<49:17, 22.75s/it] 36%|███▌      | 71/200 [31:26<48:54, 22.75s/it] 36%|███▌      | 72/200 [31:49<48:31, 22.75s/it] 36%|███▋      | 73/200 [32:11<48:07, 22.74s/it] 37%|███▋      | 74/200 [32:34<47:38, 22.69s/it] 38%|███▊      | 75/200 [32:57<47:17, 22.70s/it] 38%|███▊      | 76/200 [33:28<52:02, 25.18s/it] 38%|███▊      | 77/200 [33:50<49:45, 24.27s/it] 39%|███▉      | 78/200 [34:12<48:05, 23.65s/it] 40%|███▉      | 79/200 [34:35<47:04, 23.34s/it] 40%|████      | 80/200 [34:57<46:03, 23.03s/it] 40%|████      | 81/200 [35:20<45:29, 22.94s/it] 41%|████      | 82/200 [35:42<44:56, 22.85s/it] 42%|████▏     | 83/200 [36:05<44:28, 22.81s/it] 42%|████▏     | 84/200 [36:28<44:00, 22.76s/it] 42%|████▎     | 85/200 [36:50<43:35, 22.74s/it] 43%|████▎     | 86/200 [37:13<43:09, 22.71s/it] 44%|████▎     | 87/200 [37:36<42:44, 22.69s/it] 44%|████▍     | 88/200 [37:58<42:22, 22.70s/it] 44%|████▍     | 89/200 [38:21<42:01, 22.72s/it] 45%|████▌     | 90/200 [38:44<41:36, 22.69s/it] 46%|████▌     | 91/200 [39:15<45:42, 25.16s/it] 46%|████▌     | 92/200 [39:37<43:37, 24.24s/it] 46%|████▋     | 93/200 [39:59<42:06, 23.62s/it] 47%|████▋     | 94/200 [40:22<41:12, 23.33s/it] 48%|████▊     | 95/200 [40:44<40:15, 23.01s/it] 48%|████▊     | 96/200 [41:07<39:42, 22.91s/it] 48%|████▊     | 97/200 [41:29<39:10, 22.82s/it] 49%|████▉     | 98/200 [41:52<38:42, 22.77s/it] 50%|████▉     | 99/200 [42:14<38:15, 22.72s/it] 50%|█████     | 100/200 [42:37<37:52, 22.72s/it] 50%|█████     | 101/200 [43:00<37:28, 22.71s/it] 51%|█████     | 102/200 [43:22<37:02, 22.68s/it] 52%|█████▏    | 103/200 [43:45<36:37, 22.65s/it] 52%|█████▏    | 104/200 [44:08<36:13, 22.64s/it] 52%|█████▎    | 105/200 [44:30<35:51, 22.65s/it] 53%|█████▎    | 106/200 [45:01<39:22, 25.13s/it] 54%|█████▎    | 107/200 [45:23<37:28, 24.18s/it] 54%|█████▍    | 108/200 [45:46<36:16, 23.65s/it] 55%|█████▍    | 109/200 [46:08<35:25, 23.36s/it] 55%|█████▌    | 110/200 [46:30<34:26, 22.97s/it] 56%|█████▌    | 111/200 [46:53<33:56, 22.88s/it] 56%|█████▌    | 112/200 [47:16<33:29, 22.84s/it] 56%|█████▋    | 113/200 [47:38<33:03, 22.79s/it] 57%|█████▋    | 114/200 [48:01<32:36, 22.75s/it] 57%|█████▊    | 115/200 [48:24<32:13, 22.74s/it] 58%|█████▊    | 116/200 [48:47<31:48, 22.72s/it] 58%|█████▊    | 117/200 [49:09<31:23, 22.70s/it] 59%|█████▉    | 118/200 [49:32<30:59, 22.68s/it] 60%|█████▉    | 119/200 [49:55<30:37, 22.68s/it] 60%|██████    | 120/200 [50:17<30:14, 22.69s/it] 60%|██████    | 121/200 [50:48<33:02, 25.10s/it] 61%|██████    | 122/200 [51:10<31:22, 24.14s/it] 62%|██████▏   | 123/200 [51:32<30:23, 23.68s/it] 62%|██████▏   | 124/200 [51:55<29:38, 23.40s/it] 62%|██████▎   | 125/200 [52:17<28:47, 23.03s/it] 63%|██████▎   | 126/200 [52:40<28:16, 22.93s/it] 64%|██████▎   | 127/200 [53:03<27:50, 22.88s/it] 64%|██████▍   | 128/200 [53:26<27:23, 22.83s/it] 64%|██████▍   | 129/200 [53:48<26:57, 22.79s/it] 65%|██████▌   | 130/200 [54:11<26:34, 22.78s/it] 66%|██████▌   | 131/200 [54:34<26:09, 22.75s/it] 66%|██████▌   | 132/200 [54:56<25:45, 22.72s/it] 66%|██████▋   | 133/200 [55:19<25:21, 22.71s/it] 67%|██████▋   | 134/200 [55:42<24:58, 22.70s/it] 68%|██████▊   | 135/200 [56:04<24:33, 22.67s/it] 68%|██████▊   | 136/200 [56:35<26:44, 25.07s/it] 68%|██████▊   | 137/200 [56:57<25:22, 24.16s/it] 69%|██████▉   | 138/200 [57:20<24:30, 23.72s/it] 70%|██████▉   | 139/200 [57:42<23:45, 23.37s/it] 70%|███████   | 140/200 [58:04<23:01, 23.02s/it] 70%|███████   | 141/200 [58:27<22:32, 22.92s/it] 71%|███████   | 142/200 [58:50<22:05, 22.86s/it] 72%|███████▏  | 143/200 [59:13<21:39, 22.80s/it] 72%|███████▏  | 144/200 [59:35<21:15, 22.77s/it] 72%|███████▎  | 145/200 [59:58<20:50, 22.73s/it] 73%|███████▎  | 146/200 [1:00:21<20:26, 22.71s/it] 74%|███████▎  | 147/200 [1:00:43<20:02, 22.68s/it] 74%|███████▍  | 148/200 [1:01:06<19:38, 22.66s/it] 74%|███████▍  | 149/200 [1:01:28<19:15, 22.65s/it] 75%|███████▌  | 150/200 [1:01:51<18:52, 22.65s/it] 76%|███████▌  | 151/200 [1:02:22<20:25, 25.01s/it] 76%|███████▌  | 152/200 [1:02:44<19:18, 24.15s/it] 76%|███████▋  | 153/200 [1:03:06<18:34, 23.70s/it] 77%|███████▋  | 154/200 [1:03:29<17:56, 23.40s/it] 78%|███████▊  | 155/200 [1:03:51<17:17, 23.06s/it] 78%|███████▊  | 156/200 [1:04:14<16:49, 22.95s/it] 78%|███████▊  | 157/200 [1:04:37<16:23, 22.86s/it] 79%|███████▉  | 158/200 [1:04:59<15:58, 22.82s/it] 80%|███████▉  | 159/200 [1:05:22<15:34, 22.79s/it] 80%|████████  | 160/200 [1:05:45<15:09, 22.74s/it] 80%|████████  | 161/200 [1:06:07<14:46, 22.73s/it] 81%|████████  | 162/200 [1:06:30<14:22, 22.70s/it] 82%|████████▏ | 163/200 [1:06:53<13:59, 22.68s/it] 82%|████████▏ | 164/200 [1:07:15<13:35, 22.66s/it] 82%|████████▎ | 165/200 [1:07:38<13:12, 22.65s/it] 83%|████████▎ | 166/200 [1:08:08<14:09, 24.99s/it] 84%|████████▎ | 167/200 [1:08:30<13:16, 24.12s/it] 84%|████████▍ | 168/200 [1:08:53<12:37, 23.69s/it] 84%|████████▍ | 169/200 [1:09:16<12:04, 23.37s/it] 85%|████████▌ | 170/200 [1:09:38<11:30, 23.02s/it] 86%|████████▌ | 171/200 [1:10:01<11:04, 22.92s/it] 86%|████████▌ | 172/200 [1:10:23<10:39, 22.84s/it] 86%|████████▋ | 173/200 [1:10:46<10:15, 22.78s/it] 87%|████████▋ | 174/200 [1:11:09<09:51, 22.76s/it] 88%|████████▊ | 175/200 [1:11:31<09:27, 22.72s/it] 88%|████████▊ | 176/200 [1:11:54<09:05, 22.73s/it] 88%|████████▊ | 177/200 [1:12:17<08:42, 22.72s/it] 89%|████████▉ | 178/200 [1:12:39<08:19, 22.71s/it] 90%|████████▉ | 179/200 [1:13:02<07:56, 22.70s/it] 90%|█████████ | 180/200 [1:13:25<07:33, 22.67s/it] 90%|█████████ | 181/200 [1:13:55<07:53, 24.94s/it] 91%|█████████ | 182/200 [1:14:17<07:13, 24.10s/it] 92%|█████████▏| 183/200 [1:14:40<06:42, 23.68s/it] 92%|█████████▏| 184/200 [1:15:02<06:14, 23.38s/it] 92%|█████████▎| 185/200 [1:15:25<05:45, 23.02s/it] 93%|█████████▎| 186/200 [1:15:47<05:21, 22.93s/it] 94%|█████████▎| 187/200 [1:16:10<04:57, 22.87s/it] 94%|█████████▍| 188/200 [1:16:33<04:33, 22.81s/it] 94%|█████████▍| 189/200 [1:16:55<04:10, 22.77s/it] 95%|█████████▌| 190/200 [1:17:18<03:47, 22.74s/it] 96%|█████████▌| 191/200 [1:17:41<03:24, 22.74s/it] 96%|█████████▌| 192/200 [1:18:04<03:01, 22.72s/it] 96%|█████████▋| 193/200 [1:18:26<02:38, 22.70s/it] 97%|█████████▋| 194/200 [1:18:49<02:16, 22.70s/it] 98%|█████████▊| 195/200 [1:19:12<01:53, 22.68s/it] 98%|█████████▊| 196/200 [1:19:42<01:39, 24.96s/it] 98%|█████████▊| 197/200 [1:20:04<01:12, 24.18s/it] 99%|█████████▉| 198/200 [1:20:27<00:47, 23.73s/it][000001] f: 2.301, acc: 10.16, fv: 2.292, accv: 14.64, lr: 0.0010, time: 7.51
[000063] f: 1.813, acc: 31.34, fv: 4.028, accv: 13.95, lr: 0.0010, time: 7.20
[000125] f: 1.846, acc: 35.19, fv: 4.033, accv: 18.25, lr: 0.0010, time: 7.03
[000187] f: 1.703, acc: 41.11, fv: 4.328, accv: 17.56, lr: 0.0010, time: 7.13
[000249] f: 1.707, acc: 40.30, fv: 4.277, accv: 16.36, lr: 0.0010, time: 7.27
[000250] f: 1.516, acc: 44.32, fv: 3.304, accv: 18.58, lr: 0.0010, time: 7.58
[000251] f: 1.306, acc: 53.01, fv: 2.824, accv: 17.42, lr: 0.0010, time: 7.70
[000313] f: 1.202, acc: 56.11, fv: 2.401, accv: 27.76, lr: 0.0010, time: 7.61
[000375] f: 1.631, acc: 43.87, fv: 3.520, accv: 23.44, lr: 0.0010, time: 7.56
[000437] f: 1.082, acc: 62.22, fv: 3.339, accv: 21.57, lr: 0.0010, time: 7.72
[000499] f: 1.090, acc: 60.69, fv: 2.488, accv: 30.36, lr: 0.0010, time: 7.43
[000501] f: 1.253, acc: 54.91, fv: 2.436, accv: 33.44, lr: 0.0010, time: 7.72
[000563] f: 1.446, acc: 50.49, fv: 4.091, accv: 20.30, lr: 0.0010, time: 7.97
[000625] f: 1.011, acc: 63.44, fv: 2.299, accv: 32.83, lr: 0.0010, time: 8.10
[000687] f: 1.054, acc: 63.37, fv: 3.356, accv: 19.10, lr: 0.0010, time: 7.76
[000749] f: 0.876, acc: 69.32, fv: 2.487, accv: 29.59, lr: 0.0010, time: 7.73
[000751] f: 0.923, acc: 67.19, fv: 2.592, accv: 29.97, lr: 0.0010, time: 7.86
[000813] f: 1.114, acc: 60.93, fv: 3.801, accv: 28.60, lr: 0.0010, time: 8.02
[000875] f: 1.158, acc: 57.66, fv: 3.007, accv: 26.66, lr: 0.0010, time: 8.07
[000937] f: 0.896, acc: 68.07, fv: 2.440, accv: 34.12, lr: 0.0010, time: 7.72
[000999] f: 0.806, acc: 72.17, fv: 1.989, accv: 37.22, lr: 0.0010, time: 7.85
[001001] f: 0.906, acc: 67.35, fv: 1.832, accv: 39.97, lr: 0.0010, time: 7.77
[001063] f: 0.947, acc: 66.31, fv: 2.502, accv: 33.15, lr: 0.0010, time: 8.01
[001125] f: 1.031, acc: 63.57, fv: 2.510, accv: 34.05, lr: 0.0010, time: 8.06
[001187] f: 1.105, acc: 63.00, fv: 2.702, accv: 32.48, lr: 0.0010, time: 7.86
[001249] f: 0.786, acc: 72.08, fv: 2.031, accv: 38.92, lr: 0.0010, time: 8.06
[001750] f: 0.831, acc: 70.93, fv: 2.527, accv: 39.42, lr: 0.0010, time: 8.15
[002250] f: 0.552, acc: 81.22, fv: 2.205, accv: 41.69, lr: 0.0010, time: 8.10
[002750] f: 0.871, acc: 70.32, fv: 3.223, accv: 33.72, lr: 0.0010, time: 8.13
[003250] f: 0.527, acc: 80.51, fv: 3.757, accv: 32.39, lr: 0.0010, time: 8.17
[003750] f: 0.319, acc: 88.90, fv: 3.263, accv: 37.48, lr: 0.0010, time: 8.25
[004000] f: 0.474, acc: 83.16, fv: 3.949, accv: 35.55, lr: 0.0010, time: 8.53
[004250] f: 0.651, acc: 80.28, fv: 4.594, accv: 33.80, lr: 0.0010, time: 8.54
[004750] f: 0.502, acc: 82.72, fv: 4.051, accv: 34.71, lr: 0.0010, time: 8.41
[005250] f: 0.508, acc: 82.61, fv: 5.588, accv: 32.24, lr: 0.0010, time: 8.40
[005750] f: 0.159, acc: 94.55, fv: 4.037, accv: 38.25, lr: 0.0010, time: 8.40
[006250] f: 0.682, acc: 77.02, fv: 5.200, accv: 29.55, lr: 0.0010, time: 8.25
[006750] f: 0.219, acc: 92.33, fv: 4.115, accv: 39.12, lr: 0.0010, time: 8.29
[007250] f: 0.154, acc: 94.50, fv: 6.927, accv: 26.70, lr: 0.0009, time: 8.40
[007750] f: 0.140, acc: 94.87, fv: 4.396, accv: 38.42, lr: 0.0009, time: 8.44
[008250] f: 0.293, acc: 90.06, fv: 6.451, accv: 25.55, lr: 0.0009, time: 8.52
[008750] f: 0.285, acc: 90.27, fv: 6.124, accv: 37.00, lr: 0.0009, time: 8.57
[011500] f: 0.003, acc: 100.00, fv: 4.686, accv: 38.74, lr: 0.0009, time: 8.47
[015250] f: 0.004, acc: 100.00, fv: 5.971, accv: 36.98, lr: 0.0008, time: 8.38
[019000] f: 0.004, acc: 99.99, fv: 5.005, accv: 38.07, lr: 0.0007, time: 8.44
[022750] f: 0.000, acc: 100.00, fv: 5.911, accv: 36.10, lr: 0.0006, time: 8.52
[026500] f: 0.000, acc: 100.00, fv: 6.135, accv: 36.66, lr: 0.0005, time: 8.49
[030250] f: 0.000, acc: 100.00, fv: 6.492, accv: 35.77, lr: 0.0003, time: 8.34
[034000] f: 0.000, acc: 100.00, fv: 5.758, accv: 38.84, lr: 0.0002, time: 8.25
[037750] f: 0.000, acc: 100.00, fv: 6.078, accv: 37.30, lr: 0.0001, time: 8.25
[041500] f: 0.000, acc: 100.00, fv: 6.588, accv: 36.63, lr: 0.0001, time: 8.10
[045250] f: 0.000, acc: 100.00, fv: 6.744, accv: 36.58, lr: 0.0000, time: 8.25
[049000] f: 0.000, acc: 100.00, fv: 6.928, accv: 35.91, lr: 0.0000, time: 8.17
[050000] f: 0.000, acc: 100.00, fv: 6.864, accv: 35.92, lr: 0.0000, time: 7.52
100%|█████████▉| 199/200 [1:20:49<00:23, 23.40s/it]100%|██████████| 200/200 [1:21:17<00:00, 24.73s/it]100%|██████████| 200/200 [1:21:17<00:00, 24.39s/it]
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.305, acc: 10.00, fv: 2.303, accv: 10.44, lr: 0.0010, time: 8.42
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 0/200 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 143, in main
    ss = fit(m, ds, epochs=args.epochs, bs=optim_args['bs'], autocast=optim_args['autocast'], opt=optimizer, sched=scheduler, fix_batch=fix_batch)
  File "runner.py", line 64, in fit
    yyh = m(x)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ubuntu/results/inpca/networks/wr.py", line 74, in forward
    out = self.layer2(out)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ubuntu/results/inpca/networks/wr.py", line 30, in forward
    out = self.conv2(F.relu(self.bn2(out)))
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/functional.py", line 1299, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 22.20 GiB total capacity; 217.01 MiB already allocated; 16.06 MiB free; 276.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.310, acc: 9.61, fv: 2.309, accv: 7.74, lr: 0.0010, time: 7.81
[000001] f: 2.308, acc: 11.22, fv: 2.304, accv: 13.34, lr: 0.0010, time: 7.10
[000063] f: 1.622, acc: 39.81, fv: 2.760, accv: 17.06, lr: 0.0010, time: 7.29
[000125] f: 1.518, acc: 42.85, fv: 3.747, accv: 16.83, lr: 0.0010, time: 7.53
[000187] f: 1.331, acc: 53.07, fv: 3.143, accv: 17.52, lr: 0.0010, time: 7.82
[000249] f: 1.428, acc: 47.78, fv: 2.622, accv: 29.70, lr: 0.0010, time: 7.80
[000250] f: 1.324, acc: 51.73, fv: 2.783, accv: 27.81, lr: 0.0010, time: 7.61
[000251] f: 1.224, acc: 55.71, fv: 2.846, accv: 25.54, lr: 0.0010, time: 7.46
[000313] f: 1.165, acc: 58.70, fv: 2.563, accv: 22.00, lr: 0.0010, time: 7.46
[000375] f: 1.314, acc: 53.24, fv: 2.377, accv: 27.41, lr: 0.0010, time: 7.66
[000437] f: 1.056, acc: 62.37, fv: 3.121, accv: 19.10, lr: 0.0010, time: 7.77
[000499] f: 1.072, acc: 61.82, fv: 3.892, accv: 17.10, lr: 0.0010, time: 7.98
[000501] f: 1.134, acc: 59.29, fv: 3.429, accv: 19.19, lr: 0.0010, time: 7.66
[000563] f: 1.162, acc: 58.94, fv: 2.712, accv: 30.21, lr: 0.0010, time: 7.63
[000625] f: 0.938, acc: 66.04, fv: 2.210, accv: 33.52, lr: 0.0010, time: 7.74
[000687] f: 1.114, acc: 60.11, fv: 2.460, accv: 25.48, lr: 0.0010, time: 7.90
[000749] f: 1.255, acc: 55.85, fv: 2.681, accv: 26.82, lr: 0.0010, time: 7.94
[000751] f: 1.057, acc: 62.11, fv: 2.729, accv: 26.47, lr: 0.0010, time: 7.75
[000813] f: 0.937, acc: 67.23, fv: 2.675, accv: 27.94, lr: 0.0010, time: 7.73
[000875] f: 0.971, acc: 66.15, fv: 2.392, accv: 27.41, lr: 0.0010, time: 7.62
[000937] f: 1.022, acc: 65.06, fv: 2.921, accv: 29.62, lr: 0.0010, time: 7.90
[000999] f: 0.954, acc: 65.51, fv: 2.066, accv: 31.37, lr: 0.0010, time: 7.93
[001001] f: 0.952, acc: 65.71, fv: 2.062, accv: 31.75, lr: 0.0010, time: 7.60
[001063] f: 0.896, acc: 68.29, fv: 2.551, accv: 33.33, lr: 0.0010, time: 7.79
[001125] f: 0.733, acc: 73.94, fv: 1.847, accv: 38.15, lr: 0.0010, time: 8.34
[001187] f: 0.844, acc: 69.63, fv: 3.367, accv: 20.75, lr: 0.0010, time: 8.26
[001249] f: 0.867, acc: 68.45, fv: 2.838, accv: 26.47, lr: 0.0010, time: 8.26
[001750] f: 0.724, acc: 74.41, fv: 2.970, accv: 23.69, lr: 0.0010, time: 8.61
[002250] f: 0.743, acc: 74.10, fv: 3.130, accv: 29.45, lr: 0.0010, time: 8.60
[002750] f: 0.654, acc: 77.39, fv: 2.837, accv: 35.15, lr: 0.0010, time: 8.54
[003250] f: 0.393, acc: 86.28, fv: 3.681, accv: 28.68, lr: 0.0010, time: 8.75
[003750] f: 0.647, acc: 77.34, fv: 3.597, accv: 28.16, lr: 0.0010, time: 8.33
[004000] f: 0.406, acc: 84.62, fv: 2.962, accv: 35.03, lr: 0.0010, time: 8.26
[004250] f: 0.311, acc: 88.65, fv: 4.369, accv: 29.43, lr: 0.0010, time: 8.53
[004750] f: 0.445, acc: 84.41, fv: 4.155, accv: 27.98, lr: 0.0010, time: 8.54
[005250] f: 0.232, acc: 91.65, fv: 4.002, accv: 31.50, lr: 0.0010, time: 8.72
[005750] f: 0.199, acc: 93.06, fv: 4.283, accv: 30.31, lr: 0.0010, time: 8.77
[006250] f: 0.180, acc: 93.63, fv: 6.276, accv: 24.56, lr: 0.0010, time: 8.85
[006750] f: 0.227, acc: 91.35, fv: 4.532, accv: 29.68, lr: 0.0010, time: 8.62
[007250] f: 0.115, acc: 96.12, fv: 6.314, accv: 26.83, lr: 0.0009, time: 8.46
[007750] f: 0.107, acc: 96.38, fv: 5.628, accv: 29.09, lr: 0.0009, time: 8.52
[008250] f: 0.144, acc: 94.65, fv: 4.750, accv: 31.11, lr: 0.0009, time: 8.61
[008750] f: 0.118, acc: 95.81, fv: 5.586, accv: 29.82, lr: 0.0009, time: 8.74
[011500] f: 0.002, acc: 100.00, fv: 5.891, accv: 30.42, lr: 0.0009, time: 8.63
[015250] f: 0.004, acc: 100.00, fv: 5.949, accv: 30.76, lr: 0.0008, time: 8.72
[019000] f: 0.157, acc: 94.36, fv: 5.875, accv: 30.94, lr: 0.0007, time: 8.51
[022750] f: 0.001, acc: 100.00, fv: 7.563, accv: 26.97, lr: 0.0006, time: 8.49
[026500] f: 0.000, acc: 100.00, fv: 7.713, accv: 27.28, lr: 0.0005, time: 8.61
[030250] f: 0.000, acc: 100.00, fv: 8.253, accv: 26.66, lr: 0.0003, time: 8.31
[034000] f: 0.000, acc: 100.00, fv: 7.969, accv: 28.00, lr: 0.0002, time: 8.15
[037750] f: 0.000, acc: 100.00, fv: 8.811, accv: 26.53, lr: 0.0001, time: 8.03
[041500] f: 0.000, acc: 100.00, fv: 9.021, accv: 26.26, lr: 0.0001, time: 8.23
[045250] f: 0.000, acc: 100.00, fv: 9.124, accv: 26.85, lr: 0.0000, time: 8.63
[049000] f: 0.000, acc: 100.00, fv: 9.131, accv: 26.63, lr: 0.0000, time: 8.73
[050000] f: 0.000, acc: 100.00, fv: 9.032, accv: 26.89, lr: 0.0000, time: 8.51
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:01<3:22:53, 61.17s/it]  1%|          | 2/200 [01:56<3:09:46, 57.51s/it]  2%|▏         | 3/200 [02:50<3:04:32, 56.21s/it]  2%|▏         | 4/200 [03:46<3:02:44, 55.94s/it]  2%|▎         | 5/200 [04:45<3:05:39, 57.13s/it]  3%|▎         | 6/200 [05:05<2:23:43, 44.45s/it]  4%|▎         | 7/200 [05:35<2:08:22, 39.91s/it]  4%|▍         | 8/200 [05:56<1:47:59, 33.75s/it]  4%|▍         | 9/200 [06:26<1:44:04, 32.69s/it]  5%|▌         | 10/200 [06:47<1:31:46, 28.98s/it]  6%|▌         | 11/200 [07:18<1:32:45, 29.45s/it]  6%|▌         | 12/200 [07:38<1:23:42, 26.71s/it]  6%|▋         | 13/200 [08:09<1:27:01, 27.92s/it]  7%|▋         | 14/200 [08:29<1:19:27, 25.63s/it]  8%|▊         | 15/200 [08:58<1:22:31, 26.76s/it]  8%|▊         | 16/200 [09:28<1:24:22, 27.52s/it]  8%|▊         | 17/200 [09:58<1:26:16, 28.29s/it]  9%|▉         | 18/200 [10:18<1:18:20, 25.83s/it] 10%|▉         | 19/200 [10:48<1:22:06, 27.22s/it] 10%|█         | 20/200 [11:09<1:15:32, 25.18s/it] 10%|█         | 21/200 [11:39<1:19:56, 26.80s/it] 11%|█         | 22/200 [12:00<1:13:48, 24.88s/it] 12%|█▏        | 23/200 [12:31<1:18:35, 26.64s/it] 12%|█▏        | 24/200 [12:51<1:12:54, 24.86s/it] 12%|█▎        | 25/200 [13:22<1:17:40, 26.63s/it] 13%|█▎        | 26/200 [13:43<1:12:01, 24.83s/it] 14%|█▎        | 27/200 [14:13<1:16:31, 26.54s/it] 14%|█▍        | 28/200 [14:34<1:11:02, 24.78s/it] 14%|█▍        | 29/200 [15:04<1:15:31, 26.50s/it] 15%|█▌        | 30/200 [15:25<1:10:14, 24.79s/it] 16%|█▌        | 31/200 [15:55<1:14:29, 26.45s/it] 16%|█▌        | 32/200 [16:16<1:09:24, 24.79s/it] 16%|█▋        | 33/200 [16:47<1:13:38, 26.46s/it] 17%|█▋        | 34/200 [17:08<1:08:34, 24.79s/it] 18%|█▊        | 35/200 [17:38<1:12:52, 26.50s/it] 18%|█▊        | 36/200 [18:00<1:08:38, 25.11s/it] 18%|█▊        | 37/200 [18:22<1:05:35, 24.15s/it] 19%|█▉        | 38/200 [18:44<1:03:44, 23.61s/it] 20%|█▉        | 39/200 [19:07<1:02:26, 23.27s/it] 20%|██        | 40/200 [19:29<1:01:21, 23.01s/it] 20%|██        | 41/200 [19:52<1:00:33, 22.85s/it] 21%|██        | 42/200 [20:14<59:49, 22.72s/it]   22%|██▏       | 43/200 [20:36<59:06, 22.59s/it] 22%|██▏       | 44/200 [20:59<58:37, 22.55s/it] 22%|██▎       | 45/200 [21:21<57:42, 22.34s/it] 23%|██▎       | 46/200 [21:51<1:03:43, 24.83s/it] 24%|██▎       | 47/200 [22:13<1:01:03, 23.94s/it] 24%|██▍       | 48/200 [22:35<59:19, 23.42s/it]   24%|██▍       | 49/200 [22:57<58:02, 23.06s/it] 25%|██▌       | 50/200 [23:20<57:06, 22.84s/it] 26%|██▌       | 51/200 [23:42<56:27, 22.74s/it] 26%|██▌       | 52/200 [24:05<55:53, 22.66s/it] 26%|██▋       | 53/200 [24:27<55:19, 22.58s/it] 27%|██▋       | 54/200 [24:50<54:47, 22.52s/it] 28%|██▊       | 55/200 [25:12<54:22, 22.50s/it] 28%|██▊       | 56/200 [25:34<53:50, 22.44s/it] 28%|██▊       | 57/200 [25:57<53:27, 22.43s/it] 29%|██▉       | 58/200 [26:19<53:12, 22.48s/it] 30%|██▉       | 59/200 [26:42<52:46, 22.46s/it] 30%|███       | 60/200 [27:04<52:10, 22.36s/it] 30%|███       | 61/200 [27:34<57:19, 24.74s/it] 31%|███       | 62/200 [27:56<54:52, 23.86s/it] 32%|███▏      | 63/200 [28:18<53:29, 23.43s/it] 32%|███▏      | 64/200 [28:40<52:09, 23.01s/it] 32%|███▎      | 65/200 [29:03<51:28, 22.88s/it] 33%|███▎      | 66/200 [29:25<50:42, 22.70s/it] 34%|███▎      | 67/200 [29:48<50:04, 22.59s/it] 34%|███▍      | 68/200 [30:10<49:36, 22.55s/it] 34%|███▍      | 69/200 [30:32<49:06, 22.50s/it] 35%|███▌      | 70/200 [30:55<48:39, 22.46s/it] 36%|███▌      | 71/200 [31:17<48:11, 22.42s/it] 36%|███▌      | 72/200 [31:40<47:49, 22.42s/it] 36%|███▋      | 73/200 [32:02<47:29, 22.43s/it] 37%|███▋      | 74/200 [32:25<47:08, 22.45s/it] 38%|███▊      | 75/200 [32:47<46:43, 22.43s/it] 38%|███▊      | 76/200 [33:17<51:00, 24.68s/it] 38%|███▊      | 77/200 [33:39<48:50, 23.82s/it] 39%|███▉      | 78/200 [34:01<47:38, 23.43s/it] 40%|███▉      | 79/200 [34:23<46:21, 22.99s/it] 40%|████      | 80/200 [34:46<45:39, 22.83s/it] 40%|████      | 81/200 [35:08<45:03, 22.72s/it] 41%|████      | 82/200 [35:30<44:27, 22.61s/it] 42%|████▏     | 83/200 [35:53<43:54, 22.52s/it] 42%|████▏     | 84/200 [36:15<43:30, 22.50s/it] 42%|████▎     | 85/200 [36:38<43:05, 22.48s/it] 43%|████▎     | 86/200 [37:00<42:44, 22.49s/it] 44%|████▎     | 87/200 [37:23<42:22, 22.50s/it] 44%|████▍     | 88/200 [37:45<41:55, 22.46s/it] 44%|████▍     | 89/200 [38:07<41:31, 22.45s/it] 45%|████▌     | 90/200 [38:30<41:09, 22.45s/it] 46%|████▌     | 91/200 [39:00<44:55, 24.73s/it] 46%|████▌     | 92/200 [39:22<42:56, 23.86s/it] 46%|████▋     | 93/200 [39:44<41:48, 23.45s/it] 47%|████▋     | 94/200 [40:06<40:37, 23.00s/it] 48%|████▊     | 95/200 [40:29<39:56, 22.82s/it] 48%|████▊     | 96/200 [40:51<39:19, 22.68s/it] 48%|████▊     | 97/200 [41:13<38:49, 22.62s/it] 49%|████▉     | 98/200 [41:36<38:21, 22.57s/it] 50%|████▉     | 99/200 [41:58<37:55, 22.53s/it] 50%|█████     | 100/200 [42:21<37:27, 22.48s/it] 50%|█████     | 101/200 [42:43<37:03, 22.46s/it] 51%|█████     | 102/200 [43:05<36:38, 22.43s/it] 52%|█████▏    | 103/200 [43:28<36:17, 22.45s/it] 52%|█████▏    | 104/200 [43:50<35:52, 22.43s/it] 52%|█████▎    | 105/200 [44:13<35:31, 22.44s/it] 53%|█████▎    | 106/200 [44:43<38:37, 24.66s/it] 54%|█████▎    | 107/200 [45:05<36:59, 23.86s/it] 54%|█████▍    | 108/200 [45:27<35:55, 23.43s/it] 55%|█████▍    | 109/200 [45:49<34:51, 22.99s/it] 55%|█████▌    | 110/200 [46:12<34:15, 22.84s/it] 56%|█████▌    | 111/200 [46:34<33:40, 22.70s/it] 56%|█████▌    | 112/200 [46:56<33:11, 22.63s/it] 56%|█████▋    | 113/200 [47:19<32:42, 22.56s/it] 57%|█████▋    | 114/200 [47:41<32:17, 22.53s/it] 57%|█████▊    | 115/200 [48:04<31:51, 22.48s/it] 58%|█████▊    | 116/200 [48:26<31:27, 22.47s/it] 58%|█████▊    | 117/200 [48:48<31:03, 22.45s/it] 59%|█████▉    | 118/200 [49:11<30:42, 22.47s/it] 60%|█████▉    | 119/200 [49:33<30:16, 22.43s/it] 60%|██████    | 120/200 [49:56<29:51, 22.39s/it] 60%|██████    | 121/200 [50:25<32:26, 24.64s/it] 61%|██████    | 122/200 [50:47<30:57, 23.81s/it] 62%|██████▏   | 123/200 [51:10<30:01, 23.40s/it] 62%|██████▏   | 124/200 [51:32<29:11, 23.04s/it] 62%|██████▎   | 125/200 [51:54<28:26, 22.75s/it] 63%|██████▎   | 126/200 [52:16<27:53, 22.62s/it] 64%|██████▎   | 127/200 [52:39<27:24, 22.52s/it] 64%|██████▍   | 128/200 [53:01<27:00, 22.51s/it] 64%|██████▍   | 129/200 [53:24<26:36, 22.48s/it] 65%|██████▌   | 130/200 [53:46<26:12, 22.47s/it] 66%|██████▌   | 131/200 [54:08<25:49, 22.45s/it] 66%|██████▌   | 132/200 [54:31<25:25, 22.43s/it] 66%|██████▋   | 133/200 [54:53<25:04, 22.46s/it] 67%|██████▋   | 134/200 [55:16<24:41, 22.44s/it] 68%|██████▊   | 135/200 [55:38<24:20, 22.47s/it] 68%|██████▊   | 136/200 [56:08<26:26, 24.80s/it] 68%|██████▊   | 137/200 [56:30<25:07, 23.93s/it] 69%|██████▉   | 138/200 [56:53<24:16, 23.49s/it] 70%|██████▉   | 139/200 [57:15<23:33, 23.17s/it] 70%|███████   | 140/200 [57:37<22:48, 22.81s/it] 70%|███████   | 141/200 [58:00<22:18, 22.69s/it] 71%|███████   | 142/200 [58:22<21:52, 22.62s/it] 72%|███████▏  | 143/200 [58:45<21:26, 22.57s/it] 72%|███████▏  | 144/200 [59:07<21:02, 22.55s/it] 72%|███████▎  | 145/200 [59:29<20:37, 22.51s/it] 73%|███████▎  | 146/200 [59:52<20:13, 22.47s/it] 74%|███████▎  | 147/200 [1:00:14<19:49, 22.45s/it] 74%|███████▍  | 148/200 [1:00:37<19:28, 22.47s/it] 74%|███████▍  | 149/200 [1:00:59<19:04, 22.45s/it] 75%|███████▌  | 150/200 [1:01:22<18:42, 22.45s/it] 76%|███████▌  | 151/200 [1:01:52<20:16, 24.82s/it] 76%|███████▌  | 152/200 [1:02:14<19:05, 23.87s/it] 76%|███████▋  | 153/200 [1:02:36<18:21, 23.44s/it] 77%|███████▋  | 154/200 [1:02:58<17:43, 23.13s/it] 78%|███████▊  | 155/200 [1:03:20<17:04, 22.78s/it] 78%|███████▊  | 156/200 [1:03:43<16:37, 22.67s/it] 78%|███████▊  | 157/200 [1:04:05<16:12, 22.63s/it] 79%|███████▉  | 158/200 [1:04:28<15:47, 22.55s/it] 80%|███████▉  | 159/200 [1:04:50<15:21, 22.47s/it] 80%|████████  | 160/200 [1:05:12<14:58, 22.46s/it] 80%|████████  | 161/200 [1:05:35<14:34, 22.43s/it] 81%|████████  | 162/200 [1:05:57<14:11, 22.41s/it] 82%|████████▏ | 163/200 [1:06:20<13:49, 22.42s/it] 82%|████████▏ | 164/200 [1:06:42<13:27, 22.42s/it] 82%|████████▎ | 165/200 [1:07:04<13:03, 22.40s/it] 83%|████████▎ | 166/200 [1:07:35<14:04, 24.85s/it] 84%|████████▎ | 167/200 [1:07:56<13:05, 23.81s/it] 84%|████████▍ | 168/200 [1:08:19<12:27, 23.37s/it] 84%|████████▍ | 169/200 [1:08:41<11:56, 23.11s/it] 85%|████████▌ | 170/200 [1:09:03<11:22, 22.75s/it] 86%|████████▌ | 171/200 [1:09:25<10:56, 22.64s/it] 86%|████████▌ | 172/200 [1:09:48<10:32, 22.59s/it] 86%|████████▋ | 173/200 [1:10:10<10:08, 22.54s/it] 87%|████████▋ | 174/200 [1:10:33<09:45, 22.51s/it] 88%|████████▊ | 175/200 [1:10:55<09:22, 22.50s/it] 88%|████████▊ | 176/200 [1:11:18<08:59, 22.49s/it] 88%|████████▊ | 177/200 [1:11:40<08:37, 22.48s/it] 89%|████████▉ | 178/200 [1:12:03<08:13, 22.44s/it] 90%|████████▉ | 179/200 [1:12:25<07:51, 22.45s/it] 90%|█████████ | 180/200 [1:12:48<07:29, 22.48s/it] 90%|█████████ | 181/200 [1:13:19<07:56, 25.06s/it] 91%|█████████ | 182/200 [1:13:40<07:08, 23.80s/it] 92%|█████████▏| 183/200 [1:14:02<06:37, 23.37s/it] 92%|█████████▏| 184/200 [1:14:24<06:08, 23.06s/it] 92%|█████████▎| 185/200 [1:14:46<05:40, 22.70s/it] 93%|█████████▎| 186/200 [1:15:09<05:16, 22.64s/it] 94%|█████████▎| 187/200 [1:15:31<04:53, 22.57s/it] 94%|█████████▍| 188/200 [1:15:53<04:30, 22.50s/it] 94%|█████████▍| 189/200 [1:16:16<04:07, 22.47s/it] 95%|█████████▌| 190/200 [1:16:38<03:44, 22.46s/it] 96%|█████████▌| 191/200 [1:17:01<03:22, 22.46s/it] 96%|█████████▌| 192/200 [1:17:23<02:59, 22.43s/it] 96%|█████████▋| 193/200 [1:17:45<02:37, 22.44s/it] 97%|█████████▋| 194/200 [1:18:08<02:14, 22.44s/it] 98%|█████████▊| 195/200 [1:18:30<01:52, 22.41s/it] 98%|█████████▊| 196/200 [1:19:02<01:40, 25.07s/it] 98%|█████████▊| 197/200 [1:19:22<01:11, 23.76s/it] 99%|█████████▉| 198/200 [1:19:45<00:46, 23.38s/it]100%|█████████▉| 199/200 [1:20:07<00:23, 23.12s/it]100%|██████████| 200/200 [1:20:38<00:00, 25.37s/it]100%|██████████| 200/200 [1:20:38<00:00, 24.19s/it]
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.307, acc: 10.00, fv: 2.309, accv: 10.00, lr: 0.0010, time: 8.35
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:01<3:22:59, 61.20s/it]  1%|          | 2/200 [01:57<3:11:53, 58.15s/it]  2%|▏         | 3/200 [02:53<3:07:39, 57.15s/it]  2%|▏         | 4/200 [03:49<3:05:11, 56.69s/it]  2%|▎         | 5/200 [04:48<3:07:49, 57.79s/it]  3%|▎         | 6/200 [05:10<2:26:47, 45.40s/it]  4%|▎         | 7/200 [05:40<2:09:49, 40.36s/it]  4%|▍         | 8/200 [06:02<1:50:19, 34.47s/it]  4%|▍         | 9/200 [06:32<1:45:10, 33.04s/it]  5%|▌         | 10/200 [06:53<1:33:36, 29.56s/it]  6%|▌         | 11/200 [07:23<1:33:30, 29.68s/it]  6%|▌         | 12/200 [07:45<1:25:22, 27.25s/it]  6%|▋         | 13/200 [08:15<1:27:36, 28.11s/it]  7%|▋         | 14/200 [08:37<1:21:05, 26.16s/it]  8%|▊         | 15/200 [09:06<1:23:25, 27.06s/it]  8%|▊         | 16/200 [09:35<1:24:41, 27.61s/it]  8%|▊         | 17/200 [10:05<1:26:20, 28.31s/it]  9%|▉         | 18/200 [10:26<1:19:43, 26.29s/it] 10%|▉         | 19/200 [10:56<1:22:48, 27.45s/it] 10%|█         | 20/200 [11:18<1:16:59, 25.67s/it] 10%|█         | 21/200 [11:48<1:20:47, 27.08s/it] 11%|█         | 22/200 [12:10<1:15:18, 25.39s/it] 12%|█▏        | 23/200 [12:40<1:19:03, 26.80s/it] 12%|█▏        | 24/200 [13:01<1:13:53, 25.19s/it] 12%|█▎        | 25/200 [13:31<1:17:48, 26.67s/it] 13%|█▎        | 26/200 [13:53<1:12:49, 25.11s/it] 14%|█▎        | 27/200 [14:23<1:16:38, 26.58s/it] 14%|█▍        | 28/200 [14:44<1:11:42, 25.01s/it] 14%|█▍        | 29/200 [15:14<1:15:42, 26.57s/it] 15%|█▌        | 30/200 [15:36<1:10:48, 24.99s/it] 16%|█▌        | 31/200 [16:06<1:14:40, 26.51s/it] 16%|█▌        | 32/200 [16:27<1:09:55, 24.98s/it] 16%|█▋        | 33/200 [16:57<1:13:59, 26.58s/it] 17%|█▋        | 34/200 [17:19<1:09:10, 25.00s/it] 18%|█▊        | 35/200 [17:49<1:12:57, 26.53s/it] 18%|█▊        | 36/200 [18:11<1:09:04, 25.27s/it] 18%|█▊        | 37/200 [18:33<1:06:00, 24.30s/it] 19%|█▉        | 38/200 [18:56<1:04:05, 23.74s/it] 20%|█▉        | 39/200 [19:18<1:02:39, 23.35s/it] 20%|██        | 40/200 [19:41<1:01:33, 23.08s/it] 20%|██        | 41/200 [20:03<1:00:39, 22.89s/it] 21%|██        | 42/200 [20:26<1:00:00, 22.79s/it] 22%|██▏       | 43/200 [20:48<59:25, 22.71s/it]   22%|██▏       | 44/200 [21:10<58:48, 22.62s/it] 22%|██▎       | 45/200 [21:32<57:30, 22.26s/it] 23%|██▎       | 46/200 [22:03<1:03:36, 24.78s/it] 24%|██▎       | 47/200 [22:25<1:01:22, 24.07s/it] 24%|██▍       | 48/200 [22:47<59:27, 23.47s/it]   24%|██▍       | 49/200 [23:09<58:16, 23.15s/it] 25%|██▌       | 50/200 [23:32<57:23, 22.96s/it] 26%|██▌       | 51/200 [23:54<56:41, 22.83s/it] 26%|██▌       | 52/200 [24:17<56:05, 22.74s/it] 26%|██▋       | 53/200 [24:39<55:29, 22.65s/it] 27%|██▋       | 54/200 [25:02<54:58, 22.59s/it] 28%|██▊       | 55/200 [25:24<54:29, 22.55s/it] 28%|██▊       | 56/200 [25:47<54:05, 22.54s/it] 28%|██▊       | 57/200 [26:09<53:39, 22.51s/it] 29%|██▉       | 58/200 [26:32<53:21, 22.55s/it] 30%|██▉       | 59/200 [26:55<53:00, 22.55s/it] 30%|███       | 60/200 [27:16<51:52, 22.23s/it] 30%|███       | 61/200 [27:47<57:18, 24.74s/it] 31%|███       | 62/200 [28:09<55:21, 24.07s/it] 32%|███▏      | 63/200 [28:31<53:31, 23.44s/it] 32%|███▏      | 64/200 [28:54<52:30, 23.16s/it] 32%|███▎      | 65/200 [29:16<51:40, 22.97s/it] 33%|███▎      | 66/200 [29:39<50:58, 22.82s/it] 34%|███▎      | 67/200 [30:01<50:23, 22.73s/it] 34%|███▍      | 68/200 [30:24<49:49, 22.64s/it] 34%|███▍      | 69/200 [30:46<49:18, 22.58s/it] 35%|███▌      | 70/200 [31:08<48:52, 22.56s/it] 36%|███▌      | 71/200 [31:31<48:24, 22.52s/it] 36%|███▌      | 72/200 [31:53<47:59, 22.49s/it] 36%|███▋      | 73/200 [32:16<47:35, 22.48s/it] 37%|███▋      | 74/200 [32:38<47:06, 22.43s/it] 38%|███▊      | 75/200 [33:00<46:10, 22.17s/it] 38%|███▊      | 76/200 [33:30<51:00, 24.68s/it] 38%|███▊      | 77/200 [33:53<49:12, 24.01s/it] 39%|███▉      | 78/200 [34:15<47:37, 23.42s/it] 40%|███▉      | 79/200 [34:37<46:41, 23.16s/it] 40%|████      | 80/200 [35:00<45:56, 22.97s/it] 40%|████      | 81/200 [35:22<45:14, 22.81s/it] 41%|████      | 82/200 [35:45<44:41, 22.73s/it] 42%|████▏     | 83/200 [36:07<44:10, 22.65s/it] 42%|████▏     | 84/200 [36:30<43:40, 22.59s/it] 42%|████▎     | 85/200 [36:52<43:11, 22.54s/it] 43%|████▎     | 86/200 [37:14<42:44, 22.49s/it] 44%|████▎     | 87/200 [37:37<42:21, 22.49s/it] 44%|████▍     | 88/200 [37:59<41:59, 22.49s/it] 44%|████▍     | 89/200 [38:22<41:37, 22.50s/it] 45%|████▌     | 90/200 [38:43<40:35, 22.14s/it] 46%|████▌     | 91/200 [39:14<44:41, 24.60s/it] 46%|████▌     | 92/200 [39:36<43:04, 23.93s/it] 46%|████▋     | 93/200 [39:58<41:37, 23.34s/it] 47%|████▋     | 94/200 [40:20<40:48, 23.10s/it] 48%|████▊     | 95/200 [40:43<40:03, 22.89s/it] 48%|████▊     | 96/200 [41:05<39:30, 22.79s/it] 48%|████▊     | 97/200 [41:28<38:56, 22.68s/it] 49%|████▉     | 98/200 [41:50<38:25, 22.60s/it] 50%|████▉     | 99/200 [42:13<37:57, 22.54s/it] 50%|█████     | 100/200 [42:35<37:31, 22.51s/it] 50%|█████     | 101/200 [42:58<37:08, 22.51s/it] 51%|█████     | 102/200 [43:20<36:45, 22.51s/it] 52%|█████▏    | 103/200 [43:43<36:25, 22.53s/it] 52%|█████▏    | 104/200 [44:05<35:58, 22.49s/it] 52%|█████▎    | 105/200 [44:27<35:10, 22.21s/it] 53%|█████▎    | 106/200 [44:57<38:31, 24.59s/it] 54%|█████▎    | 107/200 [45:19<37:04, 23.92s/it] 54%|█████▍    | 108/200 [45:41<35:54, 23.42s/it] 55%|█████▍    | 109/200 [46:04<34:58, 23.06s/it] 55%|█████▌    | 110/200 [46:26<34:15, 22.83s/it] 56%|█████▌    | 111/200 [46:48<33:39, 22.69s/it] 56%|█████▌    | 112/200 [47:11<33:09, 22.60s/it] 56%|█████▋    | 113/200 [47:33<32:40, 22.54s/it] 57%|█████▋    | 114/200 [47:56<32:15, 22.50s/it] 57%|█████▊    | 115/200 [48:18<31:54, 22.52s/it] 58%|█████▊    | 116/200 [48:40<31:28, 22.48s/it] 58%|█████▊    | 117/200 [49:03<31:03, 22.46s/it] 59%|█████▉    | 118/200 [49:25<30:40, 22.44s/it] 60%|█████▉    | 119/200 [49:48<30:17, 22.44s/it] 60%|██████    | 120/200 [50:10<29:46, 22.33s/it] 60%|██████    | 121/200 [50:40<32:19, 24.55s/it] 61%|██████    | 122/200 [51:02<31:02, 23.88s/it] 62%|██████▏   | 123/200 [51:24<30:05, 23.44s/it] 62%|██████▏   | 124/200 [51:46<29:07, 22.99s/it] 62%|██████▎   | 125/200 [52:09<28:33, 22.85s/it] 63%|██████▎   | 126/200 [52:31<28:02, 22.74s/it] 64%|██████▎   | 127/200 [52:54<27:31, 22.63s/it] 64%|██████▍   | 128/200 [53:16<27:04, 22.57s/it] 64%|██████▍   | 129/200 [53:38<26:38, 22.51s/it] 65%|██████▌   | 130/200 [54:01<26:13, 22.47s/it] 66%|██████▌   | 131/200 [54:23<25:51, 22.48s/it] 66%|██████▌   | 132/200 [54:46<25:27, 22.46s/it] 66%|██████▋   | 133/200 [55:08<25:04, 22.46s/it] 67%|██████▋   | 134/200 [55:31<24:42, 22.46s/it] 68%|██████▊   | 135/200 [55:53<24:18, 22.44s/it] 68%|██████▊   | 136/200 [56:22<26:04, 24.44s/it] 68%|██████▊   | 137/200 [56:44<25:00, 23.82s/it] 69%|██████▉   | 138/200 [57:07<24:12, 23.43s/it] 70%|██████▉   | 139/200 [57:29<23:23, 23.01s/it] 70%|███████   | 140/200 [57:51<22:50, 22.84s/it] 70%|███████   | 141/200 [58:14<22:19, 22.71s/it] 71%|███████   | 142/200 [58:36<21:52, 22.63s/it] 72%|███████▏  | 143/200 [58:59<21:25, 22.55s/it] 72%|███████▏  | 144/200 [59:21<21:02, 22.54s/it] 72%|███████▎  | 145/200 [59:44<20:39, 22.53s/it] 73%|███████▎  | 146/200 [1:00:06<20:15, 22.51s/it] 74%|███████▎  | 147/200 [1:00:29<19:52, 22.50s/it] 74%|███████▍  | 148/200 [1:00:51<19:30, 22.51s/it] 74%|███████▍  | 149/200 [1:01:14<19:06, 22.48s/it] 75%|███████▌  | 150/200 [1:01:36<18:45, 22.51s/it] 76%|███████▌  | 151/200 [1:02:05<20:01, 24.52s/it] 76%|███████▌  | 152/200 [1:02:28<19:07, 23.90s/it] 76%|███████▋  | 153/200 [1:02:50<18:22, 23.47s/it] 77%|███████▋  | 154/200 [1:03:12<17:38, 23.00s/it] 78%|███████▊  | 155/200 [1:03:35<17:06, 22.82s/it] 78%|███████▊  | 156/200 [1:03:57<16:38, 22.69s/it] 78%|███████▊  | 157/200 [1:04:19<16:11, 22.60s/it] 79%|███████▉  | 158/200 [1:04:42<15:48, 22.57s/it] 80%|███████▉  | 159/200 [1:05:04<15:23, 22.52s/it] 80%|████████  | 160/200 [1:05:27<15:00, 22.52s/it] 80%|████████  | 161/200 [1:05:49<14:38, 22.52s/it] 81%|████████  | 162/200 [1:06:12<14:15, 22.50s/it] 82%|████████▏ | 163/200 [1:06:34<13:53, 22.52s/it] 82%|████████▏ | 164/200 [1:06:57<13:29, 22.49s/it] 82%|████████▎ | 165/200 [1:07:19<13:06, 22.49s/it] 83%|████████▎ | 166/200 [1:07:48<13:52, 24.49s/it] 84%|████████▎ | 167/200 [1:08:11<13:07, 23.86s/it] 84%|████████▍ | 168/200 [1:08:33<12:29, 23.43s/it] 84%|████████▍ | 169/200 [1:08:55<11:52, 22.99s/it] 85%|████████▌ | 170/200 [1:09:18<11:24, 22.82s/it] 86%|████████▌ | 171/200 [1:09:40<10:58, 22.72s/it] 86%|████████▌ | 172/200 [1:10:03<10:33, 22.63s/it] 86%|████████▋ | 173/200 [1:10:25<10:09, 22.58s/it] 87%|████████▋ | 174/200 [1:10:47<09:45, 22.53s/it] 88%|████████▊ | 175/200 [1:11:10<09:22, 22.51s/it] 88%|████████▊ | 176/200 [1:11:32<08:59, 22.50s/it] 88%|████████▊ | 177/200 [1:11:55<08:36, 22.46s/it] 89%|████████▉ | 178/200 [1:12:17<08:14, 22.46s/it] 90%|████████▉ | 179/200 [1:12:40<07:51, 22.47s/it] 90%|█████████ | 180/200 [1:13:02<07:29, 22.47s/it] 90%|█████████ | 181/200 [1:13:32<07:47, 24.59s/it] 91%|█████████ | 182/200 [1:13:54<07:11, 23.95s/it] 92%|█████████▏| 183/200 [1:14:17<06:39, 23.51s/it] 92%|█████████▏| 184/200 [1:14:39<06:09, 23.12s/it] 92%|█████████▎| 185/200 [1:15:01<05:42, 22.84s/it] 93%|█████████▎| 186/200 [1:15:23<05:17, 22.69s/it] 94%|█████████▎| 187/200 [1:15:46<04:53, 22.60s/it] 94%|█████████▍| 188/200 [1:16:08<04:30, 22.57s/it] 94%|█████████▍| 189/200 [1:16:31<04:07, 22.51s/it] 95%|█████████▌| 190/200 [1:16:53<03:44, 22.49s/it] 96%|█████████▌| 191/200 [1:17:15<03:22, 22.47s/it] 96%|█████████▌| 192/200 [1:17:38<02:59, 22.44s/it] 96%|█████████▋| 193/200 [1:18:00<02:36, 22.42s/it] 97%|█████████▋| 194/200 [1:18:23<02:14, 22.42s/it] 98%|█████████▊| 195/200 [1:18:45<01:52, 22.48s/it] 98%|█████████▊| 196/200 [1:19:15<01:38, 24.57s/it] 98%|█████████▊| 197/200 [1:19:37<01:11, 23.93s/it] 99%|█████████▉| 198/200 [1:20:00<00:46, 23.49s/it]100%|█████████▉| 199/200 [1:20:22<00:23, 23.16s/it]100%|██████████| 200/200 [1:20:48<00:00, 24.05s/it][000001] f: 2.305, acc: 10.00, fv: 2.302, accv: 10.04, lr: 0.0010, time: 7.31
[000063] f: 1.688, acc: 35.91, fv: 10.388, accv: 11.44, lr: 0.0010, time: 7.19
[000125] f: 1.485, acc: 46.24, fv: 2.898, accv: 20.79, lr: 0.0010, time: 7.49
[000187] f: 1.311, acc: 52.51, fv: 2.592, accv: 24.98, lr: 0.0010, time: 7.61
[000249] f: 1.275, acc: 54.25, fv: 3.316, accv: 24.91, lr: 0.0010, time: 7.53
[000250] f: 1.220, acc: 55.87, fv: 3.128, accv: 25.84, lr: 0.0010, time: 7.71
[000251] f: 1.235, acc: 54.56, fv: 2.867, accv: 28.18, lr: 0.0010, time: 7.78
[000313] f: 1.585, acc: 46.84, fv: 6.473, accv: 13.20, lr: 0.0010, time: 7.47
[000375] f: 1.428, acc: 48.50, fv: 5.357, accv: 15.95, lr: 0.0010, time: 7.70
[000437] f: 1.238, acc: 55.85, fv: 2.970, accv: 31.93, lr: 0.0010, time: 7.77
[000499] f: 1.008, acc: 64.38, fv: 3.665, accv: 20.22, lr: 0.0010, time: 7.63
[000501] f: 1.088, acc: 59.63, fv: 3.572, accv: 20.78, lr: 0.0010, time: 7.92
[000563] f: 0.941, acc: 67.56, fv: 2.034, accv: 35.49, lr: 0.0010, time: 7.64
[000625] f: 0.940, acc: 66.70, fv: 2.555, accv: 37.68, lr: 0.0010, time: 7.71
[000687] f: 1.080, acc: 61.01, fv: 2.897, accv: 29.62, lr: 0.0010, time: 7.80
[000749] f: 0.932, acc: 66.88, fv: 4.243, accv: 24.41, lr: 0.0010, time: 7.68
[000751] f: 0.885, acc: 68.26, fv: 3.949, accv: 26.20, lr: 0.0010, time: 7.81
[000813] f: 0.937, acc: 66.89, fv: 3.033, accv: 31.07, lr: 0.0010, time: 7.71
[000875] f: 1.121, acc: 62.31, fv: 2.373, accv: 34.30, lr: 0.0010, time: 7.84
[000937] f: 0.777, acc: 72.78, fv: 2.287, accv: 34.16, lr: 0.0010, time: 7.87
[000999] f: 1.082, acc: 61.82, fv: 3.097, accv: 28.98, lr: 0.0010, time: 7.67
[001001] f: 0.912, acc: 68.35, fv: 3.832, accv: 23.12, lr: 0.0010, time: 7.81
[001063] f: 0.774, acc: 73.40, fv: 2.189, accv: 37.81, lr: 0.0010, time: 8.06
[001125] f: 1.102, acc: 62.38, fv: 3.633, accv: 28.80, lr: 0.0010, time: 8.25
[001187] f: 0.854, acc: 70.23, fv: 3.330, accv: 26.48, lr: 0.0010, time: 8.38
[001249] f: 0.771, acc: 73.01, fv: 4.109, accv: 23.82, lr: 0.0010, time: 7.92
[001750] f: 0.848, acc: 71.09, fv: 2.913, accv: 33.32, lr: 0.0010, time: 8.17
[002250] f: 0.652, acc: 77.43, fv: 3.933, accv: 28.56, lr: 0.0010, time: 8.14
[002750] f: 0.642, acc: 77.35, fv: 3.840, accv: 36.37, lr: 0.0010, time: 8.20
[003250] f: 0.360, acc: 88.11, fv: 3.874, accv: 31.86, lr: 0.0010, time: 8.36
[003750] f: 0.461, acc: 82.81, fv: 3.895, accv: 34.03, lr: 0.0010, time: 8.55
[004000] f: 0.300, acc: 89.20, fv: 3.649, accv: 34.65, lr: 0.0010, time: 8.46
[004250] f: 0.423, acc: 84.76, fv: 5.803, accv: 26.26, lr: 0.0010, time: 8.29
[004750] f: 0.409, acc: 85.34, fv: 4.406, accv: 30.37, lr: 0.0010, time: 8.44
[005250] f: 0.234, acc: 91.90, fv: 5.622, accv: 29.36, lr: 0.0010, time: 8.69
[005750] f: 0.173, acc: 93.94, fv: 4.006, accv: 36.33, lr: 0.0010, time: 8.37
[006250] f: 0.115, acc: 96.32, fv: 5.321, accv: 31.40, lr: 0.0010, time: 8.52
[006750] f: 0.244, acc: 91.12, fv: 5.730, accv: 30.20, lr: 0.0010, time: 8.38
[007250] f: 0.105, acc: 96.57, fv: 4.683, accv: 37.59, lr: 0.0009, time: 8.41
[007750] f: 0.182, acc: 93.35, fv: 6.728, accv: 29.32, lr: 0.0009, time: 8.43
[008250] f: 0.179, acc: 93.02, fv: 6.993, accv: 30.30, lr: 0.0009, time: 8.77
[008750] f: 0.703, acc: 79.45, fv: 10.314, accv: 24.56, lr: 0.0009, time: 8.55
[011500] f: 0.002, acc: 100.00, fv: 7.231, accv: 30.00, lr: 0.0009, time: 8.68
[015250] f: 0.001, acc: 100.00, fv: 6.235, accv: 32.92, lr: 0.0008, time: 8.58
[019000] f: 0.001, acc: 100.00, fv: 6.093, accv: 33.93, lr: 0.0007, time: 8.61
[022750] f: 0.001, acc: 100.00, fv: 6.190, accv: 34.48, lr: 0.0006, time: 8.47
[026500] f: 0.000, acc: 100.00, fv: 7.323, accv: 31.51, lr: 0.0005, time: 8.42
[030250] f: 0.000, acc: 100.00, fv: 7.317, accv: 32.81, lr: 0.0003, time: 8.47
[034000] f: 0.000, acc: 100.00, fv: 8.016, accv: 31.05, lr: 0.0002, time: 8.49
[037750] f: 0.000, acc: 100.00, fv: 8.156, accv: 31.60, lr: 0.0001, time: 8.63
[041500] f: 0.000, acc: 100.00, fv: 8.721, accv: 31.14, lr: 0.0001, time: 8.40
[045250] f: 0.000, acc: 100.00, fv: 8.554, accv: 31.71, lr: 0.0000, time: 8.66
[049000] f: 0.000, acc: 100.00, fv: 8.907, accv: 31.01, lr: 0.0000, time: 8.12
[050000] f: 0.000, acc: 100.00, fv: 8.837, accv: 31.10, lr: 0.0000, time: 7.24
100%|██████████| 200/200 [1:20:48<00:00, 24.24s/it]
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
[000000] f: 2.304, acc: 10.00, fv: 2.306, accv: 10.00, lr: 0.0010, time: 8.25
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [01:03<3:30:20, 63.42s/it]  1%|          | 2/200 [01:59<3:15:25, 59.22s/it]  2%|▏         | 3/200 [02:55<3:09:04, 57.58s/it]  2%|▏         | 4/200 [03:50<3:05:10, 56.69s/it]  2%|▎         | 5/200 [04:52<3:10:31, 58.62s/it]  3%|▎         | 6/200 [05:14<2:29:11, 46.14s/it]  4%|▎         | 7/200 [05:44<2:11:21, 40.84s/it]  4%|▍         | 8/200 [06:06<1:51:30, 34.84s/it]  4%|▍         | 9/200 [06:36<1:46:03, 33.31s/it]  5%|▌         | 10/200 [06:58<1:34:19, 29.79s/it]  6%|▌         | 11/200 [07:28<1:33:56, 29.82s/it]  6%|▌         | 12/200 [07:50<1:25:55, 27.42s/it]  6%|▋         | 13/200 [08:20<1:27:45, 28.16s/it]  7%|▋         | 14/200 [08:41<1:20:34, 25.99s/it]  8%|▊         | 15/200 [09:10<1:23:06, 26.95s/it]  8%|▊         | 16/200 [09:40<1:25:46, 27.97s/it]  8%|▊         | 17/200 [10:10<1:26:59, 28.52s/it]  9%|▉         | 18/200 [10:32<1:20:34, 26.56s/it] 10%|▉         | 19/200 [11:02<1:23:07, 27.56s/it] 10%|█         | 20/200 [11:24<1:17:35, 25.87s/it] 10%|█         | 21/200 [11:54<1:20:43, 27.06s/it] 11%|█         | 22/200 [12:16<1:15:48, 25.55s/it] 12%|█▏        | 23/200 [12:46<1:19:29, 26.95s/it] 12%|█▏        | 24/200 [13:08<1:14:32, 25.41s/it] 12%|█▎        | 25/200 [13:38<1:18:15, 26.83s/it] 13%|█▎        | 26/200 [14:00<1:13:34, 25.37s/it] 14%|█▎        | 27/200 [14:30<1:17:08, 26.75s/it] 14%|█▍        | 28/200 [14:52<1:12:32, 25.31s/it] 14%|█▍        | 29/200 [15:22<1:16:16, 26.76s/it] 15%|█▌        | 30/200 [15:44<1:11:42, 25.31s/it] 16%|█▌        | 31/200 [16:14<1:15:24, 26.77s/it] 16%|█▌        | 32/200 [16:36<1:10:47, 25.28s/it] 16%|█▋        | 33/200 [17:06<1:14:24, 26.73s/it] 17%|█▋        | 34/200 [17:28<1:09:56, 25.28s/it] 18%|█▊        | 35/200 [17:59<1:14:23, 27.05s/it] 18%|█▊        | 36/200 [18:22<1:10:22, 25.75s/it] 18%|█▊        | 37/200 [18:44<1:07:30, 24.85s/it] 19%|█▉        | 38/200 [19:07<1:05:27, 24.24s/it] 20%|█▉        | 39/200 [19:30<1:03:50, 23.79s/it] 20%|██        | 40/200 [19:53<1:02:36, 23.48s/it] 20%|██        | 41/200 [20:15<1:01:39, 23.27s/it] 21%|██        | 42/200 [20:38<1:00:52, 23.12s/it] 22%|██▏       | 43/200 [21:00<59:38, 22.79s/it]   22%|██▏       | 44/200 [21:23<58:58, 22.68s/it] 22%|██▎       | 45/200 [21:44<57:55, 22.42s/it] 23%|██▎       | 46/200 [22:16<1:04:18, 25.05s/it] 24%|██▎       | 47/200 [22:38<1:02:04, 24.35s/it] 24%|██▍       | 48/200 [23:01<1:00:31, 23.89s/it] 24%|██▍       | 49/200 [23:24<59:11, 23.52s/it]   25%|██▌       | 50/200 [23:47<58:15, 23.30s/it] 26%|██▌       | 51/200 [24:09<57:26, 23.13s/it] 26%|██▌       | 52/200 [24:32<56:48, 23.03s/it] 26%|██▋       | 53/200 [24:55<56:11, 22.94s/it] 27%|██▋       | 54/200 [25:18<55:41, 22.89s/it] 28%|██▊       | 55/200 [25:40<55:13, 22.85s/it] 28%|██▊       | 56/200 [26:03<54:45, 22.81s/it] 28%|██▊       | 57/200 [26:26<54:18, 22.79s/it] 29%|██▉       | 58/200 [26:48<53:20, 22.54s/it] 30%|██▉       | 59/200 [27:10<52:44, 22.45s/it] 30%|███       | 60/200 [27:32<52:09, 22.35s/it] 30%|███       | 61/200 [28:03<57:54, 24.99s/it] 31%|███       | 62/200 [28:26<55:51, 24.29s/it] 32%|███▏      | 63/200 [28:49<54:23, 23.82s/it] 32%|███▏      | 64/200 [29:12<53:20, 23.53s/it] 32%|███▎      | 65/200 [29:34<52:23, 23.28s/it] 33%|███▎      | 66/200 [29:57<51:39, 23.13s/it] 34%|███▎      | 67/200 [30:20<51:04, 23.04s/it] 34%|███▍      | 68/200 [30:43<50:29, 22.95s/it] 34%|███▍      | 69/200 [31:05<49:57, 22.88s/it] 35%|███▌      | 70/200 [31:28<49:29, 22.84s/it] 36%|███▌      | 71/200 [31:51<49:01, 22.80s/it] 36%|███▌      | 72/200 [32:14<48:38, 22.80s/it] 36%|███▋      | 73/200 [32:36<47:43, 22.55s/it] 37%|███▋      | 74/200 [32:58<47:02, 22.40s/it] 38%|███▊      | 75/200 [33:20<46:36, 22.37s/it] 38%|███▊      | 76/200 [33:51<51:44, 25.04s/it] 38%|███▊      | 77/200 [34:14<49:50, 24.31s/it] 39%|███▉      | 78/200 [34:36<48:26, 23.83s/it] 40%|███▉      | 79/200 [34:59<47:22, 23.49s/it] 40%|████      | 80/200 [35:22<46:33, 23.28s/it] 40%|████      | 81/200 [35:45<45:51, 23.12s/it] 41%|████      | 82/200 [36:07<45:11, 22.98s/it] 42%|████▏     | 83/200 [36:30<44:41, 22.92s/it] 42%|████▏     | 84/200 [36:53<44:13, 22.87s/it] 42%|████▎     | 85/200 [37:16<43:44, 22.82s/it] 43%|████▎     | 86/200 [37:38<43:18, 22.79s/it] 44%|████▎     | 87/200 [38:01<42:51, 22.76s/it] 44%|████▍     | 88/200 [38:23<42:01, 22.52s/it] 44%|████▍     | 89/200 [38:45<41:28, 22.42s/it] 45%|████▌     | 90/200 [39:08<41:02, 22.39s/it] 46%|████▌     | 91/200 [39:39<45:30, 25.05s/it] 46%|████▌     | 92/200 [40:01<43:48, 24.34s/it] 46%|████▋     | 93/200 [40:24<42:33, 23.86s/it] 47%|████▋     | 94/200 [40:47<41:35, 23.54s/it] 48%|████▊     | 95/200 [41:10<40:47, 23.31s/it] 48%|████▊     | 96/200 [41:32<40:05, 23.13s/it] 48%|████▊     | 97/200 [41:55<39:31, 23.03s/it] 49%|████▉     | 98/200 [42:18<38:59, 22.94s/it] 50%|████▉     | 99/200 [42:41<38:31, 22.88s/it] 50%|█████     | 100/200 [43:03<38:02, 22.83s/it] 50%|█████     | 101/200 [43:26<37:37, 22.80s/it] 51%|█████     | 102/200 [43:49<37:12, 22.78s/it] 52%|█████▏    | 103/200 [44:11<36:19, 22.47s/it] 52%|█████▏    | 104/200 [44:33<35:44, 22.34s/it] 52%|█████▎    | 105/200 [44:55<35:23, 22.35s/it] 53%|█████▎    | 106/200 [45:26<39:17, 25.08s/it] 54%|█████▎    | 107/200 [45:49<37:45, 24.36s/it] 54%|█████▍    | 108/200 [46:12<36:35, 23.86s/it] 55%|█████▍    | 109/200 [46:35<35:40, 23.52s/it] 55%|█████▌    | 110/200 [46:57<34:57, 23.31s/it] 56%|█████▌    | 111/200 [47:20<34:20, 23.15s/it] 56%|█████▌    | 112/200 [47:43<33:45, 23.02s/it] 56%|█████▋    | 113/200 [48:06<33:14, 22.93s/it] 57%|█████▋    | 114/200 [48:28<32:47, 22.88s/it] 57%|█████▊    | 115/200 [48:51<32:20, 22.83s/it] 58%|█████▊    | 116/200 [49:14<31:55, 22.80s/it] 58%|█████▊    | 117/200 [49:37<31:32, 22.80s/it] 59%|█████▉    | 118/200 [49:58<30:36, 22.39s/it] 60%|█████▉    | 119/200 [50:20<30:03, 22.27s/it] 60%|██████    | 120/200 [50:43<29:51, 22.39s/it] 60%|██████    | 121/200 [51:14<33:02, 25.10s/it] 61%|██████    | 122/200 [51:37<31:40, 24.36s/it] 62%|██████▏   | 123/200 [52:00<30:38, 23.87s/it] 62%|██████▏   | 124/200 [52:22<29:48, 23.53s/it] 62%|██████▎   | 125/200 [52:45<29:06, 23.29s/it] 63%|██████▎   | 126/200 [53:08<28:30, 23.11s/it] 64%|██████▎   | 127/200 [53:30<28:00, 23.01s/it] 64%|██████▍   | 128/200 [53:53<27:31, 22.94s/it] 64%|██████▍   | 129/200 [54:16<27:03, 22.87s/it] 65%|██████▌   | 130/200 [54:39<26:37, 22.82s/it] 66%|██████▌   | 131/200 [55:01<26:11, 22.78s/it] 66%|██████▌   | 132/200 [55:24<25:48, 22.76s/it] 66%|██████▋   | 133/200 [55:45<24:52, 22.27s/it] 67%|██████▋   | 134/200 [56:07<24:28, 22.24s/it] 68%|██████▊   | 135/200 [56:30<24:14, 22.38s/it] 68%|██████▊   | 136/200 [57:01<26:41, 25.03s/it] 68%|██████▊   | 137/200 [57:24<25:31, 24.32s/it] 69%|██████▉   | 138/200 [57:47<24:37, 23.83s/it] 70%|██████▉   | 139/200 [58:09<23:53, 23.50s/it] 70%|███████   | 140/200 [58:32<23:15, 23.26s/it] 70%|███████   | 141/200 [58:55<22:42, 23.08s/it] 71%|███████   | 142/200 [59:17<22:12, 22.98s/it] 72%|███████▏  | 143/200 [59:40<21:45, 22.90s/it] 72%|███████▏  | 144/200 [1:00:03<21:19, 22.84s/it] 72%|███████▎  | 145/200 [1:00:26<20:55, 22.84s/it] 73%|███████▎  | 146/200 [1:00:48<20:30, 22.79s/it] 74%|███████▎  | 147/200 [1:01:11<20:07, 22.78s/it] 74%|███████▍  | 148/200 [1:01:32<19:15, 22.23s/it] 74%|███████▍  | 149/200 [1:01:54<18:54, 22.24s/it] 75%|███████▌  | 150/200 [1:02:17<18:38, 22.36s/it] 76%|███████▌  | 151/200 [1:02:48<20:26, 25.03s/it] 76%|███████▌  | 152/200 [1:03:11<19:27, 24.32s/it] 76%|███████▋  | 153/200 [1:03:34<18:41, 23.86s/it] 77%|███████▋  | 154/200 [1:03:56<18:01, 23.52s/it] 78%|███████▊  | 155/200 [1:04:19<17:27, 23.28s/it] 78%|███████▊  | 156/200 [1:04:42<16:56, 23.11s/it] 78%|███████▊  | 157/200 [1:05:05<16:28, 22.99s/it] 79%|███████▉  | 158/200 [1:05:27<16:03, 22.93s/it] 80%|███████▉  | 159/200 [1:05:50<15:37, 22.87s/it] 80%|████████  | 160/200 [1:06:13<15:12, 22.82s/it] 80%|████████  | 161/200 [1:06:36<14:48, 22.79s/it] 81%|████████  | 162/200 [1:06:58<14:25, 22.78s/it] 82%|████████▏ | 163/200 [1:07:19<13:45, 22.30s/it] 82%|████████▏ | 164/200 [1:07:42<13:22, 22.28s/it] 82%|████████▎ | 165/200 [1:08:04<13:04, 22.42s/it] 83%|████████▎ | 166/200 [1:08:36<14:13, 25.11s/it] 84%|████████▎ | 167/200 [1:08:59<13:24, 24.39s/it] 84%|████████▍ | 168/200 [1:09:21<12:44, 23.88s/it] 84%|████████▍ | 169/200 [1:09:44<12:09, 23.54s/it] 85%|████████▌ | 170/200 [1:10:07<11:38, 23.29s/it] 86%|████████▌ | 171/200 [1:10:29<11:10, 23.13s/it] 86%|████████▌ | 172/200 [1:10:52<10:43, 22.98s/it] 86%|████████▋ | 173/200 [1:11:15<10:18, 22.90s/it] 87%|████████▋ | 174/200 [1:11:38<09:54, 22.86s/it] 88%|████████▊ | 175/200 [1:12:00<09:30, 22.81s/it] 88%|████████▊ | 176/200 [1:12:23<09:06, 22.77s/it] 88%|████████▊ | 177/200 [1:12:46<08:43, 22.76s/it] 89%|████████▉ | 178/200 [1:13:07<08:08, 22.22s/it] 90%|████████▉ | 179/200 [1:13:29<07:46, 22.24s/it] 90%|█████████ | 180/200 [1:13:52<07:27, 22.40s/it] 90%|█████████ | 181/200 [1:14:23<07:56, 25.10s/it] 91%|█████████ | 182/200 [1:14:46<07:18, 24.35s/it] 92%|█████████▏| 183/200 [1:15:08<06:45, 23.86s/it] 92%|█████████▏| 184/200 [1:15:31<06:16, 23.53s/it] 92%|█████████▎| 185/200 [1:15:54<05:49, 23.30s/it] 93%|█████████▎| 186/200 [1:16:17<05:23, 23.12s/it] 94%|█████████▎| 187/200 [1:16:39<04:59, 23.01s/it] 94%|█████████▍| 188/200 [1:17:02<04:35, 22.92s/it] 94%|█████████▍| 189/200 [1:17:25<04:11, 22.87s/it] 95%|█████████▌| 190/200 [1:17:48<03:48, 22.84s/it] 96%|█████████▌| 191/200 [1:18:10<03:25, 22.80s/it] 96%|█████████▌| 192/200 [1:18:33<03:01, 22.73s/it] 96%|█████████▋| 193/200 [1:18:54<02:35, 22.16s/it] 97%|█████████▋| 194/200 [1:19:16<02:13, 22.25s/it] 98%|█████████▊| 195/200 [1:19:39<01:51, 22.39s/it] 98%|█████████▊| 196/200 [1:20:10<01:40, 25.10s/it] 98%|█████████▊| 197/200 [1:20:25<01:06, 22.13s/it] 99%|█████████▉| 198/200 [1:20:32<00:34, 17.41s/it][000001] f: 2.302, acc: 10.00, fv: 2.302, accv: 10.26, lr: 0.0010, time: 7.47
[000063] f: 1.785, acc: 35.01, fv: 2.516, accv: 18.02, lr: 0.0010, time: 7.72
[000125] f: 1.627, acc: 40.98, fv: 3.215, accv: 18.83, lr: 0.0010, time: 7.61
[000187] f: 1.462, acc: 46.55, fv: 4.140, accv: 14.52, lr: 0.0010, time: 7.55
[000249] f: 1.375, acc: 49.65, fv: 2.880, accv: 21.68, lr: 0.0010, time: 7.67
[000250] f: 1.297, acc: 52.20, fv: 2.665, accv: 24.35, lr: 0.0010, time: 7.53
[000251] f: 1.285, acc: 53.89, fv: 2.515, accv: 27.64, lr: 0.0010, time: 7.61
[000313] f: 1.167, acc: 58.03, fv: 3.005, accv: 23.32, lr: 0.0010, time: 7.61
[000375] f: 1.662, acc: 47.65, fv: 4.035, accv: 25.49, lr: 0.0010, time: 7.59
[000437] f: 1.491, acc: 50.40, fv: 3.424, accv: 24.76, lr: 0.0010, time: 7.72
[000499] f: 1.158, acc: 59.32, fv: 2.745, accv: 25.94, lr: 0.0010, time: 7.66
[000501] f: 1.095, acc: 61.42, fv: 2.553, accv: 28.02, lr: 0.0010, time: 7.80
[000563] f: 1.113, acc: 59.07, fv: 3.139, accv: 23.46, lr: 0.0010, time: 7.66
[000625] f: 1.078, acc: 61.57, fv: 5.578, accv: 16.36, lr: 0.0010, time: 7.66
[000687] f: 1.565, acc: 46.49, fv: 4.436, accv: 15.98, lr: 0.0010, time: 7.67
[000749] f: 1.028, acc: 63.49, fv: 3.098, accv: 24.97, lr: 0.0010, time: 7.56
[000751] f: 1.078, acc: 62.16, fv: 2.708, accv: 28.96, lr: 0.0010, time: 7.82
[000813] f: 0.853, acc: 70.04, fv: 3.435, accv: 28.53, lr: 0.0010, time: 7.79
[000875] f: 0.871, acc: 69.09, fv: 3.221, accv: 24.09, lr: 0.0010, time: 7.73
[000937] f: 1.015, acc: 62.78, fv: 3.099, accv: 29.75, lr: 0.0010, time: 7.79
[000999] f: 0.891, acc: 67.74, fv: 3.097, accv: 32.29, lr: 0.0010, time: 8.06
[001001] f: 0.818, acc: 70.34, fv: 3.365, accv: 31.21, lr: 0.0010, time: 8.19
[001063] f: 0.986, acc: 64.64, fv: 3.713, accv: 28.88, lr: 0.0010, time: 8.25
[001125] f: 1.002, acc: 63.25, fv: 3.433, accv: 29.75, lr: 0.0010, time: 8.13
[001187] f: 0.769, acc: 72.67, fv: 3.003, accv: 31.89, lr: 0.0010, time: 8.48
[001249] f: 0.870, acc: 67.36, fv: 3.397, accv: 27.39, lr: 0.0010, time: 8.54
[001750] f: 0.876, acc: 69.37, fv: 3.369, accv: 32.80, lr: 0.0010, time: 8.59
[002250] f: 1.003, acc: 68.93, fv: 4.208, accv: 26.39, lr: 0.0010, time: 8.67
[002750] f: 0.661, acc: 75.27, fv: 3.941, accv: 28.44, lr: 0.0010, time: 8.61
[003250] f: 0.586, acc: 78.58, fv: 3.095, accv: 37.27, lr: 0.0010, time: 8.66
[003750] f: 0.461, acc: 82.86, fv: 3.110, accv: 37.92, lr: 0.0010, time: 8.31
[004000] f: 0.331, acc: 88.52, fv: 4.164, accv: 33.26, lr: 0.0010, time: 8.23
[004250] f: 0.274, acc: 90.99, fv: 3.711, accv: 32.55, lr: 0.0010, time: 8.63
[004750] f: 0.298, acc: 89.47, fv: 5.543, accv: 28.49, lr: 0.0010, time: 8.64
[005250] f: 0.383, acc: 86.39, fv: 3.827, accv: 36.50, lr: 0.0010, time: 8.68
[005750] f: 0.243, acc: 91.35, fv: 5.350, accv: 31.45, lr: 0.0010, time: 8.81
[006250] f: 0.154, acc: 94.81, fv: 6.290, accv: 28.05, lr: 0.0010, time: 8.75
[006750] f: 0.162, acc: 94.42, fv: 4.199, accv: 36.15, lr: 0.0010, time: 8.58
[007250] f: 0.168, acc: 93.98, fv: 5.956, accv: 31.10, lr: 0.0009, time: 8.65
[007750] f: 0.210, acc: 92.61, fv: 6.238, accv: 34.34, lr: 0.0009, time: 8.74
[008250] f: 0.241, acc: 91.54, fv: 5.823, accv: 33.38, lr: 0.0009, time: 8.70
[008750] f: 0.227, acc: 91.94, fv: 6.384, accv: 35.37, lr: 0.0009, time: 8.66
[011500] f: 0.002, acc: 100.00, fv: 6.303, accv: 34.09, lr: 0.0009, time: 8.48
[015250] f: 0.001, acc: 100.00, fv: 6.442, accv: 34.72, lr: 0.0008, time: 8.44
[019000] f: 0.001, acc: 100.00, fv: 7.328, accv: 33.34, lr: 0.0007, time: 8.54
[022750] f: 0.001, acc: 100.00, fv: 7.620, accv: 32.89, lr: 0.0006, time: 8.54
[026500] f: 0.001, acc: 100.00, fv: 8.811, accv: 30.02, lr: 0.0005, time: 8.73
[030250] f: 0.000, acc: 100.00, fv: 7.658, accv: 33.59, lr: 0.0003, time: 8.70
[034000] f: 0.000, acc: 100.00, fv: 8.840, accv: 31.04, lr: 0.0002, time: 8.47
[037750] f: 0.000, acc: 100.00, fv: 8.647, accv: 31.30, lr: 0.0001, time: 8.50
[041500] f: 0.000, acc: 100.00, fv: 9.051, accv: 31.60, lr: 0.0001, time: 8.67
[045250] f: 0.000, acc: 100.00, fv: 9.340, accv: 31.05, lr: 0.0000, time: 8.67
[049000] f: 0.000, acc: 100.00, fv: 9.444, accv: 30.90, lr: 0.0000, time: 8.73
[050000] f: 0.000, acc: 100.00, fv: 9.581, accv: 30.47, lr: 0.0000, time: 6.74
100%|█████████▉| 199/200 [1:20:38<00:14, 14.09s/it]100%|██████████| 200/200 [1:20:51<00:00, 13.79s/it]100%|██████████| 200/200 [1:20:51<00:00, 24.26s/it]
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 10, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 8, 'bn': True}, model_config='./configs/model/wr-10-4-8.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-10-4-8","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  300226
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='wide_resnet', model_args={'depth': 16, 'widen_factor': 4, 'dropout_rate': 0.0, 'num_classes': 10, 'in_planes': 64, 'bn': True}, model_config='./configs/model/wr-16-4-64.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"wr-16-4-64","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Num parameters:  43890122
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='ViT', model_args={'image_size': 32, 'patch_size': 4, 'num_classes': 10, 'dim': 512, 'depth': 6, 'heads': 8, 'mlp_dim': 512, 'dropout_rate': 0.1, 'emb_dropout': 0.1, 'bn': True}, model_config='./configs/model/vit.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"vit","bn":true,"drop":0.1,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='convmixer', model_args={'dim': 256, 'depth': 8, 'kernel_size': 5, 'patch_size': 2, 'n_classes': 10, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/convmixer.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"convmixer","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='fcnn', model_args={'dims': [3072, 1024, 512, 256, 128, 10], 'bn': True, 'bias': True, 'dropout_rate': 0.0}, model_config='./configs/model/fc.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"fc","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.001, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.001-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.001,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.01-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.1-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/adam-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'weight_decay': 0.0}, optim_config='./configs/optim/sgd-200-0.01-0.0.yaml', resize=1, sched_args={'cycle_mult': 1, 'gamma': 0.1, 'warmup_steps': 10}, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgd","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.1, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.1-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.1,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='SGD', opt_args={'lr': 0.01, 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0}, optim_config='./configs/optim/sgdn-200-0.01-0.0.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"sgdn","bs":200,"lr":0.01,"wd":0.0}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=42, shuffle=False, sub_sample=0)
{"seed":42,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=43, shuffle=False, sub_sample=0)
{"seed":43,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=44, shuffle=False, sub_sample=0)
{"seed":44,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=45, shuffle=False, sub_sample=0)
{"seed":45,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=46, shuffle=False, sub_sample=0)
{"seed":46,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=47, shuffle=False, sub_sample=0)
{"seed":47,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=48, shuffle=False, sub_sample=0)
{"seed":48,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=49, shuffle=False, sub_sample=0)
{"seed":49,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=50, shuffle=False, sub_sample=0)
{"seed":50,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
namespace(aug='none', autocast=False, batch_seed=-1, bs=200, corner='normal', data='CIFAR10', data_config='./configs/data/cifar10-none.yaml', epochs=200, init_config='./configs/init/normal.yaml', m='allcnn', model_args={'num_classes': 10, 'c1': 96, 'c2': 144, 'bn': True, 'dropout_rate': 0.0}, model_config='./configs/model/allcnn.yaml', opt='Adam', opt_args={'lr': 0.0001, 'weight_decay': 1e-05}, optim_config='./configs/optim/adam-200-0.0001-0.00001.yaml', resize=1, sched_args=None, scheduler='cosine', seed=51, shuffle=False, sub_sample=0)
{"seed":51,"bseed":-1,"aug":"none","m":"allcnn","bn":true,"drop":0.0,"opt":"adam","bs":200,"lr":0.0001,"wd":1e-05}
Traceback (most recent call last):
  File "runner.py", line 149, in <module>
    main()
  File "runner.py", line 139, in main
    m = get_model(model_args, dev=dev)
  File "/home/ubuntu/results/inpca/utils/configure.py", line 41, in get_model
    return getattr(networks, m)(**model_args).to(dev)
TypeError: __init__() got an unexpected keyword argument 'dropout_rate'
